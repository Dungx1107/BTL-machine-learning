{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:06:54.515053Z","iopub.execute_input":"2025-12-13T16:06:54.515268Z","iopub.status.idle":"2025-12-13T16:06:56.611758Z","shell.execute_reply.started":"2025-12-13T16:06:54.515248Z","shell.execute_reply":"2025-12-13T16:06:56.610783Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\n\n\n# const\nINPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\nTRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\nTRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\nTEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n\nWORKING_DIR = Path(\"/kaggle/working\")\n\nINDEX_COLS = [\n    \"video_id\",\n    \"agent_mouse_id\",\n    \"target_mouse_id\",\n    \"video_frame\",\n]\n\n# taken from others' notebooks\n\nSELF_BEHAVIORS = [\n    \"biteobject\",\n    \"climb\",\n    \"dig\",\n    \"exploreobject\",\n    \"freeze\",\n    \"genitalgroom\",\n    \"huddle\",\n    \"rear\",\n    \"rest\",\n    \"run\",\n    \"selfgroom\",\n]\n\nPAIR_BEHAVIORS = [\n    \"allogroom\",\n    \"approach\",\n    \"attack\",\n    \"attemptmount\",\n    \"avoid\",\n    \"chase\",\n    \"chaseattack\",\n    \"defend\",\n    \"disengage\",\n    \"dominance\",\n    \"dominancegroom\",\n    \"dominancemount\",\n    \"ejaculate\",\n    \"escape\",\n    \"flinch\",\n    \"follow\",\n    \"intromit\",\n    \"mount\",\n    \"reciprocalsniff\",\n    \"shepherd\",\n    \"sniff\",\n    \"sniffbody\",\n    \"sniffface\",\n    \"sniffgenital\",\n    \"submit\",\n    \"tussle\",\n]\n\nBODY_PARTS = [\n    \"ear_left\",\n    \"ear_right\",\n    \"nose\",\n    #\"neck\",\n    \"body_center\",\n    \"lateral_left\",\n    \"lateral_right\",\n    \"hip_left\",\n    \"hip_right\",\n    \"tail_base\",\n    \"tail_tip\",\n]\n\nself_idx = {\n    (1,1): 0,\n    (2,2): 1,\n    (3,3): 2,\n    (4,4): 3\n}\n\npair_idx = {\n    (1,2): 4,\n    (1,3): 5,\n    (1,4): 6,\n    (2,1): 7,\n    (2,3): 8,\n    (2,4): 9,\n    (3,1): 10,\n    (3,2): 11,\n    (3,4): 12,\n    (4,1): 13,\n    (4,2): 14,\n    (4,3): 15\n}\n\nBEHAVIOR_TO_ID = {\n    # Self behaviors (0–10)\n    \"biteobject\": 1,\n    \"climb\": 2,\n    \"dig\": 3,\n    \"exploreobject\": 4,\n    \"freeze\": 5,\n    \"genitalgroom\": 6,\n    \"huddle\": 7,\n    \"rear\": 8,\n    \"rest\": 9,\n    \"run\": 10,\n    \"selfgroom\": 11,\n\n    # Pair behaviors (11–36)\n    \"allogroom\": 1,\n    \"approach\": 2,\n    \"attack\": 3,\n    \"attemptmount\": 4,\n    \"avoid\": 5,\n    \"chase\": 6,\n    \"chaseattack\": 7,\n    \"defend\": 8,\n    \"disengage\": 9,\n    \"dominance\": 10,\n    \"dominancegroom\": 11,\n    \"dominancemount\": 12,\n    \"ejaculate\": 13,\n    \"escape\": 14,\n    \"flinch\": 15,\n    \"follow\": 16,\n    \"intromit\": 17,\n    \"mount\": 18,\n    \"reciprocalsniff\": 19,\n    \"shepherd\": 20,\n    \"sniff\": 21,\n    \"sniffbody\": 22,\n    \"sniffface\": 23,\n    \"sniffgenital\": 24,\n    \"submit\": 25,\n    \"tussle\": 26,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:06:56.614096Z","iopub.execute_input":"2025-12-13T16:06:56.614576Z","iopub.status.idle":"2025-12-13T16:06:56.623566Z","shell.execute_reply.started":"2025-12-13T16:06:56.614542Z","shell.execute_reply":"2025-12-13T16:06:56.621533Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_meta = pd.read_csv(INPUT_DIR/\"train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:06:56.624806Z","iopub.execute_input":"2025-12-13T16:06:56.625153Z","iopub.status.idle":"2025-12-13T16:06:56.764212Z","shell.execute_reply.started":"2025-12-13T16:06:56.625123Z","shell.execute_reply":"2025-12-13T16:06:56.763227Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"what do we do with different labs? train them together?","metadata":{}},{"cell_type":"code","source":"# load data?\ntrain_behavior = train_meta[train_meta[\"behaviors_labeled\"].notna()].copy() # drop videos with no annotation\ntrue_train_meta = train_behavior.copy()\ntrain_behavior.loc[:, \"behaviors_labeled_list\"] = (\n    train_behavior[\"behaviors_labeled\"].apply(eval)\n)\n# explode the behaviors into rows\ntrain_behavior = train_behavior[[\"lab_id\", \"video_id\", \"behaviors_labeled_list\"]]\ntrain_behavior = (\n    train_behavior.explode(\"behaviors_labeled_list\")\n    .rename(columns={\"behaviors_labeled_list\": \"behaviors_labeled_element\"})\n)\n\nsplit_elements = train_behavior[\"behaviors_labeled_element\"].str.split(\",\", expand=True)\ntrain_behavior[\"agent\"] = split_elements[0].str.replace(\"'\", \"\", regex=False)\ntrain_behavior[\"target\"] = split_elements[1].str.replace(\"'\", \"\", regex=False)\ntrain_behavior[\"behavior\"] = split_elements[2].str.replace(\"'\", \"\", regex=False)\n\ntrain_behavior = train_behavior[[\"lab_id\", \"video_id\", \"agent\", \"target\", \"behavior\"]].copy()\n\ntrue_train_meta.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:12:21.112580Z","iopub.execute_input":"2025-12-13T16:12:21.112898Z","iopub.status.idle":"2025-12-13T16:12:21.183856Z","shell.execute_reply.started":"2025-12-13T16:12:21.112864Z","shell.execute_reply":"2025-12-13T16:12:21.182899Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"           lab_id   video_id mouse1_strain mouse1_color mouse1_sex  mouse1_id  \\\n0  AdaptableSnail   44566106    CD-1 (ICR)        white       male       10.0   \n1  AdaptableSnail  143861384    CD-1 (ICR)        white       male        3.0   \n2  AdaptableSnail  209576908    CD-1 (ICR)        white       male        7.0   \n3  AdaptableSnail  278643799    CD-1 (ICR)        white       male       11.0   \n4  AdaptableSnail  351967631    CD-1 (ICR)        white       male       14.0   \n\n   mouse1_age mouse1_condition mouse2_strain mouse2_color  ...  \\\n0  8-12 weeks  wireless device    CD-1 (ICR)        white  ...   \n1  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n2  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n3  8-12 weeks  wireless device    CD-1 (ICR)        white  ...   \n4  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n\n  pix_per_cm_approx  video_width_pix video_height_pix arena_width_cm  \\\n0              16.0             1228             1068           60.0   \n1               9.7              968              608           60.0   \n2              16.0             1266             1100           60.0   \n3              16.0             1224             1100           60.0   \n4              16.0             1204             1068           60.0   \n\n  arena_height_cm arena_shape arena_type  \\\n0            60.0      square   familiar   \n1            60.0      square   familiar   \n2            60.0      square   familiar   \n3            60.0      square   familiar   \n4            60.0      square   familiar   \n\n                                  body_parts_tracked  \\\n0  [\"body_center\", \"ear_left\", \"ear_right\", \"head...   \n1  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n2  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n3  [\"body_center\", \"ear_left\", \"ear_right\", \"head...   \n4  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n\n                                   behaviors_labeled tracking_method  \n0  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n1  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n2  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n3  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n4  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n\n[5 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lab_id</th>\n      <th>video_id</th>\n      <th>mouse1_strain</th>\n      <th>mouse1_color</th>\n      <th>mouse1_sex</th>\n      <th>mouse1_id</th>\n      <th>mouse1_age</th>\n      <th>mouse1_condition</th>\n      <th>mouse2_strain</th>\n      <th>mouse2_color</th>\n      <th>...</th>\n      <th>pix_per_cm_approx</th>\n      <th>video_width_pix</th>\n      <th>video_height_pix</th>\n      <th>arena_width_cm</th>\n      <th>arena_height_cm</th>\n      <th>arena_shape</th>\n      <th>arena_type</th>\n      <th>body_parts_tracked</th>\n      <th>behaviors_labeled</th>\n      <th>tracking_method</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AdaptableSnail</td>\n      <td>44566106</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>male</td>\n      <td>10.0</td>\n      <td>8-12 weeks</td>\n      <td>wireless device</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>1228</td>\n      <td>1068</td>\n      <td>60.0</td>\n      <td>60.0</td>\n      <td>square</td>\n      <td>familiar</td>\n      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"head...</td>\n      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n      <td>DeepLabCut</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AdaptableSnail</td>\n      <td>143861384</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>male</td>\n      <td>3.0</td>\n      <td>8-12 weeks</td>\n      <td>NaN</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>...</td>\n      <td>9.7</td>\n      <td>968</td>\n      <td>608</td>\n      <td>60.0</td>\n      <td>60.0</td>\n      <td>square</td>\n      <td>familiar</td>\n      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n      <td>DeepLabCut</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AdaptableSnail</td>\n      <td>209576908</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>male</td>\n      <td>7.0</td>\n      <td>8-12 weeks</td>\n      <td>NaN</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>1266</td>\n      <td>1100</td>\n      <td>60.0</td>\n      <td>60.0</td>\n      <td>square</td>\n      <td>familiar</td>\n      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n      <td>DeepLabCut</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AdaptableSnail</td>\n      <td>278643799</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>male</td>\n      <td>11.0</td>\n      <td>8-12 weeks</td>\n      <td>wireless device</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>1224</td>\n      <td>1100</td>\n      <td>60.0</td>\n      <td>60.0</td>\n      <td>square</td>\n      <td>familiar</td>\n      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"head...</td>\n      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n      <td>DeepLabCut</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AdaptableSnail</td>\n      <td>351967631</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>male</td>\n      <td>14.0</td>\n      <td>8-12 weeks</td>\n      <td>NaN</td>\n      <td>CD-1 (ICR)</td>\n      <td>white</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>1204</td>\n      <td>1068</td>\n      <td>60.0</td>\n      <td>60.0</td>\n      <td>square</td>\n      <td>familiar</td>\n      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n      <td>DeepLabCut</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 38 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 4. Filter for self and pair behaviors\n# should I use this\ntrain_self_behavior_dataframe = (\n    train_behavior[train_behavior[\"behavior\"].isin(SELF_BEHAVIORS)]\n    .copy()\n)\ntrain_pair_behavior_dataframe = (\n    train_behavior[train_behavior[\"behavior\"].isin(PAIR_BEHAVIORS)]\n    .copy()\n)\nprint(train_behavior.info())\ntrain_behavior = train_behavior[train_behavior[\"behavior\"].isin(SELF_BEHAVIORS + PAIR_BEHAVIORS)].copy()\n\nprint(train_behavior.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:11:54.034606Z","iopub.execute_input":"2025-12-13T16:11:54.035187Z","iopub.status.idle":"2025-12-13T16:11:54.059027Z","shell.execute_reply.started":"2025-12-13T16:11:54.035159Z","shell.execute_reply":"2025-12-13T16:11:54.058071Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 4919 entries, 0 to 8788\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   lab_id    4919 non-null   object\n 1   video_id  4919 non-null   int64 \n 2   agent     4919 non-null   object\n 3   target    4919 non-null   object\n 4   behavior  4919 non-null   object\ndtypes: int64(1), object(4)\nmemory usage: 230.6+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nIndex: 4919 entries, 0 to 8788\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   lab_id    4919 non-null   object\n 1   video_id  4919 non-null   int64 \n 2   agent     4919 non-null   object\n 3   target    4919 non-null   object\n 4   behavior  4919 non-null   object\ndtypes: int64(1), object(4)\nmemory usage: 230.6+ KB\nNone\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def make_features_labels(tracking_path, anno_path):\n    try:\n        tracking = pd.read_parquet(tracking_path)\n    except FileNotFoundError:\n        return\n    track = track[track[\"bodypart\"].isin(BODY_PARTS)].copy() # when to copy?\n\n    track_pivot = track.pivot_table(\n        index=[\"video_frame\", \"mouse_id\"],\n        columns=\"bodypart\",\n        values=[\"x\", \"y\"]\n    ).reset_index()\n\n    track_pivot = track_pivot.interpolate(method='linear') # I hate data processing\n    track_pivot = track_pivot.ffill().bfill()\n\n    track_pivot = track_pivot.sort_index(axis=1)\n\n\n    track_wide = track_pivot.pivot_table(\n        index=\"video_frame\",\n        columns=\"mouse_id\",\n        values= track_pivot.columns.drop([\"video_frame\", \"mouse_id\"])\n    )\n    \n    # there might be less than 4 mouses tracked, read from metadata to find the number of mouses for each video\n    track_wide = track_wide.interpolate(method='linear') # I hate data processing\n    track_wide = track_wide.ffill().bfill()\n    \n    track_wide = track_wide.sort_index(axis=1)  # tidy order\n\n\n    # CREATE LABELS FROM HERE\n    # explode labels\n\n    slot_names = []\n    self_idx = {}\n    pair_idx = {}\n    \n    slot_id = 0\n    \n    # self slots\n    for i in range(1, 5):\n        self_idx[(i, i)] = slot_id\n        slot_names.append(f\"self{i}\")\n        slot_id += 1\n    \n    # pair slots (ordered)\n    for i in range(1, 5):\n        for j in range(1, 5):\n            if i != j:\n                pair_idx[(i, j)] = slot_id\n                slot_names.append(f\"pair{i}{j}\")\n                slot_id += 1\n    \n    assert slot_id == 16\n    \n    frames = track_wide.index.to_numpy()\n    \n    label_df = pd.DataFrame(\n        0,\n        index=frames,\n        columns=slot_names,\n        dtype=np.int64\n    )\n    \n    label_df.index.name = \"video_frame\"\n    \n    anno = pd.read_parquet(anno_path)\n    \n    for _, row in anno.iterrows():\n        agent  = int(row[\"agent_id\"])\n        target = int(row[\"target_id\"])\n        beh    = row[\"action\"]\n    \n        beh_id = BEHAVIOR_TO_ID[beh]\n    \n        if agent == target:\n            col = slot_names[self_idx[(agent, agent)]]\n        else:\n            col = slot_names[pair_idx[(agent, target)]]\n    \n        label_df.loc[row[\"start_frame\"]:row[\"stop_frame\"], col] = beh_id\n\n        \n    return track_wide, label_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T17:49:28.998221Z","iopub.execute_input":"2025-12-13T17:49:28.998521Z","iopub.status.idle":"2025-12-13T17:49:29.010437Z","shell.execute_reply.started":"2025-12-13T17:49:28.998498Z","shell.execute_reply":"2025-12-13T17:49:29.009597Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# def make_labels(anno_path):   \n#     anno = pd.read_parquet(\"/kaggle/input/MABe-mouse-behavior-detection/train_annotation/AdaptableSnail/1212811043.parquet\")\n#     anno.head()\n#     frame_labels = []\n#     for _, row in anno.iterrows():\n#         for frame in range(row[\"start_frame\"], row[\"stop_frame\"] + 1):\n#             frame_labels.append({\n#                 \"frame\": frame,\n#                 \"agent_id\" : row[\"agent_id\"],\n#                 \"target_id\" : int(row[\"target_id\"]),\n#                 \"action\" : row[\"action\"]\n#             })\n#     frame_labels_df = pd.DataFrame(frame_labels)\n#     frame_labels_df.head()\n\n# # this for loop probably won't be good\n\n# #problem: target_id for self? it's the same as agent_id\n\n# # annos = pd.read_parquet(\"/kaggle/input/MABe-mouse-behavior-detection/train_annotation/GroovyShrew/991578442.parquet\")\n# # annos = annos[annos[\"target_id\"] == annos[\"agent_id\"]]\n# # annos.head()\n\n# num_frames = len(df_wide) # how to really find num_frames (non-action at the end, will break Y)\n\n# Y = np.zeros((num_frames + 1, 16), dtype=int) \n\n# for _, row in frame_labels_df.iterrows():\n#     f = row[\"frame\"]\n#     i = row[\"agent_id\"]\n#     r = row[\"target_id\"]\n#     beh = row[\"action\"]\n\n#     beh_id = BEHAVIOR_TO_ID[beh]\n\n#     if i == r:  # self action\n#         idx = self_idx[(i,i)]\n#     else:\n#         idx = pair_idx[(i,r)]\n\n#     Y[f, idx] = beh_id\n\n# print(Y[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T04:21:05.855129Z","iopub.execute_input":"2025-12-12T04:21:05.855447Z","iopub.status.idle":"2025-12-12T04:21:06.529915Z","shell.execute_reply.started":"2025-12-12T04:21:05.855404Z","shell.execute_reply":"2025-12-12T04:21:06.529035Z"}},"outputs":[{"name":"stdout","text":"[0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\ndef process_video(row):\n    \"\"\"Process a single video\"\"\"\n    lab_id = row[\"lab_id\"]\n    if lab_id.startswith('MABe22'): \n        return\n    video_id = row[\"video_id\"]\n\n    tracking_path = TRAIN_TRACKING_DIR / f\"{lab_id}/{video_id}.parquet\"\n    anno_path = TRAIN_ANNOTATION_DIR / f\"{lab_id}/{video_id}.parquet\"\n\n\n    # self_features = make_self_features(metadata=row, tracking=tracking)\n    # pair_features = make_pair_features(metadata=row, tracking=tracking)\n\n    # self_features.write_parquet(WORKING_DIR / \"self_features\" / f\"{video_id}.parquet\")\n    # pair_features.write_parquet(WORKING_DIR / \"pair_features\" / f\"{video_id}.parquet\")\n\n    features, labels = make_features(lab_id, video_id)\n    features.to_parquet(WORKING_DIR / \"features\" / f\"{video_id}.parquet\")\n    labels.to_parquet(WORKING_DIR / \"labels\" / f\"{video_id}.parquet\")\n\n    return video_id\n\nrows = true_train_meta\n# this ain't good\nfor _, row in tqdm(rows.iterrows(), total=len(rows), desc=\"Processing videos\"):\n    process_video(row)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T16:26:53.240434Z","iopub.execute_input":"2025-12-13T16:26:53.241218Z","iopub.status.idle":"2025-12-13T16:31:31.951711Z","shell.execute_reply.started":"2025-12-13T16:26:53.241188Z","shell.execute_reply":"2025-12-13T16:31:31.951095Z"}},"outputs":[{"name":"stderr","text":"Processing videos: 100%|██████████| 848/848 [04:38<00:00,  3.04it/s] \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"test = pd.read_parquet(\"/kaggle/working/1059582964.parquet\")\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T17:03:41.634891Z","iopub.execute_input":"2025-12-13T17:03:41.635704Z","iopub.status.idle":"2025-12-13T17:03:41.860126Z","shell.execute_reply.started":"2025-12-13T17:03:41.635647Z","shell.execute_reply":"2025-12-13T17:03:41.859195Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                      x                                                  \\\nbodypart    body_center                ear_left               ear_right   \nmouse_id              1           2           1           2           1   \nvideo_frame                                                               \n0            256.928375  259.561096  254.337341  265.714478  257.576691   \n1            256.928375  259.561096  254.454559  265.774231  257.576691   \n2            256.928375  259.561096  254.419815  265.755035  257.576691   \n3            256.928375  259.561096  254.435913  265.865112  257.576691   \n4            256.928375  259.561096  254.559723  265.840881  257.576691   \n\n                                                                         \\\nbodypart                       nose               tail_base               \nmouse_id              2           1           2           1           2   \nvideo_frame                                                               \n0            253.268875  265.797089  274.311096  266.671265  261.547028   \n1            253.268875  265.726105  274.311096  266.671265  261.512115   \n2            253.268875  265.820007  274.315704  266.671265  261.535095   \n3            253.268875  265.746063  274.320282  266.671265  261.486542   \n4            253.268875  265.737518  274.324890  266.671265  261.496674   \n\n                      y                                                  \\\nbodypart    body_center                ear_left               ear_right   \nmouse_id              1           2           1           2           1   \nvideo_frame                                                               \n0            233.626511  233.566818  240.892166  240.928375  241.284576   \n1            233.626511  233.566818  240.913528  240.866180  241.284576   \n2            233.626511  233.566818  240.836395  240.740402  241.284576   \n3            233.626511  233.566818  241.016708  240.788574  241.284576   \n4            233.626511  233.566818  240.969833  240.811935  241.284576   \n\n                                                                         \nbodypart                       nose               tail_base              \nmouse_id              2           1           2           1           2  \nvideo_frame                                                              \n0            242.128891  233.380829  242.796249  254.485504  218.579285  \n1            242.128891  233.339371  242.796249  254.485504  218.556870  \n2            242.128891  233.385681  242.786972  254.485504  218.520996  \n3            242.128891  233.388962  242.777695  254.485504  218.499298  \n4            242.128891  233.391556  242.768417  254.485504  218.499451  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"10\" halign=\"left\">x</th>\n      <th colspan=\"10\" halign=\"left\">y</th>\n    </tr>\n    <tr>\n      <th>bodypart</th>\n      <th colspan=\"2\" halign=\"left\">body_center</th>\n      <th colspan=\"2\" halign=\"left\">ear_left</th>\n      <th colspan=\"2\" halign=\"left\">ear_right</th>\n      <th colspan=\"2\" halign=\"left\">nose</th>\n      <th colspan=\"2\" halign=\"left\">tail_base</th>\n      <th colspan=\"2\" halign=\"left\">body_center</th>\n      <th colspan=\"2\" halign=\"left\">ear_left</th>\n      <th colspan=\"2\" halign=\"left\">ear_right</th>\n      <th colspan=\"2\" halign=\"left\">nose</th>\n      <th colspan=\"2\" halign=\"left\">tail_base</th>\n    </tr>\n    <tr>\n      <th>mouse_id</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n    <tr>\n      <th>video_frame</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>256.928375</td>\n      <td>259.561096</td>\n      <td>254.337341</td>\n      <td>265.714478</td>\n      <td>257.576691</td>\n      <td>253.268875</td>\n      <td>265.797089</td>\n      <td>274.311096</td>\n      <td>266.671265</td>\n      <td>261.547028</td>\n      <td>233.626511</td>\n      <td>233.566818</td>\n      <td>240.892166</td>\n      <td>240.928375</td>\n      <td>241.284576</td>\n      <td>242.128891</td>\n      <td>233.380829</td>\n      <td>242.796249</td>\n      <td>254.485504</td>\n      <td>218.579285</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>256.928375</td>\n      <td>259.561096</td>\n      <td>254.454559</td>\n      <td>265.774231</td>\n      <td>257.576691</td>\n      <td>253.268875</td>\n      <td>265.726105</td>\n      <td>274.311096</td>\n      <td>266.671265</td>\n      <td>261.512115</td>\n      <td>233.626511</td>\n      <td>233.566818</td>\n      <td>240.913528</td>\n      <td>240.866180</td>\n      <td>241.284576</td>\n      <td>242.128891</td>\n      <td>233.339371</td>\n      <td>242.796249</td>\n      <td>254.485504</td>\n      <td>218.556870</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>256.928375</td>\n      <td>259.561096</td>\n      <td>254.419815</td>\n      <td>265.755035</td>\n      <td>257.576691</td>\n      <td>253.268875</td>\n      <td>265.820007</td>\n      <td>274.315704</td>\n      <td>266.671265</td>\n      <td>261.535095</td>\n      <td>233.626511</td>\n      <td>233.566818</td>\n      <td>240.836395</td>\n      <td>240.740402</td>\n      <td>241.284576</td>\n      <td>242.128891</td>\n      <td>233.385681</td>\n      <td>242.786972</td>\n      <td>254.485504</td>\n      <td>218.520996</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>256.928375</td>\n      <td>259.561096</td>\n      <td>254.435913</td>\n      <td>265.865112</td>\n      <td>257.576691</td>\n      <td>253.268875</td>\n      <td>265.746063</td>\n      <td>274.320282</td>\n      <td>266.671265</td>\n      <td>261.486542</td>\n      <td>233.626511</td>\n      <td>233.566818</td>\n      <td>241.016708</td>\n      <td>240.788574</td>\n      <td>241.284576</td>\n      <td>242.128891</td>\n      <td>233.388962</td>\n      <td>242.777695</td>\n      <td>254.485504</td>\n      <td>218.499298</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>256.928375</td>\n      <td>259.561096</td>\n      <td>254.559723</td>\n      <td>265.840881</td>\n      <td>257.576691</td>\n      <td>253.268875</td>\n      <td>265.737518</td>\n      <td>274.324890</td>\n      <td>266.671265</td>\n      <td>261.496674</td>\n      <td>233.626511</td>\n      <td>233.566818</td>\n      <td>240.969833</td>\n      <td>240.811935</td>\n      <td>241.284576</td>\n      <td>242.128891</td>\n      <td>233.391556</td>\n      <td>242.768417</td>\n      <td>254.485504</td>\n      <td>218.499451</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def make_windows(, seq_len)\nX = track_wide.to_numpy()  # shape (num_frames, feature_dim)\nX.shape\n\nseq_len = 40\nstride = 20\n\nX_sequences = []\nfor start in range(0, len(df_wide) - seq_len + 1, stride):\n    seq = df_wide.iloc[start : start + seq_len].values\n    X_sequences.append(seq)\n\nX = np.array(X_sequences)\nprint(X.shape)  # (num_sequences, 40, num_features)\nnan_count_X = np.isnan(X).sum()\nprint(nan_count_X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T04:20:56.947976Z","iopub.execute_input":"2025-12-12T04:20:56.948281Z","iopub.status.idle":"2025-12-12T04:20:57.082851Z","shell.execute_reply.started":"2025-12-12T04:20:56.948257Z","shell.execute_reply":"2025-12-12T04:20:57.081937Z"}},"outputs":[{"name":"stdout","text":"(4497, 40, 64)\n0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T04:21:02.334703Z","iopub.execute_input":"2025-12-12T04:21:02.335013Z","iopub.status.idle":"2025-12-12T04:21:02.486349Z","shell.execute_reply.started":"2025-12-12T04:21:02.334991Z","shell.execute_reply":"2025-12-12T04:21:02.485615Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   frame  agent_id  target_id action\n0      2         1          3  chase\n1      3         1          3  chase\n2      4         1          3  chase\n3      5         1          3  chase\n4      6         1          3  chase","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>agent_id</th>\n      <th>target_id</th>\n      <th>action</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>chase</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>chase</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>1</td>\n      <td>3</td>\n      <td>chase</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>chase</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>1</td>\n      <td>3</td>\n      <td>chase</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# with this you need to do something so Y size is the same as X\ndef build_Y_sequences(Y, seq_len=40, stride=20):\n    \"\"\"\n    Y: np.array of shape (num_frames, 16)\n    Returns: np.array of shape (num_sequences, seq_len, 16)\n    \"\"\"\n    Y_sequences = []\n\n    for start in range(0, len(Y) - seq_len + 1, stride):\n        seq = Y[start : start + seq_len]\n        Y_sequences.append(seq)\n\n    return np.array(Y_sequences)\n\n\nY_seq = build_Y_sequences(Y, seq_len=40, stride=20)\nprint(Y_seq.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T04:21:11.318876Z","iopub.execute_input":"2025-12-12T04:21:11.319203Z","iopub.status.idle":"2025-12-12T04:21:11.336004Z","shell.execute_reply.started":"2025-12-12T04:21:11.319178Z","shell.execute_reply":"2025-12-12T04:21:11.334915Z"}},"outputs":[{"name":"stdout","text":"(4497, 40, 16)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#training\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score, accuracy_score, hamming_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom collections import Counter\n\nimport tensorflow as tf\n\ndropout = 0.3 #dropout\nn_splits = 3 #for fold\nbatch_size = 64\n\n# sizes for LSTM\nseq_len = X.shape[1]\nnum_features = X.shape[2] \nnum_labels = Y_seq.shape[2]  # 16\n\nnum_classes_per_output = [\n    11, 11, 11, 11,  # self-actions for 4 mice \n    26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26  # pair-actions for 12 pairs\n]\n\ndef focal_loss(gamma=2., alpha=4.0):\n    def focal_loss_fixed(y_true, y_pred):\n        # Sparse Categorical Focal Loss\n        y_true = tf.cast(y_true, tf.int32)\n        y_true = tf.one_hot(y_true, depth=y_pred.shape[-1])\n        y_true = tf.cast(y_true, tf.float32)\n        \n        epsilon = tf.keras.backend.epsilon()\n        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n        \n        # Calculate Cross Entropy\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        \n        # Calculate Focal Loss weights\n        loss = alpha * tf.math.pow(1 - y_pred, gamma) * cross_entropy\n        \n        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n    return focal_loss_fixed\n\ndef build_model(seq_len, num_features):\n\n    # model = Sequential()\n    \n    # # LSTM layer: return_sequences=True to predict per frame\n    # model.add(LSTM(128, input_shape=(seq_len, num_features), return_sequences=True))\n    # model.add(Dropout(0.3))\n    # model.add(BatchNormalization())\n\n    # # Output layer: 16 behaviors, sigmoid for multi-label\n    # model.add(Dense(num_classes, activation='softmax'))\n    \n    # adam_optimizer = Adam(\n    #     learning_rate=0.001,\n    #     # Implement Gradient Clipping\n    #     # We clip by L2 norm, a common and effective technique for LSTMs.\n    #     clipnorm=1.0 \n    # )\n    \n    # # Compile with binary_crossentropy for multi-label\n    # model.compile(\n    #     optimizer= adam_optimizer,\n    #     loss= 'categorical_crossentropy',\n    #     metrics=['accuracy']\n    # )\n    \n    inputs = Input(shape=(seq_len, num_features))\n    x = LSTM(128, return_sequences=True)(inputs)  # LSTM with sequences\n\n    outputs = []\n    for n_classes in num_classes_per_output:\n        out = Dense(n_classes, activation='softmax')(x)  # softmax per output\n        outputs.append(out)\n\n    adam_optimizer = Adam(\n        learning_rate=0.001,\n        clipnorm=1.0         # <-- CRITICAL: Prevents gradients from exploding\n    )\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(\n        optimizer= adam_optimizer,\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'] * 16\n    )\n    \n    return model\n\ngkf = GroupKFold(n_splits=n_splits) # DON'T USE STRATIFIED K FOLD BUT WHY I FORGOR\n# do this later anyway\n\n\n#Scale the features\nN, T, F = X.shape # e.g. (36000, 100, 16)\nX_flat = X.reshape(-1, F)\nscaler = StandardScaler()\nX_flat_scaled = scaler.fit_transform(X_flat)\n#Reshape back to 3D sequences\nX_scaled = X_flat_scaled.reshape(N, T, F)\nprint(X_scaled.shape)\n\n# 1. Split the raw arrays first (X_scaled and Y_seq both have N samples)\nX_train, X_val, Y_seq_train, Y_seq_val = train_test_split(\n    X_scaled, \n    Y_seq, \n    test_size=0.2, \n    random_state=42\n)\n\n\n# # Y_seq_train is the array of shape (N_train, T, 16)\n\n# 1. Flatten Y_seq_train into a single array of frame-labels\n# Shape: (N_train * T * 16,)\ny_train_flat_all = Y_seq_train.flatten()\nprint(y_train_flat_all)\n\n\n# Filter the array to keep only non-zero labels\nnon_zero_labels = y_train_flat_all[y_train_flat_all != 0]\n\n# Count the frequency of each non-zero label\nlabel_counts = Counter(non_zero_labels)\n\n# Convert to a DataFrame for easy viewing and sorting\nimport pandas as pd\ncounts_df = pd.DataFrame(label_counts.items(), columns=['Class ID', 'Count'])\ncounts_df = counts_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n\nprint(\"\\n--- Frequency of Non-Zero Behaviors ---\")\nprint(counts_df.to_markdown(index=False))\n\n\n\n# 2. Identify all unique classes present across the entire dataset (0 up to 25)\n# Note: You need to know the highest possible class ID, which is 25.\nunique_classes_present = np.unique(y_train_flat_all)\n\n# 3. Compute Class Weights\n# 'balanced' mode automatically sets weights inversely proportional to class frequencies.\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',\n    classes=unique_classes_present,\n    y=y_train_flat_all\n)\nMAX_WEIGHT_CAP = 5\n# 4. Convert the array back into a dictionary for Keras\nclass_weights_dict = dict(zip(unique_classes_present, class_weights_array))\nfor class_id in list(class_weights_dict.keys()): # Iterate over a copy of keys\n    weight = class_weights_dict[class_id]\n    if weight > MAX_WEIGHT_CAP:\n        class_weights_dict[class_id] = MAX_WEIGHT_CAP\n\nprint(\"--- Calculated Class Weights (ID: Weight) ---\")\n# Displaying the results to show how much more weight is assigned to minority classes\nfor class_id, weight in list(class_weights_dict.items())[:5]: # Show first 5\n    print(f\"Class {class_id}: {weight:.2f}\")\n\n# Example interpretation: If Class 0 has a weight of ~0.5 and Class 25 has a weight of ~50.0,\n# misclassifying Class 25 is 100 times worse than misclassifying Class 0.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T05:07:33.040670Z","iopub.execute_input":"2025-12-12T05:07:33.041303Z","iopub.status.idle":"2025-12-12T05:07:33.719367Z","shell.execute_reply.started":"2025-12-12T05:07:33.041277Z","shell.execute_reply":"2025-12-12T05:07:33.718502Z"}},"outputs":[{"name":"stdout","text":"(4497, 40, 64)\n[0 0 0 ... 0 0 0]\n\n--- Frequency of Non-Zero Behaviors ---\n|   Class ID |   Count |\n|-----------:|--------:|\n|          3 |    9001 |\n|          5 |    5580 |\n|          6 |    5093 |\n|          7 |    1609 |\n--- Calculated Class Weights (ID: Weight) ---\nClass 0: 0.20\nClass 3: 5.00\nClass 5: 5.00\nClass 6: 5.00\nClass 7: 5.00\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\n# # --- Proceed to Step 2 (Applying Weights) ---\n\n# Map the weights to every label in the training set (N_train, T, 16)\nsample_weight_matrix = np.vectorize(class_weights_dict.get)(Y_seq_train)\n\n# Convert the sample_weight_matrix into the Keras LIST format\nsample_weight_list = [sample_weight_matrix[:, :, i] for i in range(16)]\n\n# 2. NOW convert the splits into the list format your model needs (16 outputs)\ny_train = [Y_seq_train[:, :, i] for i in range(16)]\ny_val   = [Y_seq_val[:, :, i] for i in range(16)]\n\nmodel = build_model(seq_len, num_features)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=1,\n    batch_size=batch_size,\n    sample_weight=sample_weight_list,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T05:07:52.685166Z","iopub.execute_input":"2025-12-12T05:07:52.685489Z","iopub.status.idle":"2025-12-12T05:08:13.273422Z","shell.execute_reply.started":"2025-12-12T05:07:52.685469Z","shell.execute_reply":"2025-12-12T05:08:13.272478Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 142ms/step - dense_48_accuracy: 0.5623 - dense_48_loss: 0.3109 - dense_49_accuracy: 0.5333 - dense_49_loss: 0.3354 - dense_50_accuracy: 0.4400 - dense_50_loss: 0.3537 - dense_51_accuracy: 0.5032 - dense_51_loss: 0.3383 - dense_52_accuracy: 0.3842 - dense_52_loss: 0.9209 - dense_53_accuracy: 0.3497 - dense_53_loss: 0.9244 - dense_54_accuracy: 0.3643 - dense_54_loss: 1.1287 - dense_55_accuracy: 0.3062 - dense_55_loss: 0.5968 - dense_56_accuracy: 0.4778 - dense_56_loss: 0.4755 - dense_57_accuracy: 0.3905 - dense_57_loss: 0.5047 - dense_58_accuracy: 0.3388 - dense_58_loss: 0.5810 - dense_59_accuracy: 0.4058 - dense_59_loss: 0.5137 - dense_60_accuracy: 0.3856 - dense_60_loss: 0.5638 - dense_61_accuracy: 0.4275 - dense_61_loss: 0.7251 - dense_62_accuracy: 0.4696 - dense_62_loss: 0.5413 - dense_63_accuracy: 0.3381 - dense_63_loss: 0.6292 - loss: 9.4455 - val_dense_48_accuracy: 1.0000 - val_dense_48_loss: 0.0633 - val_dense_49_accuracy: 1.0000 - val_dense_49_loss: 0.0752 - val_dense_50_accuracy: 1.0000 - val_dense_50_loss: 0.0821 - val_dense_51_accuracy: 1.0000 - val_dense_51_loss: 0.0759 - val_dense_52_accuracy: 0.7345 - val_dense_52_loss: 0.8269 - val_dense_53_accuracy: 0.8833 - val_dense_53_loss: 0.7455 - val_dense_54_accuracy: 0.6067 - val_dense_54_loss: 1.1060 - val_dense_55_accuracy: 0.9962 - val_dense_55_loss: 0.1706 - val_dense_56_accuracy: 0.9999 - val_dense_56_loss: 0.1178 - val_dense_57_accuracy: 0.9989 - val_dense_57_loss: 0.1384 - val_dense_58_accuracy: 0.9969 - val_dense_58_loss: 0.2087 - val_dense_59_accuracy: 0.9977 - val_dense_59_loss: 0.1486 - val_dense_60_accuracy: 0.9871 - val_dense_60_loss: 0.2748 - val_dense_61_accuracy: 0.9772 - val_dense_61_loss: 0.4846 - val_dense_62_accuracy: 0.9928 - val_dense_62_loss: 0.2178 - val_dense_63_accuracy: 0.9912 - val_dense_63_loss: 0.2710 - val_loss: 4.9849\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Assume X_val (validation inputs) is available\n\n# 1. Select the first sequence for inspection\nX_sample = X_val[0:1] # Shape (1, seq_len, num_features)\n\n# 2. Get predictions (y_pred_list is a list of 16 arrays (16 neurons output, 4 + 12))\n# Each array in the list has shape: (1, seq_len, num_classes)\ny_pred_list = model.predict(X_sample)\n\nfor i, pred in enumerate(y_pred_list):\n    print(f\"Output {i}: shape = {pred.shape}\")\n    print(pred)\n\n# 3. Choose a specific time step (frame) to inspect\nFRAME_INDEX = 3 \n\nprint(f\"--- Predictions for Sequence 0, Frame {FRAME_INDEX} ---\")\n\noutput_data = []\n\n# Loop through each of the 16 behavioral outputs\nfor i, pred_array in enumerate(y_pred_list):\n    \n    # Extract the prediction vector for the chosen frame\n    # pred_vector shape: (num_classes,)\n    pred_vector = pred_array[0, FRAME_INDEX, :] \n    \n    # Get the class with the highest probability\n    predicted_class_id = np.argmax(pred_vector)\n    max_probability = np.max(pred_vector)\n    \n    # Get the name/class count for context\n    output_name = f\"Output {i+1} ({'Self' if i < 4 else 'Pair'})\"\n    n_classes = len(pred_vector)\n\n    output_data.append({\n        'Output Name': output_name,\n        'Classes': n_classes,\n        'Predicted ID': predicted_class_id,\n        'Max Probability': f\"{max_probability:.4f}\",\n        'Prediction Vector (Example)': pred_vector[:min(n_classes, 5)] # Show first 5 probabilities\n    })\n\n# 4. Display the results in a formatted table\npred_df = pd.DataFrame(output_data)\nprint(pred_df.to_markdown(index=False))\n\nprint(\"\\n\\n--- Raw Prediction Vector Examples ---\")\nprint(\"These are the probabilities (softmax output) for the first few classes in each output.\")\nprint(\"The full vector length is either 11 or 26.\")\n\n# Print the full vectors for the first two outputs as an example\nprint(f\"\\nOutput 1 (11 classes) Vector:\\n{y_pred_list[0][0, FRAME_INDEX, :]}\")\nprint(f\"\\nOutput 5 (26 classes) Vector:\\n{y_pred_list[4][0, FRAME_INDEX, :]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T05:10:40.758262Z","iopub.execute_input":"2025-12-12T05:10:40.758964Z","iopub.status.idle":"2025-12-12T05:10:40.880185Z","shell.execute_reply.started":"2025-12-12T05:10:40.758936Z","shell.execute_reply":"2025-12-12T05:10:40.879192Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\nOutput 0: shape = (1, 40, 11)\n[[[1.82278782e-01 7.07856193e-02 6.35230988e-02 9.00594518e-02\n   6.87034205e-02 8.88280496e-02 8.24742690e-02 8.00233781e-02\n   9.34133083e-02 8.44425261e-02 9.54679996e-02]\n  [3.63122642e-01 4.84428853e-02 3.90781350e-02 7.99590573e-02\n   4.53748778e-02 6.81126565e-02 7.17513785e-02 5.70326522e-02\n   8.07887167e-02 6.64324388e-02 7.99045339e-02]\n  [6.56614482e-01 2.29238104e-02 1.70496628e-02 5.08041382e-02\n   2.13870760e-02 3.27332430e-02 4.92553376e-02 2.72350870e-02\n   4.45299596e-02 3.49001959e-02 4.25669327e-02]\n  [8.93769860e-01 5.91998687e-03 4.32328694e-03 1.84681676e-02\n   5.89036150e-03 8.44706967e-03 2.16430500e-02 7.39216432e-03\n   1.20666865e-02 1.00716958e-02 1.20076425e-02]\n  [9.76169109e-01 1.03294791e-03 8.07524833e-04 4.54738783e-03\n   1.14557706e-03 1.52524456e-03 6.89159753e-03 1.44824514e-03\n   2.08997959e-03 2.03966210e-03 2.30271020e-03]\n  [9.94047165e-01 2.04169948e-04 1.81895652e-04 1.16775208e-03\n   2.55571213e-04 3.17287311e-04 2.13684118e-03 3.17277329e-04\n   4.08888765e-04 4.72935964e-04 4.90248844e-04]\n  [9.97874081e-01 6.22548396e-05 6.49302383e-05 4.25470556e-04\n   8.80278167e-05 1.03741404e-04 8.30119185e-04 1.03420360e-04\n   1.29924709e-04 1.61870412e-04 1.56152790e-04]\n  [9.98873115e-01 3.04755013e-05 3.64398038e-05 2.31181577e-04\n   4.75380657e-05 5.40912079e-05 4.47180821e-04 5.24979441e-05\n   6.68407520e-05 8.29846686e-05 7.77213427e-05]\n  [9.99186933e-01 2.12287596e-05 2.79966152e-05 1.69918785e-04\n   3.52959505e-05 3.94015515e-05 3.21578584e-04 3.72890208e-05\n   4.84872508e-05 5.78563559e-05 5.39913999e-05]\n  [9.99301434e-01 1.79935050e-05 2.52014888e-05 1.47941042e-04\n   3.10553223e-05 3.42678104e-05 2.75321101e-04 3.17645572e-05\n   4.21366640e-05 4.80005292e-05 4.49353793e-05]\n  [9.99345720e-01 1.68324023e-05 2.44091771e-05 1.39325901e-04\n   2.97687402e-05 3.25340407e-05 2.56498170e-04 2.96660619e-05\n   3.98467564e-05 4.38855968e-05 4.14558453e-05]\n  [9.99362171e-01 1.63837067e-05 2.42350125e-05 1.36523144e-04\n   2.93256780e-05 3.19780738e-05 2.49385717e-04 2.88982137e-05\n   3.92408983e-05 4.20303768e-05 3.98655029e-05]\n  [9.99371111e-01 1.61712160e-05 2.42819369e-05 1.35267765e-04\n   2.91028882e-05 3.16992882e-05 2.44994822e-04 2.85470087e-05\n   3.91070134e-05 4.09844688e-05 3.88594890e-05]\n  [9.99377191e-01 1.60730651e-05 2.41674043e-05 1.34316579e-04\n   2.88657338e-05 3.14177196e-05 2.42336304e-04 2.82520796e-05\n   3.90807436e-05 4.01454927e-05 3.82360340e-05]\n  [9.99374330e-01 1.61719781e-05 2.43431805e-05 1.34816553e-04\n   2.89008003e-05 3.15187026e-05 2.43818067e-04 2.83019144e-05\n   3.95510360e-05 3.99759410e-05 3.82029029e-05]\n  [9.99369800e-01 1.62865908e-05 2.46020354e-05 1.35561000e-04\n   2.89174313e-05 3.16460282e-05 2.46549404e-04 2.83305180e-05\n   4.00386743e-05 3.99420351e-05 3.83266197e-05]\n  [9.99365270e-01 1.63405803e-05 2.48215183e-05 1.36347269e-04\n   2.91593351e-05 3.19174214e-05 2.48393393e-04 2.85844362e-05\n   4.05056999e-05 4.00643512e-05 3.85584754e-05]\n  [9.99361694e-01 1.64764588e-05 2.50754383e-05 1.37022784e-04\n   2.93322373e-05 3.21248699e-05 2.49617122e-04 2.87809817e-05\n   4.09534077e-05 4.00734934e-05 3.87763102e-05]\n  [9.99368370e-01 1.61999596e-05 2.48983160e-05 1.35005772e-04\n   2.90890457e-05 3.15748330e-05 2.47945689e-04 2.86066152e-05\n   4.08972846e-05 3.92510956e-05 3.82906146e-05]\n  [9.99363720e-01 1.63070072e-05 2.50443263e-05 1.35834794e-04\n   2.92240729e-05 3.16428122e-05 2.50359066e-04 2.88298834e-05\n   4.14781862e-05 3.90338828e-05 3.85430867e-05]\n  [9.99358594e-01 1.65275378e-05 2.51918991e-05 1.36540504e-04\n   2.94867423e-05 3.18053608e-05 2.52655096e-04 2.91800334e-05\n   4.21100303e-05 3.88943736e-05 3.89255838e-05]\n  [9.99347746e-01 1.68829156e-05 2.54951410e-05 1.38078685e-04\n   2.99691947e-05 3.21913431e-05 2.57408567e-04 2.98207142e-05\n   4.31433364e-05 3.92386828e-05 3.99378368e-05]\n  [9.99333203e-01 1.73387161e-05 2.59943863e-05 1.39988886e-04\n   3.04643672e-05 3.28208662e-05 2.64383241e-04 3.04934329e-05\n   4.42917117e-05 3.99406053e-05 4.10970788e-05]\n  [9.99314129e-01 1.79648268e-05 2.67053238e-05 1.43356330e-04\n   3.12026023e-05 3.36973535e-05 2.72031350e-04 3.12986012e-05\n   4.57717870e-05 4.09779059e-05 4.27119958e-05]\n  [9.99300003e-01 1.83746361e-05 2.72026846e-05 1.45809958e-04\n   3.16076839e-05 3.41069608e-05 2.78756139e-04 3.19832834e-05\n   4.69706611e-05 4.16403745e-05 4.37850176e-05]\n  [9.99287605e-01 1.87424212e-05 2.77036252e-05 1.47710060e-04\n   3.20585277e-05 3.44755172e-05 2.84274720e-04 3.25415203e-05\n   4.79303853e-05 4.21809091e-05 4.47377643e-05]\n  [9.99279499e-01 1.89023867e-05 2.79056421e-05 1.49109459e-04\n   3.22827545e-05 3.46226479e-05 2.88117735e-04 3.29734212e-05\n   4.87801954e-05 4.24089849e-05 4.53828834e-05]\n  [9.99276638e-01 1.89296097e-05 2.79172473e-05 1.49427200e-04\n   3.23249915e-05 3.44952641e-05 2.90578057e-04 3.30295370e-05\n   4.92911822e-05 4.21027580e-05 4.53071661e-05]\n  [9.99274731e-01 1.89431157e-05 2.79415926e-05 1.49417931e-04\n   3.23964960e-05 3.44687251e-05 2.92089186e-04 3.31146293e-05\n   4.97144611e-05 4.18096315e-05 4.52983077e-05]\n  [9.99277592e-01 1.88516569e-05 2.77531162e-05 1.48750260e-04\n   3.23122295e-05 3.41868254e-05 2.91319448e-04 3.31265728e-05\n   4.96727080e-05 4.14325914e-05 4.49664149e-05]\n  [9.99279737e-01 1.87889837e-05 2.75634993e-05 1.48312043e-04\n   3.21943371e-05 3.39837352e-05 2.91007949e-04 3.30252296e-05\n   4.97677465e-05 4.10744651e-05 4.46624290e-05]\n  [9.99279022e-01 1.87909427e-05 2.75315597e-05 1.48554143e-04\n   3.21545813e-05 3.38798054e-05 2.91839009e-04 3.29731229e-05\n   4.99708331e-05 4.08917549e-05 4.44659636e-05]\n  [9.99277592e-01 1.88251379e-05 2.75197599e-05 1.48781488e-04\n   3.21352818e-05 3.39066464e-05 2.92850658e-04 3.28704082e-05\n   5.01739487e-05 4.08705273e-05 4.43603931e-05]\n  [9.99277830e-01 1.87738315e-05 2.74565355e-05 1.48649342e-04\n   3.21794178e-05 3.37982783e-05 2.93738238e-04 3.27720700e-05\n   5.02172807e-05 4.07104235e-05 4.39993237e-05]\n  [9.99282837e-01 1.85633271e-05 2.72715715e-05 1.46992810e-04\n   3.20430299e-05 3.33763746e-05 2.92995217e-04 3.25093861e-05\n   5.00946553e-05 4.00558129e-05 4.32511442e-05]\n  [9.99297380e-01 1.97769259e-05 2.60825018e-05 1.47558167e-04\n   3.13812234e-05 3.28202223e-05 2.82001449e-04 3.15641628e-05\n   4.98865629e-05 4.01465768e-05 4.16144612e-05]\n  [9.99325097e-01 1.90356714e-05 2.50619942e-05 1.42566452e-04\n   3.00561569e-05 3.14153331e-05 2.71850207e-04 2.97435236e-05\n   4.79592054e-05 3.80053425e-05 3.92370457e-05]\n  [9.99338448e-01 1.87542300e-05 2.46362470e-05 1.40010321e-04\n   2.97027054e-05 3.07998307e-05 2.66971503e-04 2.89592990e-05\n   4.69773513e-05 3.69136505e-05 3.78650802e-05]\n  [9.99346316e-01 1.85515146e-05 2.44301991e-05 1.38559961e-04\n   2.95282825e-05 3.03640300e-05 2.64368777e-04 2.84349026e-05\n   4.64199220e-05 3.61933344e-05 3.68227011e-05]\n  [9.99346554e-01 1.87351270e-05 2.47434164e-05 1.37690862e-04\n   2.76790888e-05 2.77248455e-05 2.71513272e-04 2.69272787e-05\n   4.75812412e-05 3.41911109e-05 3.66484892e-05]]]\nOutput 1: shape = (1, 40, 11)\n[[[1.30973041e-01 8.51875767e-02 8.69467109e-02 8.09115916e-02\n   1.00350887e-01 8.52204636e-02 8.16077888e-02 9.41621736e-02\n   9.24332440e-02 8.53984281e-02 7.68080428e-02]\n  [2.23631322e-01 7.43104964e-02 8.02435875e-02 6.47158474e-02\n   9.94503349e-02 7.62996301e-02 6.52166605e-02 9.30531174e-02\n   8.21825266e-02 7.92628452e-02 6.16335422e-02]\n  [4.42154199e-01 5.05208336e-02 6.11108430e-02 3.96531038e-02\n   7.83633441e-02 5.58962487e-02 3.91241945e-02 7.44016171e-02\n   5.54379895e-02 6.34755567e-02 3.98621149e-02]\n  [7.51515210e-01 2.03423910e-02 2.87303794e-02 1.43346656e-02\n   3.81811149e-02 2.46603712e-02 1.33830439e-02 3.65208164e-02\n   2.29380019e-02 3.40028182e-02 1.53911794e-02]\n  [9.29340780e-01 5.00863371e-03 8.33589677e-03 3.21287662e-03\n   1.14618978e-02 6.43077446e-03 2.74554803e-03 1.13176927e-02\n   6.12032367e-03 1.22331725e-02 3.79233388e-03]\n  [9.80060339e-01 1.23736367e-03 2.43568304e-03 7.52945780e-04\n   3.20223137e-03 1.51776674e-03 5.87121060e-04 3.31491162e-03\n   1.57640316e-03 4.32415633e-03 9.90988454e-04]\n  [9.92276669e-01 4.46680788e-04 9.93567752e-04 2.65863200e-04\n   1.17967988e-03 4.77248715e-04 1.92034699e-04 1.26929907e-03\n   5.39968314e-04 1.98077527e-03 3.78161989e-04]\n  [9.95672107e-01 2.51030520e-04 5.93399513e-04 1.44696271e-04\n   6.22507534e-04 2.24365955e-04 1.00237929e-04 6.80011581e-04\n   2.66005227e-04 1.22928503e-03 2.16236789e-04]\n  [9.96836364e-01 1.91434083e-04 4.57676128e-04 1.04226026e-04\n   4.34199756e-04 1.45047670e-04 7.15659698e-05 4.73880500e-04\n   1.73414664e-04 9.50623304e-04 1.61442425e-04]\n  [9.97296035e-01 1.70835468e-04 4.06932202e-04 8.80589141e-05\n   3.59915633e-04 1.15223585e-04 6.05407622e-05 3.90101311e-04\n   1.36216986e-04 8.35855841e-04 1.40296179e-04]\n  [9.97477472e-01 1.65036909e-04 3.88557208e-04 8.12012076e-05\n   3.29408242e-04 1.03207254e-04 5.63268986e-05 3.53455718e-04\n   1.19936791e-04 7.92972336e-04 1.32484216e-04]\n  [9.97552454e-01 1.63388322e-04 3.82187252e-04 7.82692223e-05\n   3.15026933e-04 9.78154349e-05 5.44455543e-05 3.36119905e-04\n   1.12514470e-04 7.78297253e-04 1.29425825e-04]\n  [9.97596502e-01 1.62292272e-04 3.79325269e-04 7.65236837e-05\n   3.07358394e-04 9.47996741e-05 5.34493556e-05 3.26385052e-04\n   1.08164008e-04 7.67265796e-04 1.27824955e-04]\n  [9.97628331e-01 1.60813914e-04 3.75255651e-04 7.56563531e-05\n   3.02410772e-04 9.30512761e-05 5.24761417e-05 3.20605439e-04\n   1.05578154e-04 7.58793089e-04 1.26927669e-04]\n  [9.97638226e-01 1.61112242e-04 3.74521100e-04 7.54083012e-05\n   3.00456944e-04 9.22214022e-05 5.19127934e-05 3.17565893e-04\n   1.04237770e-04 7.57442089e-04 1.27018415e-04]\n  [9.97643352e-01 1.60772790e-04 3.74886440e-04 7.55782385e-05\n   2.99798470e-04 9.19226295e-05 5.14316635e-05 3.16477293e-04\n   1.03536659e-04 7.56095338e-04 1.26208019e-04]\n  [9.97629404e-01 1.61473974e-04 3.77111894e-04 7.57966845e-05\n   3.02907865e-04 9.26762223e-05 5.14962703e-05 3.17455299e-04\n   1.03740000e-04 7.60994095e-04 1.26840212e-04]\n  [9.97624338e-01 1.60881842e-04 3.76013428e-04 7.58161041e-05\n   3.05977184e-04 9.31334725e-05 5.13921950e-05 3.17815662e-04\n   1.03770937e-04 7.63789401e-04 1.26992672e-04]\n  [9.97634888e-01 1.58398951e-04 3.72812909e-04 7.53042041e-05\n   3.07674956e-04 9.33806296e-05 5.07337718e-05 3.18058883e-04\n   1.03677376e-04 7.59092625e-04 1.25986640e-04]\n  [9.97651219e-01 1.56307404e-04 3.70104361e-04 7.48217353e-05\n   3.09111987e-04 9.36834695e-05 4.99738126e-05 3.18335689e-04\n   1.03245511e-04 7.48394697e-04 1.24792161e-04]\n  [9.97642517e-01 1.56444279e-04 3.71578732e-04 7.52455526e-05\n   3.12731194e-04 9.50428512e-05 4.99477392e-05 3.20289109e-04\n   1.04162100e-04 7.46475649e-04 1.25676073e-04]\n  [9.97615397e-01 1.58244104e-04 3.73638497e-04 7.60795738e-05\n   3.18356702e-04 9.67545493e-05 5.01268551e-05 3.23824672e-04\n   1.05908563e-04 7.54494162e-04 1.27162042e-04]\n  [9.97566521e-01 1.61713455e-04 3.80367128e-04 7.80243863e-05\n   3.26015637e-04 9.96165181e-05 5.07019504e-05 3.29915230e-04\n   1.08469503e-04 7.68859289e-04 1.29794018e-04]\n  [9.97515261e-01 1.66004029e-04 3.85456689e-04 8.01525166e-05\n   3.33810895e-04 1.02308986e-04 5.16829350e-05 3.35535675e-04\n   1.11324800e-04 7.86044402e-04 1.32460176e-04]\n  [9.97482896e-01 1.68119950e-04 3.89238237e-04 8.12781218e-05\n   3.39278864e-04 1.03842351e-04 5.17608678e-05 3.40791012e-04\n   1.12982401e-04 7.95680273e-04 1.34033849e-04]\n  [9.97458518e-01 1.69359118e-04 3.92087561e-04 8.23128867e-05\n   3.43735301e-04 1.05452425e-04 5.19516288e-05 3.45338747e-04\n   1.14452130e-04 8.01490678e-04 1.35242532e-04]\n  [9.97437239e-01 1.70455722e-04 3.94769711e-04 8.30635036e-05\n   3.47238209e-04 1.07088505e-04 5.21466391e-05 3.48332076e-04\n   1.15760362e-04 8.07453878e-04 1.36464354e-04]\n  [9.97431219e-01 1.69402454e-04 3.94426577e-04 8.31141879e-05\n   3.49996233e-04 1.07781190e-04 5.19237583e-05 3.50929040e-04\n   1.16815427e-04 8.06920172e-04 1.37333816e-04]\n  [9.97433364e-01 1.67521532e-04 3.93563969e-04 8.30418721e-05\n   3.51547496e-04 1.08272419e-04 5.17230183e-05 3.51808092e-04\n   1.17357689e-04 8.04398675e-04 1.37306750e-04]\n  [9.97443795e-01 1.65966645e-04 3.91997426e-04 8.25050884e-05\n   3.50833405e-04 1.08087021e-04 5.16296750e-05 3.51067371e-04\n   1.17101998e-04 7.99731177e-04 1.37125899e-04]\n  [9.97457385e-01 1.64078025e-04 3.89322202e-04 8.21004141e-05\n   3.49425914e-04 1.07683838e-04 5.13928608e-05 3.49875423e-04\n   1.16808689e-04 7.95358734e-04 1.36749033e-04]\n  [9.97468650e-01 1.62757206e-04 3.87739594e-04 8.16599131e-05\n   3.49008740e-04 1.07263084e-04 5.11707658e-05 3.48351663e-04\n   1.16499948e-04 7.90473656e-04 1.36439630e-04]\n  [9.97469246e-01 1.61984601e-04 3.86876782e-04 8.17716355e-05\n   3.49011592e-04 1.07274507e-04 5.11266044e-05 3.47741123e-04\n   1.16731229e-04 7.91224593e-04 1.36846415e-04]\n  [9.97468412e-01 1.60836440e-04 3.86157335e-04 8.15795647e-05\n   3.50334216e-04 1.07403393e-04 5.12155748e-05 3.48254252e-04\n   1.16846299e-04 7.91883736e-04 1.37099458e-04]\n  [9.97481108e-01 1.58263705e-04 3.85231833e-04 8.09991034e-05\n   3.51468450e-04 1.07385400e-04 5.08602243e-05 3.47211579e-04\n   1.16831077e-04 7.84086704e-04 1.36555231e-04]\n  [9.97666419e-01 1.48366598e-04 3.54944932e-04 7.81355484e-05\n   3.25895031e-04 9.53835552e-05 5.19625573e-05 3.10327130e-04\n   1.08104548e-04 7.34256115e-04 1.26164246e-04]\n  [9.97745752e-01 1.42427787e-04 3.43733787e-04 7.57043090e-05\n   3.17809783e-04 9.23642583e-05 5.03497868e-05 3.02239903e-04\n   1.04199709e-04 7.02207675e-04 1.23224410e-04]\n  [9.97779787e-01 1.39081720e-04 3.39556951e-04 7.39104798e-05\n   3.16651596e-04 9.14712582e-05 4.97267465e-05 2.99232721e-04\n   1.02907376e-04 6.85647887e-04 1.22128287e-04]\n  [9.97800708e-01 1.35587688e-04 3.37397767e-04 7.27422666e-05\n   3.18251638e-04 9.12778851e-05 4.92165018e-05 2.98388943e-04\n   1.02394304e-04 6.72703085e-04 1.21455196e-04]\n  [9.97849822e-01 1.29460328e-04 3.25591769e-04 7.72236817e-05\n   3.19815968e-04 8.89079893e-05 4.60276569e-05 2.88726646e-04\n   9.93250651e-05 6.55023032e-04 1.20120094e-04]]]\nOutput 2: shape = (1, 40, 11)\n[[[1.19544022e-01 9.96784046e-02 9.00234058e-02 9.73942652e-02\n   9.31940228e-02 8.48652422e-02 7.57332295e-02 1.11168534e-01\n   6.94374442e-02 6.88772723e-02 9.00842249e-02]\n  [1.78217709e-01 9.92443487e-02 8.34456831e-02 1.00786485e-01\n   9.34138894e-02 7.64753893e-02 6.08774871e-02 1.18784279e-01\n   5.38843870e-02 4.71759737e-02 8.76942649e-02]\n  [3.28300327e-01 8.00733417e-02 6.68199360e-02 9.22655314e-02\n   8.80051702e-02 6.17323704e-02 4.27946746e-02 9.97459143e-02\n   4.03361544e-02 2.70699672e-02 7.28565305e-02]\n  [6.26626670e-01 4.18199934e-02 3.46593782e-02 5.71058504e-02\n   6.04264438e-02 3.43971848e-02 2.10593194e-02 5.16868792e-02\n   2.28221007e-02 1.05268536e-02 3.88693400e-02]\n  [8.85722756e-01 1.19615393e-02 9.47886705e-03 1.81249585e-02\n   2.28596404e-02 1.02299443e-02 6.18735841e-03 1.38819823e-02\n   7.77250715e-03 2.38800771e-03 1.13924220e-02]\n  [9.70905721e-01 2.95210327e-03 2.19888985e-03 4.39866679e-03\n   6.82084356e-03 2.44644121e-03 1.62090675e-03 3.05138808e-03\n   2.21068785e-03 4.90128703e-04 2.90414086e-03]\n  [9.89992380e-01 1.00306969e-03 7.47006445e-04 1.40490464e-03\n   2.55894265e-03 7.86143413e-04 5.85598580e-04 9.11593204e-04\n   8.14243685e-04 1.47208790e-04 1.04888622e-03]\n  [9.94836271e-01 5.09729784e-04 4.04062448e-04 6.83671446e-04\n   1.35195162e-03 3.91932874e-04 3.14851815e-04 4.23542893e-04\n   4.36392438e-04 7.05758066e-05 5.77036524e-04]\n  [9.96423185e-01 3.47369845e-04 2.96512560e-04 4.60479350e-04\n   9.26633191e-04 2.69836950e-04 2.25224052e-04 2.73558515e-04\n   3.09931871e-04 4.71290041e-05 4.20246564e-04]\n  [9.97074783e-01 2.77911342e-04 2.54894228e-04 3.74545227e-04\n   7.42803968e-04 2.21650131e-04 1.87658690e-04 2.15556967e-04\n   2.57832959e-04 3.75436939e-05 3.54732882e-04]\n  [9.97356057e-01 2.48684635e-04 2.38589986e-04 3.40026687e-04\n   6.56421005e-04 2.01118601e-04 1.71233653e-04 1.91446787e-04\n   2.35782878e-04 3.33228236e-05 3.27331509e-04]\n  [9.97491837e-01 2.33763727e-04 2.31541009e-04 3.24150722e-04\n   6.11850934e-04 1.92029678e-04 1.63228877e-04 1.80669798e-04\n   2.25131065e-04 3.12345255e-05 3.14503181e-04]\n  [9.97568667e-01 2.24555712e-04 2.28630728e-04 3.14320816e-04\n   5.86243463e-04 1.87366371e-04 1.58522467e-04 1.74629837e-04\n   2.19586407e-04 3.00401480e-05 3.07422219e-04]\n  [9.97609317e-01 2.19314912e-04 2.27533092e-04 3.10204050e-04\n   5.71620010e-04 1.84533783e-04 1.56169728e-04 1.71172622e-04\n   2.16564353e-04 2.95799400e-05 3.03905777e-04]\n  [9.97644067e-01 2.15306020e-04 2.25415512e-04 3.07106122e-04\n   5.58933360e-04 1.82484728e-04 1.53624773e-04 1.68652856e-04\n   2.14190470e-04 2.91400574e-05 3.01127671e-04]\n  [9.97654617e-01 2.13704479e-04 2.24660311e-04 3.07551352e-04\n   5.53353864e-04 1.82276548e-04 1.52585038e-04 1.67797771e-04\n   2.13490785e-04 2.90643184e-05 3.00809974e-04]\n  [9.97658193e-01 2.13371866e-04 2.24054311e-04 3.08898714e-04\n   5.51481964e-04 1.81360301e-04 1.51741871e-04 1.67695660e-04\n   2.12865591e-04 2.87889707e-05 3.01534252e-04]\n  [9.97664154e-01 2.12639818e-04 2.23349067e-04 3.09783267e-04\n   5.50691329e-04 1.80305593e-04 1.50290056e-04 1.67659891e-04\n   2.11406601e-04 2.84015532e-05 3.01380816e-04]\n  [9.97671664e-01 2.11965380e-04 2.23913361e-04 3.10912670e-04\n   5.48808370e-04 1.77278096e-04 1.49635031e-04 1.66512269e-04\n   2.09477730e-04 2.81637622e-05 3.01745167e-04]\n  [9.97689128e-01 2.09980426e-04 2.22806208e-04 3.10779957e-04\n   5.44288429e-04 1.74655259e-04 1.48153646e-04 1.64694866e-04\n   2.08165409e-04 2.78175012e-05 2.99529289e-04]\n  [9.97689843e-01 2.09125850e-04 2.22424220e-04 3.12009040e-04\n   5.45583433e-04 1.73855369e-04 1.46928782e-04 1.65060410e-04\n   2.07831123e-04 2.76875708e-05 2.99716950e-04]\n  [9.97679889e-01 2.10670478e-04 2.22354560e-04 3.15564830e-04\n   5.47165109e-04 1.73561901e-04 1.46652514e-04 1.66344602e-04\n   2.08872822e-04 2.76150386e-05 3.01193068e-04]\n  [9.97659624e-01 2.12592422e-04 2.22733972e-04 3.20479448e-04\n   5.51532139e-04 1.75189227e-04 1.47073632e-04 1.69201783e-04\n   2.10592494e-04 2.77814725e-05 3.03249981e-04]\n  [9.97625768e-01 2.15796244e-04 2.24790550e-04 3.26484937e-04\n   5.59350243e-04 1.78443181e-04 1.47945728e-04 1.73437162e-04\n   2.12762476e-04 2.79924880e-05 3.07226728e-04]\n  [9.97604668e-01 2.17664259e-04 2.25713782e-04 3.31661373e-04\n   5.64314891e-04 1.80023213e-04 1.48859690e-04 1.75354260e-04\n   2.14311338e-04 2.81920729e-05 3.09175113e-04]\n  [9.97580290e-01 2.19742942e-04 2.27214012e-04 3.36550584e-04\n   5.71269076e-04 1.81114010e-04 1.49885396e-04 1.77332186e-04\n   2.15920620e-04 2.84716516e-05 3.12200951e-04]\n  [9.97561514e-01 2.21274779e-04 2.28715886e-04 3.41033417e-04\n   5.74939826e-04 1.82216609e-04 1.50544991e-04 1.78915958e-04\n   2.17334164e-04 2.86447939e-05 3.14784411e-04]\n  [9.97555077e-01 2.19992580e-04 2.30569363e-04 3.43013729e-04\n   5.77126921e-04 1.81511103e-04 1.50862223e-04 1.79123250e-04\n   2.17286957e-04 2.86673439e-05 3.16587539e-04]\n  [9.97557700e-01 2.19149268e-04 2.30668273e-04 3.44024156e-04\n   5.76459570e-04 1.80286021e-04 1.50394742e-04 1.78649279e-04\n   2.16744680e-04 2.85813767e-05 3.17312311e-04]\n  [9.97564971e-01 2.18069879e-04 2.30165228e-04 3.43471154e-04\n   5.75687736e-04 1.78631337e-04 1.50020089e-04 1.77323716e-04\n   2.16414948e-04 2.85196675e-05 3.16756166e-04]\n  [9.97577071e-01 2.16251079e-04 2.29475190e-04 3.42270825e-04\n   5.73306228e-04 1.77018344e-04 1.49081461e-04 1.75478548e-04\n   2.15575696e-04 2.84019443e-05 3.16016260e-04]\n  [9.97585297e-01 2.14817104e-04 2.29213096e-04 3.41490988e-04\n   5.71611454e-04 1.76271846e-04 1.48428138e-04 1.74360655e-04\n   2.14944157e-04 2.82924739e-05 3.15198064e-04]\n  [9.97587323e-01 2.13640538e-04 2.29418249e-04 3.41581937e-04\n   5.71986951e-04 1.76264293e-04 1.47736719e-04 1.73799723e-04\n   2.14827785e-04 2.83014379e-05 3.15201411e-04]\n  [9.97592092e-01 2.12318904e-04 2.29379541e-04 3.42181360e-04\n   5.71694633e-04 1.75365087e-04 1.47241895e-04 1.72756161e-04\n   2.13288571e-04 2.82630026e-05 3.15377023e-04]\n  [9.97609079e-01 2.09251550e-04 2.29341007e-04 3.40653758e-04\n   5.67380397e-04 1.73785433e-04 1.45718383e-04 1.71242631e-04\n   2.10523984e-04 2.79809701e-05 3.14916833e-04]\n  [9.97712195e-01 2.08140453e-04 2.24650139e-04 3.05340945e-04\n   5.45460614e-04 1.68806291e-04 1.37400551e-04 1.59324496e-04\n   2.03939926e-04 2.72028319e-05 3.07570881e-04]\n  [9.97786582e-01 1.98041060e-04 2.21137132e-04 2.94557627e-04\n   5.32622391e-04 1.63191959e-04 1.32363726e-04 1.52680033e-04\n   1.94398861e-04 2.61570185e-05 2.98236206e-04]\n  [9.97830927e-01 1.92659470e-04 2.18434667e-04 2.88786978e-04\n   5.24927280e-04 1.59234696e-04 1.29019230e-04 1.48589461e-04\n   1.89231534e-04 2.54633396e-05 2.92747573e-04]\n  [9.97849345e-01 1.89852886e-04 2.18193003e-04 2.86379363e-04\n   5.23359922e-04 1.56989699e-04 1.27487088e-04 1.46453225e-04\n   1.86291625e-04 2.51191923e-05 2.90482509e-04]\n  [9.97822464e-01 1.87633646e-04 2.12612722e-04 2.97082472e-04\n   5.34247374e-04 1.67784383e-04 1.24237864e-04 1.46099832e-04\n   1.79180148e-04 2.66835468e-05 3.01909953e-04]]]\nOutput 3: shape = (1, 40, 11)\n[[[1.44012272e-01 7.52104893e-02 9.90584046e-02 8.37887749e-02\n   1.10703431e-01 7.95922577e-02 6.84412420e-02 9.19610336e-02\n   8.88698250e-02 7.50583038e-02 8.33040401e-02]\n  [2.49174759e-01 6.13768436e-02 9.38199237e-02 7.28940293e-02\n   1.16972685e-01 6.50891140e-02 4.88715060e-02 8.58548805e-02\n   7.75635839e-02 5.93393184e-02 6.90433234e-02]\n  [4.64377344e-01 4.41958122e-02 6.79792017e-02 5.27332909e-02\n   9.36607793e-02 4.33908962e-02 2.77490243e-02 6.53032511e-02\n   5.53940944e-02 3.86126973e-02 4.66035716e-02]\n  [7.51137137e-01 2.29236409e-02 3.14193442e-02 2.54555233e-02\n   4.49763425e-02 1.83529500e-02 9.70375072e-03 3.17829289e-02\n   2.64996756e-02 1.67027749e-02 2.10460499e-02]\n  [9.24511671e-01 8.30488745e-03 9.65765025e-03 8.02447088e-03\n   1.26406671e-02 4.90827439e-03 2.16284278e-03 9.99906193e-03\n   8.50711670e-03 4.87149460e-03 6.41183695e-03]\n  [9.78048861e-01 2.82082357e-03 2.90273782e-03 2.30166223e-03\n   3.20778391e-03 1.23702583e-03 4.91411483e-04 3.03821173e-03\n   2.61229347e-03 1.39581633e-03 1.94336532e-03]\n  [9.91626799e-01 1.16883800e-03 1.16090407e-03 8.24314309e-04\n   1.09527120e-03 4.15423594e-04 1.64144338e-04 1.20735983e-03\n   1.02846231e-03 5.34303370e-04 7.74225395e-04]\n  [9.95472372e-01 6.48798305e-04 6.56107266e-04 4.11753688e-04\n   5.67976851e-04 2.04929631e-04 8.49004355e-05 6.68822322e-04\n   5.66212053e-04 2.91123462e-04 4.26969113e-04]\n  [9.96821404e-01 4.55807167e-04 4.74059168e-04 2.70724937e-04\n   3.99537385e-04 1.36739021e-04 6.00465391e-05 4.71710489e-04\n   4.02656209e-04 2.06132740e-04 3.01239401e-04]\n  [9.97360587e-01 3.74038034e-04 3.98239994e-04 2.14786734e-04\n   3.38240498e-04 1.11151392e-04 5.09710953e-05 3.94206319e-04\n   3.34512122e-04 1.73279172e-04 2.49994075e-04]\n  [9.97595549e-01 3.35636287e-04 3.65308806e-04 1.89676299e-04\n   3.13976663e-04 1.00526719e-04 4.71150743e-05 3.59725498e-04\n   3.06261180e-04 1.58712486e-04 2.27561861e-04]\n  [9.97701526e-01 3.17709346e-04 3.49249865e-04 1.78288159e-04\n   3.04384652e-04 9.57598822e-05 4.55228474e-05 3.44021391e-04\n   2.93551886e-04 1.52278488e-04 2.17726076e-04]\n  [9.97756481e-01 3.07887705e-04 3.40635219e-04 1.71942025e-04\n   3.00535583e-04 9.32866169e-05 4.47106868e-05 3.36259836e-04\n   2.86896131e-04 1.48667386e-04 2.12540879e-04]\n  [9.97802377e-01 3.00494372e-04 3.32809519e-04 1.67552978e-04\n   2.95403996e-04 9.15536148e-05 4.40124422e-05 3.29377945e-04\n   2.80642067e-04 1.46767750e-04 2.09099162e-04]\n  [9.97823894e-01 2.95522797e-04 3.29147966e-04 1.64956218e-04\n   2.93825433e-04 9.07378126e-05 4.37562558e-05 3.25640140e-04\n   2.78869382e-04 1.45709171e-04 2.07822173e-04]\n  [9.97834027e-01 2.94666330e-04 3.26487061e-04 1.64513433e-04\n   2.92199285e-04 9.02268584e-05 4.38144089e-05 3.23714688e-04\n   2.77374667e-04 1.45618833e-04 2.07414239e-04]\n  [9.97836769e-01 2.92666111e-04 3.26010369e-04 1.63835939e-04\n   2.92609999e-04 9.07255744e-05 4.37571143e-05 3.22655949e-04\n   2.76120205e-04 1.46407649e-04 2.08384532e-04]\n  [9.97831881e-01 2.91718636e-04 3.26888869e-04 1.64047451e-04\n   2.92316807e-04 9.16128411e-05 4.38358365e-05 3.23996908e-04\n   2.76075152e-04 1.47317041e-04 2.10248152e-04]\n  [9.97836411e-01 2.91283766e-04 3.24779801e-04 1.63675490e-04\n   2.90480966e-04 9.20716848e-05 4.36052978e-05 3.24649154e-04\n   2.74643389e-04 1.47680912e-04 2.10685044e-04]\n  [9.97831285e-01 2.91650591e-04 3.24705994e-04 1.64599449e-04\n   2.89626099e-04 9.25574423e-05 4.35388392e-05 3.27361020e-04\n   2.74578313e-04 1.48152991e-04 2.11830542e-04]\n  [9.97819364e-01 2.92539888e-04 3.25273315e-04 1.65842561e-04\n   2.89920834e-04 9.36528158e-05 4.36807168e-05 3.29721719e-04\n   2.77082960e-04 1.49315762e-04 2.13676991e-04]\n  [9.97786820e-01 2.95736507e-04 3.29210132e-04 1.68179045e-04\n   2.92676268e-04 9.60743346e-05 4.42870769e-05 3.33850825e-04\n   2.83721660e-04 1.51745087e-04 2.17680747e-04]\n  [9.97735262e-01 3.01664841e-04 3.34368873e-04 1.72487576e-04\n   2.98949250e-04 9.93964786e-05 4.53237735e-05 3.40073748e-04\n   2.92730954e-04 1.56496433e-04 2.23358598e-04]\n  [9.97676075e-01 3.08192190e-04 3.41745093e-04 1.76843881e-04\n   3.06146278e-04 1.03278617e-04 4.65496778e-05 3.46626126e-04\n   3.02857894e-04 1.61735225e-04 2.29960686e-04]\n  [9.97632146e-01 3.13614699e-04 3.46537156e-04 1.80282266e-04\n   3.09807016e-04 1.06112340e-04 4.76884779e-05 3.53267533e-04\n   3.09309835e-04 1.66023950e-04 2.35153930e-04]\n  [9.97598171e-01 3.18576291e-04 3.50988470e-04 1.82634583e-04\n   3.12752207e-04 1.08161708e-04 4.83813055e-05 3.57326178e-04\n   3.15404817e-04 1.69254301e-04 2.38230859e-04]\n  [9.97574210e-01 3.22217151e-04 3.52341449e-04 1.84941135e-04\n   3.14567849e-04 1.09770524e-04 4.87922480e-05 3.60727427e-04\n   3.18995764e-04 1.71618711e-04 2.41826914e-04]\n  [9.97566164e-01 3.22839245e-04 3.51655675e-04 1.86165242e-04\n   3.15250654e-04 1.10441455e-04 4.87120742e-05 3.63895058e-04\n   3.19839426e-04 1.72506043e-04 2.42423936e-04]\n  [9.97559369e-01 3.24066612e-04 3.51285562e-04 1.87479542e-04\n   3.15492100e-04 1.11037181e-04 4.86895842e-05 3.66308290e-04\n   3.19769228e-04 1.72929824e-04 2.43525486e-04]\n  [9.97572482e-01 3.23150598e-04 3.48875183e-04 1.86787249e-04\n   3.12822289e-04 1.10350666e-04 4.82840951e-05 3.65927292e-04\n   3.17212311e-04 1.71595500e-04 2.42454611e-04]\n  [9.97587204e-01 3.21446394e-04 3.46499757e-04 1.86112724e-04\n   3.10076226e-04 1.09818655e-04 4.78906877e-05 3.64899082e-04\n   3.14239151e-04 1.70524596e-04 2.41400092e-04]\n  [9.97600019e-01 3.19660001e-04 3.44149477e-04 1.85889643e-04\n   3.08200717e-04 1.09281180e-04 4.75943489e-05 3.63815634e-04\n   3.11831012e-04 1.69234278e-04 2.40256428e-04]\n  [9.97603834e-01 3.18794482e-04 3.43858846e-04 1.85827783e-04\n   3.07379407e-04 1.09196066e-04 4.72834072e-05 3.63713625e-04\n   3.11361771e-04 1.68704762e-04 2.39880057e-04]\n  [9.97617424e-01 3.16873455e-04 3.41804669e-04 1.85432713e-04\n   3.05819558e-04 1.08608678e-04 4.68805301e-05 3.63306404e-04\n   3.07185779e-04 1.67916674e-04 2.38633147e-04]\n  [9.97627020e-01 3.15890473e-04 3.39718215e-04 1.85285302e-04\n   3.05303227e-04 1.08054308e-04 4.64333534e-05 3.63645609e-04\n   3.03810317e-04 1.67499267e-04 2.37339264e-04]\n  [9.97861743e-01 2.87084607e-04 3.14683741e-04 1.61177071e-04\n   2.67549418e-04 9.81885460e-05 4.46588092e-05 3.38603364e-04\n   2.48687138e-04 1.55989517e-04 2.21661612e-04]\n  [9.97942984e-01 2.69211363e-04 3.05264228e-04 1.55236266e-04\n   2.58202199e-04 9.42006445e-05 4.29946485e-05 3.31730495e-04\n   2.35426269e-04 1.50726701e-04 2.14163127e-04]\n  [9.97988284e-01 2.58949556e-04 3.00046871e-04 1.52596476e-04\n   2.52683618e-04 9.23899715e-05 4.20059514e-05 3.29551811e-04\n   2.26039730e-04 1.46696155e-04 2.10669692e-04]\n  [9.98013735e-01 2.52794183e-04 2.95858510e-04 1.51419750e-04\n   2.50524725e-04 9.15357305e-05 4.13210728e-05 3.29291273e-04\n   2.19899026e-04 1.44403079e-04 2.09138278e-04]\n  [9.98050272e-01 2.56283703e-04 2.91052682e-04 1.49240528e-04\n   2.37210683e-04 8.78233041e-05 4.10987268e-05 3.10074014e-04\n   2.20418122e-04 1.44254067e-04 2.12099811e-04]]]\nOutput 4: shape = (1, 40, 26)\n[[[4.78045270e-02 3.33458371e-02 3.34787332e-02 ... 3.41116451e-02\n   3.40394229e-02 3.52394618e-02]\n  [6.86707050e-02 2.80850399e-02 2.89305039e-02 ... 2.64736246e-02\n   2.89046634e-02 3.11526861e-02]\n  [1.18656814e-01 2.13987771e-02 2.45582405e-02 ... 1.69487204e-02\n   2.18855143e-02 2.44297236e-02]\n  ...\n  [6.25105619e-01 5.72147721e-04 2.92754709e-03 ... 2.58533197e-04\n   3.58387973e-04 6.24765584e-04]\n  [6.30235136e-01 5.67683484e-04 2.86996621e-03 ... 2.56029249e-04\n   3.55664029e-04 6.13728771e-04]\n  [6.25046372e-01 6.30087103e-04 2.99651083e-03 ... 2.48581084e-04\n   3.59930535e-04 6.22008636e-04]]]\nOutput 5: shape = (1, 40, 26)\n[[[6.9050692e-02 3.5286237e-02 3.0540613e-02 ... 4.1500460e-02\n   3.3328488e-02 3.6034551e-02]\n  [1.2773487e-01 3.1925526e-02 2.1853969e-02 ... 3.9541829e-02\n   2.8943313e-02 3.0522754e-02]\n  [2.4976145e-01 2.5624506e-02 1.3188244e-02 ... 2.9597834e-02\n   2.2791088e-02 2.1034896e-02]\n  ...\n  [7.9787242e-01 1.3376377e-03 8.4569037e-05 ... 6.4429140e-04\n   7.9574232e-04 3.2899401e-04]\n  [7.9991090e-01 1.3272339e-03 8.3491403e-05 ... 6.3395005e-04\n   7.8845769e-04 3.2917544e-04]\n  [8.0424857e-01 1.3572018e-03 8.2186700e-05 ... 6.3776353e-04\n   7.9420669e-04 3.3775516e-04]]]\nOutput 6: shape = (1, 40, 26)\n[[[6.1819494e-02 3.2156181e-02 3.6213875e-02 ... 4.3850381e-02\n   4.0576506e-02 3.8575772e-02]\n  [1.0283261e-01 2.4558360e-02 3.3157289e-02 ... 4.6011873e-02\n   4.0870406e-02 3.7110452e-02]\n  [1.7575528e-01 1.5701205e-02 2.8616844e-02 ... 4.2955771e-02\n   3.5179939e-02 3.2303553e-02]\n  ...\n  [5.6590265e-01 1.6190736e-04 1.8882724e-03 ... 2.8048614e-03\n   1.8709557e-03 9.8212040e-04]\n  [5.7036984e-01 1.6063619e-04 1.8663515e-03 ... 2.7842124e-03\n   1.8497561e-03 9.6952595e-04]\n  [5.5894679e-01 1.5256123e-04 1.8863310e-03 ... 2.5336454e-03\n   1.7593451e-03 9.5130841e-04]]]\nOutput 7: shape = (1, 40, 26)\n[[[6.3175365e-02 3.5107646e-02 3.9162692e-02 ... 3.3272956e-02\n   3.2410007e-02 3.8315419e-02]\n  [1.1194250e-01 3.0263215e-02 3.7117135e-02 ... 3.1462692e-02\n   2.7673716e-02 3.3860024e-02]\n  [2.3030585e-01 2.2729672e-02 3.1002361e-02 ... 2.9088838e-02\n   2.2874406e-02 2.4625206e-02]\n  ...\n  [9.8302943e-01 1.3504465e-04 3.9297293e-04 ... 6.0883531e-04\n   5.0060562e-04 3.2499636e-05]\n  [9.8317838e-01 1.3293131e-04 3.8898882e-04 ... 5.9881393e-04\n   4.9600779e-04 3.1778014e-05]\n  [9.8304290e-01 1.3693111e-04 4.2091060e-04 ... 6.4460729e-04\n   5.3360965e-04 3.3473534e-05]]]\nOutput 8: shape = (1, 40, 26)\n[[[7.2148301e-02 4.7765449e-02 2.6790243e-02 ... 4.2125732e-02\n   3.4937732e-02 3.9857756e-02]\n  [1.5399984e-01 5.2239157e-02 1.8114345e-02 ... 4.4996910e-02\n   3.2414615e-02 3.5597168e-02]\n  [3.6958268e-01 4.1568059e-02 1.0978133e-02 ... 4.1255523e-02\n   2.6573332e-02 2.3827929e-02]\n  ...\n  [9.9757314e-01 8.3696679e-05 2.1064363e-05 ... 3.5829516e-04\n   1.8228877e-04 2.3124509e-05]\n  [9.9762589e-01 8.2165905e-05 2.0696683e-05 ... 3.5202250e-04\n   1.7680447e-04 2.2533392e-05]\n  [9.9765086e-01 7.8215904e-05 2.0206591e-05 ... 3.3713513e-04\n   1.7356746e-04 2.2534772e-05]]]\nOutput 9: shape = (1, 40, 26)\n[[[6.6560522e-02 2.8904783e-02 3.5288725e-02 ... 3.2371175e-02\n   3.3938754e-02 4.4269331e-02]\n  [1.3657163e-01 2.0646667e-02 3.2442331e-02 ... 2.5801193e-02\n   2.9327737e-02 4.7732197e-02]\n  [3.2801157e-01 1.2383369e-02 2.6493158e-02 ... 1.7798593e-02\n   2.2188097e-02 4.1194968e-02]\n  ...\n  [9.9738127e-01 2.4268327e-05 1.8781648e-04 ... 3.4473305e-05\n   4.9275684e-05 6.7281770e-05]\n  [9.9743479e-01 2.3472958e-05 1.8394247e-04 ... 3.3795837e-05\n   4.8109396e-05 6.5480774e-05]\n  [9.9746650e-01 2.2543461e-05 1.7949173e-04 ... 3.3690747e-05\n   4.8225447e-05 6.0983493e-05]]]\nOutput 10: shape = (1, 40, 26)\n[[[5.7896916e-02 3.6164079e-02 3.4470748e-02 ... 3.7034966e-02\n   3.4494769e-02 3.4473956e-02]\n  [9.6013121e-02 3.1049907e-02 2.9392021e-02 ... 3.5464399e-02\n   3.0413162e-02 3.0918600e-02]\n  [1.9170645e-01 2.2508319e-02 2.2367038e-02 ... 3.0225152e-02\n   2.5542838e-02 2.5681563e-02]\n  ...\n  [9.6565247e-01 1.3768343e-04 1.7621873e-04 ... 2.0697055e-04\n   7.9676468e-04 5.0117431e-04]\n  [9.6590054e-01 1.3371454e-04 1.7434143e-04 ... 2.0736754e-04\n   7.9286244e-04 4.9850723e-04]\n  [9.6727055e-01 1.2527178e-04 1.7022165e-04 ... 1.9566438e-04\n   7.1515585e-04 4.5741000e-04]]]\nOutput 11: shape = (1, 40, 26)\n[[[6.0947903e-02 3.9246798e-02 3.1800047e-02 ... 4.0216025e-02\n   3.9534017e-02 4.0998291e-02]\n  [1.0852384e-01 3.6700618e-02 2.6421931e-02 ... 4.2101342e-02\n   3.9777718e-02 4.1907243e-02]\n  [2.3901877e-01 2.7905164e-02 2.0378802e-02 ... 4.0023547e-02\n   3.4822762e-02 3.7408717e-02]\n  ...\n  [9.9415368e-01 6.0784780e-05 5.8624471e-05 ... 2.8395336e-04\n   1.5256566e-04 2.8232671e-04]\n  [9.9418670e-01 6.0861912e-05 5.8012683e-05 ... 2.8075380e-04\n   1.5026164e-04 2.8249907e-04]\n  [9.9425805e-01 6.3160260e-05 5.8421992e-05 ... 2.7785715e-04\n   1.4406876e-04 2.8881087e-04]]]\nOutput 12: shape = (1, 40, 26)\n[[[6.6126056e-02 3.3197757e-02 3.1588238e-02 ... 3.3795774e-02\n   3.7896305e-02 3.1273410e-02]\n  [1.2572695e-01 2.7233882e-02 2.5238043e-02 ... 2.9092547e-02\n   3.5335850e-02 2.4565609e-02]\n  [2.5949079e-01 1.9213226e-02 1.7421702e-02 ... 2.2879090e-02\n   2.8333858e-02 1.6913369e-02]\n  ...\n  [8.8381076e-01 8.6400352e-05 1.2442494e-04 ... 4.1035531e-04\n   3.4590115e-04 1.2767526e-04]\n  [8.8337952e-01 8.5131389e-05 1.2153988e-04 ... 4.0255906e-04\n   3.4001947e-04 1.2433303e-04]\n  [8.8272578e-01 9.0221569e-05 1.2581336e-04 ... 4.3142313e-04\n   3.6469448e-04 1.3425499e-04]]]\nOutput 13: shape = (1, 40, 26)\n[[[5.15569784e-02 3.53069343e-02 3.51530798e-02 ... 3.28728743e-02\n   3.45979221e-02 3.49514410e-02]\n  [7.84304738e-02 2.90543549e-02 3.08228564e-02 ... 2.59208325e-02\n   3.18296440e-02 2.96053141e-02]\n  [1.41647056e-01 2.02540252e-02 2.32111085e-02 ... 1.84265915e-02\n   2.88258940e-02 2.23703142e-02]\n  ...\n  [8.33420396e-01 2.22813105e-04 1.14638264e-04 ... 3.70491878e-04\n   1.05237262e-03 2.12233965e-04]\n  [8.34641159e-01 2.23328519e-04 1.12318048e-04 ... 3.65285319e-04\n   1.04303227e-03 2.11277511e-04]\n  [8.32837224e-01 2.23228693e-04 1.14923518e-04 ... 3.57100682e-04\n   1.02095900e-03 2.13601132e-04]]]\nOutput 14: shape = (1, 40, 26)\n[[[6.8717368e-02 3.0052518e-02 4.1483968e-02 ... 3.6336709e-02\n   3.9139051e-02 3.3484388e-02]\n  [1.3259666e-01 2.2928564e-02 4.1745182e-02 ... 3.1911526e-02\n   3.5991665e-02 2.8681302e-02]\n  [2.7938908e-01 1.6026007e-02 3.4827795e-02 ... 2.3743128e-02\n   2.6868284e-02 2.1881003e-02]\n  ...\n  [9.6492910e-01 2.3181686e-04 2.5082353e-04 ... 1.8377860e-04\n   1.8934239e-04 3.1472775e-04]\n  [9.6488446e-01 2.3023658e-04 2.4700921e-04 ... 1.8394613e-04\n   1.8998749e-04 3.0897270e-04]\n  [9.6415430e-01 2.2771362e-04 2.5053514e-04 ... 1.7760116e-04\n   1.9011676e-04 3.2829281e-04]]]\nOutput 15: shape = (1, 40, 26)\n[[[6.5456562e-02 3.2606419e-02 3.7071668e-02 ... 4.0416967e-02\n   3.4741849e-02 3.9564271e-02]\n  [1.1958862e-01 2.6133550e-02 3.3519082e-02 ... 3.9204467e-02\n   3.0856397e-02 3.8344458e-02]\n  [2.4210416e-01 1.8166333e-02 2.5695104e-02 ... 3.2629099e-02\n   2.5865227e-02 3.0319519e-02]\n  ...\n  [9.2134577e-01 2.2365920e-04 2.8839830e-04 ... 7.6740928e-04\n   1.1844165e-03 2.9329382e-04]\n  [9.2214823e-01 2.1983685e-04 2.8657942e-04 ... 7.6955283e-04\n   1.1704037e-03 2.8815330e-04]\n  [9.1965783e-01 2.3197892e-04 3.0295397e-04 ... 7.8895281e-04\n   1.2452614e-03 2.9661841e-04]]]\n--- Predictions for Sequence 0, Frame 3 ---\n| Output Name      |   Classes |   Predicted ID |   Max Probability | Prediction Vector (Example)                              |\n|:-----------------|----------:|---------------:|------------------:|:---------------------------------------------------------|\n| Output 1 (Self)  |        11 |              0 |            0.8938 | [0.89376986 0.00591999 0.00432329 0.01846817 0.00589036] |\n| Output 2 (Self)  |        11 |              0 |            0.7515 | [0.7515152  0.02034239 0.02873038 0.01433467 0.03818111] |\n| Output 3 (Self)  |        11 |              0 |            0.6266 | [0.6266267  0.04181999 0.03465938 0.05710585 0.06042644] |\n| Output 4 (Self)  |        11 |              0 |            0.7511 | [0.75113714 0.02292364 0.03141934 0.02545552 0.04497634] |\n| Output 5 (Pair)  |        26 |              0 |            0.2206 | [0.22058597 0.01324038 0.01900049 0.15160339 0.01194399] |\n| Output 6 (Pair)  |        26 |              0 |            0.449  | [0.44896814 0.01560835 0.00588993 0.10709047 0.0094207 ] |\n| Output 7 (Pair)  |        26 |              0 |            0.2859 | [0.28594598 0.00771627 0.0218447  0.09594902 0.01257131] |\n| Output 8 (Pair)  |        26 |              0 |            0.4696 | [0.46964586 0.01239529 0.01976024 0.1273794  0.01229618] |\n| Output 9 (Pair)  |        26 |              0 |            0.7145 | [0.7145167  0.01772531 0.00439577 0.01279641 0.01911327] |\n| Output 10 (Pair) |        26 |              0 |            0.6738 | [0.6738483  0.00469735 0.01407825 0.00840019 0.012545  ] |\n| Output 11 (Pair) |        26 |              0 |            0.409  | [0.40895152 0.01180143 0.01316086 0.07209934 0.01761768] |\n| Output 12 (Pair) |        26 |              0 |            0.5341 | [0.53413683 0.01361158 0.01106526 0.00990799 0.01633856] |\n| Output 13 (Pair) |        26 |              0 |            0.4869 | [0.4868974  0.00976577 0.00837197 0.01175255 0.01443059] |\n| Output 14 (Pair) |        26 |              0 |            0.2795 | [0.27952674 0.0113222  0.01259843 0.06764934 0.02406073] |\n| Output 15 (Pair) |        26 |              0 |            0.5415 | [0.5415034  0.00839474 0.01925848 0.01294337 0.01471638] |\n| Output 16 (Pair) |        26 |              0 |            0.4593 | [0.45934278 0.00943034 0.0140066  0.02213788 0.01518433] |\n\n\n--- Raw Prediction Vector Examples ---\nThese are the probabilities (softmax output) for the first few classes in each output.\nThe full vector length is either 11 or 26.\n\nOutput 1 (11 classes) Vector:\n[0.89376986 0.00591999 0.00432329 0.01846817 0.00589036 0.00844707\n 0.02164305 0.00739216 0.01206669 0.0100717  0.01200764]\n\nOutput 5 (26 classes) Vector:\n[0.22058597 0.01324038 0.01900049 0.15160339 0.01194399 0.08826932\n 0.12610742 0.03680214 0.00979245 0.02424129 0.03623024 0.02671146\n 0.00689333 0.01598468 0.01237685 0.00922995 0.02576888 0.03621756\n 0.02176547 0.01434439 0.03054091 0.01905422 0.0069651  0.00833321\n 0.01271173 0.01528521]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"\n# Predict on validation set\n# y_pred is already a LIST of 16 NumPy arrays (output of model.predict on multi-output model)\ny_pred = model.predict(X_val)\n\nprint(y_pred.size)\n\n# --- FIX START ---\n\n# 1. Convert y_val (list of 16 arrays of shape (N_val, seq_len)) back into a single array:\n# The original shape was (N_val, seq_len, 16)\n# np.stack joins them along a new axis (the last one, axis=-1)\nY_val_stacked = np.stack(y_val, axis=-1)\n\n# 2. Flatten for frame-level metrics:\n# (N_val, seq_len, 16) -> (N_val * seq_len, 16)\ny_val_flat = Y_val_stacked.reshape(-1, Y_val_stacked.shape[-1]) # or just (-1, 16)\n\n\n# 3. Handle y_pred: y_pred is also a list of 16 arrays. Stack and flatten it too.\nY_pred_stacked = np.stack(y_pred, axis=-1)\ny_pred_flat = Y_pred_stacked.reshape(-1, Y_pred_stacked.shape[-1])\n\n# --- FIX END ---\n\nprint(y_pred_flat)\ny_val_df = pd.DataFrame(y_pred_flat)\nnon_zero_rows = y_val_df[(y_val_df != 0).any(axis=1)]\n\n# The check for 'is_binary' needs to be done on the true labels (y_val_flat)\n# if y_val contained integer labels, which it should based on your loss function.\nis_binary = np.isin(y_val_flat, [0, 1]).all() \n\nprint(is_binary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:09:37.199515Z","iopub.execute_input":"2025-12-11T09:09:37.200386Z","iopub.status.idle":"2025-12-11T09:09:38.068420Z","shell.execute_reply.started":"2025-12-11T09:09:37.200358Z","shell.execute_reply":"2025-12-11T09:09:38.067242Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3521507407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# --- FIX START ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'size'","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, accuracy_score\n\n# --- Define the class structure for interpretation ---\n# The first 4 have 11 classes, the next 12 have 26 classes\nNUM_CLASSES_PER_OUTPUT = [11, 11, 11, 11, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\nOUTPUT_NAMES = [\n    'Mouse 1 Self', 'Mouse 2 Self', 'Mouse 3 Self', 'Mouse 4 Self',\n    'Pair 1', 'Pair 2', 'Pair 3', 'Pair 4', 'Pair 5', 'Pair 6', \n    'Pair 7', 'Pair 8', 'Pair 9', 'Pair 10', 'Pair 11', 'Pair 12'\n]\n\n# 1. Predict on validation set\ny_pred_list = model.predict(X_val)\n# y_val is already a list of 16 true label arrays (integer labels)\n\n# --- Loop through each of the 16 outputs ---\nprint(\"\\n--- Detailed Per-Output Validation Metrics ---\")\nprint(\"-\" * 50)\n\nsummary_data = []\n\nfor i in range(len(OUTPUT_NAMES)):\n    output_name = OUTPUT_NAMES[i]\n    n_classes = NUM_CLASSES_PER_OUTPUT[i]\n\n    # A. Extract True Labels and Predictions for this specific output (i)\n    # y_val[i] is shape (N_val, seq_len) - True integer labels\n    # y_pred_list[i] is shape (N_val, seq_len, n_classes) - Predicted probabilities\n\n    y_true_sequence = y_val[i]\n    y_pred_prob_sequence = y_pred_list[i]\n    \n    # B. Flatten the sequences to get frame-level metrics (N_frames,)\n    # True labels are flattened directly\n    y_true_flat = y_true_sequence.flatten()\n    \n    # Predictions need argmax after flattening the N_val, seq_len dimensions\n    y_pred_prob_flat = y_pred_prob_sequence.reshape(-1, n_classes)\n    y_pred_flat = np.argmax(y_pred_prob_flat, axis=-1)\n\n    # C. Calculate Metrics\n    \n    # Accuracy\n    acc = accuracy_score(y_true_flat, y_pred_flat)\n    \n    # Macro F1-Score (Crucial for class imbalance)\n    macro_f1 = f1_score(y_true_flat, y_pred_flat, average='macro', zero_division=0)\n    \n    # Store data for summary table\n    summary_data.append({\n        'Output': output_name,\n        'Classes': n_classes,\n        'Accuracy': f\"{acc:.4f}\",\n        'Macro F1': f\"{macro_f1:.4f}\"\n    })\n    \n    # D. Print Detailed Report\n    print(f\"## {output_name} ({n_classes} Classes)\")\n    print(f\"Frame Accuracy: {acc:.4f} | Macro F1: {macro_f1:.4f}\")\n    \n    # Generate report, limiting classes to the actual number for this output (0 to n_classes-1)\n    report = classification_report(\n        y_true_flat, \n        y_pred_flat, \n        labels=np.arange(n_classes), # Only show classes that exist in this output\n        zero_division=0,\n        output_dict=True\n    )\n    \n    # Print specific metrics for a high-level view (e.g., F1 for classes 0, 1, 2)\n    print(\"Top 3 Class F1-Scores:\")\n    for class_id in range(min(n_classes, 3)):\n        f1 = report[str(class_id)]['f1-score']\n        print(f\"  Class {class_id}: {f1:.4f}\")\n        \n    print(\"-\" * 50)\n\n# --- Print Summary Table ---\nprint(\"\\n\\n### Overall Validation Summary\")\nsummary_df = pd.DataFrame(summary_data)\nprint(summary_df.to_markdown(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T09:20:39.382631Z","iopub.execute_input":"2025-12-11T09:20:39.383007Z","iopub.status.idle":"2025-12-11T09:20:41.018156Z","shell.execute_reply.started":"2025-12-11T09:20:39.382965Z","shell.execute_reply":"2025-12-11T09:20:41.017321Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n\n--- Detailed Per-Output Validation Metrics ---\n--------------------------------------------------\n## Mouse 1 Self (11 Classes)\nFrame Accuracy: 0.9999 | Macro F1: 0.3333\nTop 3 Class F1-Scores:\n  Class 0: 0.9999\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Mouse 2 Self (11 Classes)\nFrame Accuracy: 1.0000 | Macro F1: 1.0000\nTop 3 Class F1-Scores:\n  Class 0: 1.0000\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Mouse 3 Self (11 Classes)\nFrame Accuracy: 1.0000 | Macro F1: 1.0000\nTop 3 Class F1-Scores:\n  Class 0: 1.0000\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Mouse 4 Self (11 Classes)\nFrame Accuracy: 1.0000 | Macro F1: 0.5000\nTop 3 Class F1-Scores:\n  Class 0: 1.0000\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 1 (26 Classes)\nFrame Accuracy: 0.9640 | Macro F1: 0.1963\nTop 3 Class F1-Scores:\n  Class 0: 0.9817\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 2 (26 Classes)\nFrame Accuracy: 0.9721 | Macro F1: 0.1972\nTop 3 Class F1-Scores:\n  Class 0: 0.9858\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 3 (26 Classes)\nFrame Accuracy: 0.9528 | Macro F1: 0.1952\nTop 3 Class F1-Scores:\n  Class 0: 0.9758\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 4 (26 Classes)\nFrame Accuracy: 0.9972 | Macro F1: 0.2497\nTop 3 Class F1-Scores:\n  Class 0: 0.9986\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 5 (26 Classes)\nFrame Accuracy: 0.9999 | Macro F1: 0.5000\nTop 3 Class F1-Scores:\n  Class 0: 0.9999\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 6 (26 Classes)\nFrame Accuracy: 0.9990 | Macro F1: 0.3332\nTop 3 Class F1-Scores:\n  Class 0: 0.9995\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 7 (26 Classes)\nFrame Accuracy: 0.9970 | Macro F1: 0.2496\nTop 3 Class F1-Scores:\n  Class 0: 0.9985\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 8 (26 Classes)\nFrame Accuracy: 0.9981 | Macro F1: 0.4995\nTop 3 Class F1-Scores:\n  Class 0: 0.9990\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 9 (26 Classes)\nFrame Accuracy: 0.9920 | Macro F1: 0.3320\nTop 3 Class F1-Scores:\n  Class 0: 0.9960\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 10 (26 Classes)\nFrame Accuracy: 0.9805 | Macro F1: 0.1650\nTop 3 Class F1-Scores:\n  Class 0: 0.9902\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 11 (26 Classes)\nFrame Accuracy: 0.9958 | Macro F1: 0.4990\nTop 3 Class F1-Scores:\n  Class 0: 0.9979\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n## Pair 12 (26 Classes)\nFrame Accuracy: 0.9921 | Macro F1: 0.3320\nTop 3 Class F1-Scores:\n  Class 0: 0.9960\n  Class 1: 0.0000\n  Class 2: 0.0000\n--------------------------------------------------\n\n\n### Overall Validation Summary\n| Output       |   Classes |   Accuracy |   Macro F1 |\n|:-------------|----------:|-----------:|-----------:|\n| Mouse 1 Self |        11 |     0.9999 |     0.3333 |\n| Mouse 2 Self |        11 |     1      |     1      |\n| Mouse 3 Self |        11 |     1      |     1      |\n| Mouse 4 Self |        11 |     1      |     0.5    |\n| Pair 1       |        26 |     0.964  |     0.1963 |\n| Pair 2       |        26 |     0.9721 |     0.1972 |\n| Pair 3       |        26 |     0.9528 |     0.1952 |\n| Pair 4       |        26 |     0.9972 |     0.2497 |\n| Pair 5       |        26 |     0.9999 |     0.5    |\n| Pair 6       |        26 |     0.999  |     0.3332 |\n| Pair 7       |        26 |     0.997  |     0.2496 |\n| Pair 8       |        26 |     0.9981 |     0.4995 |\n| Pair 9       |        26 |     0.992  |     0.332  |\n| Pair 10      |        26 |     0.9805 |     0.165  |\n| Pair 11      |        26 |     0.9958 |     0.499  |\n| Pair 12      |        26 |     0.9921 |     0.332  |\n","output_type":"stream"}],"execution_count":31}]}