{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6787ab3d",
   "metadata": {
    "papermill": {
     "duration": 0.008026,
     "end_time": "2025-12-14T18:11:33.965465",
     "exception": false,
     "start_time": "2025-12-14T18:11:33.957439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "\n",
    "MABe Nearest Neighbors: The Original â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸\n",
    "MABe EDA which makes sense â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸\n",
    "MABe Validated baseline without machine learning\n",
    "Squeeze GBT\n",
    "Social Action Recognition in Mice | XGBoost1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4c3f3",
   "metadata": {
    "papermill": {
     "duration": 0.006091,
     "end_time": "2025-12-14T18:11:33.977993",
     "exception": false,
     "start_time": "2025-12-14T18:11:33.971902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d509e7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:33.991538Z",
     "iopub.status.busy": "2025-12-14T18:11:33.991287Z",
     "iopub.status.idle": "2025-12-14T18:11:35.895953Z",
     "shell.execute_reply": "2025-12-14T18:11:35.895354Z"
    },
    "papermill": {
     "duration": 1.913196,
     "end_time": "2025-12-14T18:11:35.897330",
     "exception": false,
     "start_time": "2025-12-14T18:11:33.984134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"F Beta customized for the data format of the MABe challenge.\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
    "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "    for row in lab_solution.to_dicts():\n",
    "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
    "\n",
    "    for video in lab_solution['video_id'].unique():\n",
    "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
    "        active_labels: set[str] = set(json.loads(active_labels))\n",
    "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
    "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
    "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
    "                continue\n",
    "\n",
    "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
    "            # Ignore truly redundant predictions.\n",
    "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
    "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
    "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
    "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
    "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
    "            prediction_frames[row['prediction_key']].update(new_frames)\n",
    "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
    "\n",
    "    tps = defaultdict(int)\n",
    "    fns = defaultdict(int)\n",
    "    fps = defaultdict(int)\n",
    "    for key, pred_frames in prediction_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        matched_label_frames = label_frames[key]\n",
    "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
    "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
    "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
    "\n",
    "    distinct_actions = set()\n",
    "    for key, frames in label_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        if key not in prediction_frames:\n",
    "            fns[action] += len(frames)\n",
    "\n",
    "    action_f1s = []\n",
    "    for action in distinct_actions:\n",
    "        if tps[action] + fns[action] + fps[action] == 0:\n",
    "            action_f1s.append(0)\n",
    "        else:\n",
    "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
    "    return sum(action_f1s) / len(action_f1s)\n",
    "\n",
    "\n",
    "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
    "    if len(solution) == 0 or len(submission) == 0:\n",
    "        raise ValueError('Missing solution or submission data')\n",
    "\n",
    "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "    for col in expected_cols:\n",
    "        if col not in solution.columns:\n",
    "            raise ValueError(f'Solution is missing column {col}')\n",
    "        if col not in submission.columns:\n",
    "            raise ValueError(f'Submission is missing column {col}')\n",
    "\n",
    "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
    "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
    "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
    "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
    "    solution_videos = set(solution['video_id'].unique())\n",
    "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
    "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
    "\n",
    "    solution = solution.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key'),\n",
    "    )\n",
    "    submission = submission.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key'),\n",
    "    )\n",
    "\n",
    "    lab_scores = []\n",
    "    for lab in solution['lab_id'].unique():\n",
    "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
    "        lab_videos = set(lab_solution['video_id'].unique())\n",
    "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
    "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
    "\n",
    "    return sum(lab_scores) / len(lab_scores)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    F1 score for the MABe Challenge\n",
    "    \"\"\"\n",
    "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    return mouse_fbeta(solution, submission, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb5b794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:35.912740Z",
     "iopub.status.busy": "2025-12-14T18:11:35.911953Z",
     "iopub.status.idle": "2025-12-14T18:11:38.191969Z",
     "shell.execute_reply": "2025-12-14T18:11:38.191248Z"
    },
    "papermill": {
     "duration": 2.289584,
     "end_time": "2025-12-14T18:11:38.193723",
     "exception": false,
     "start_time": "2025-12-14T18:11:35.904139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/koolbox-library/koolbox-0.1.3-py3-none-any.whl\r\n",
      "Installing collected packages: koolbox\r\n",
      "Successfully installed koolbox-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/koolbox-library/koolbox-0.1.3-py3-none-any.whl --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedd01f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:38.208669Z",
     "iopub.status.busy": "2025-12-14T18:11:38.208165Z",
     "iopub.status.idle": "2025-12-14T18:11:43.658088Z",
     "shell.execute_reply": "2025-12-14T18:11:43.657233Z"
    },
    "papermill": {
     "duration": 5.458582,
     "end_time": "2025-12-14T18:11:43.659259",
     "exception": false,
     "start_time": "2025-12-14T18:11:38.200677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from koolbox import Trainer\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from typing import Dict, Optional, Tuple\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 1234\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bab51",
   "metadata": {
    "papermill": {
     "duration": 0.006461,
     "end_time": "2025-12-14T18:11:43.672571",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.666110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chiáº¿n thuáº­t \"Láº¥y máº«u phÃ¢n táº§ng\" (Stratified Subsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5344276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.687688Z",
     "iopub.status.busy": "2025-12-14T18:11:43.687237Z",
     "iopub.status.idle": "2025-12-14T18:11:43.707973Z",
     "shell.execute_reply": "2025-12-14T18:11:43.707369Z"
    },
    "papermill": {
     "duration": 0.030154,
     "end_time": "2025-12-14T18:11:43.709074",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.678920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StratifiedSubsetClassifierWEval(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 estimator,\n",
    "                 n_samples=None,\n",
    "                 random_state: int = 42,\n",
    "                 valid_size: float = 0.10,\n",
    "                 val_cap_ratio: float = 0.25,\n",
    "                 es_rounds: \"int|str\" = \"auto\",\n",
    "                 es_metric: str = \"auto\"):\n",
    "        self.estimator = estimator\n",
    "        self.n_samples = (int(n_samples) if (n_samples is not None) else None)\n",
    "        self.random_state = random_state\n",
    "        self.valid_size = float(valid_size)\n",
    "        self.val_cap_ratio = float(val_cap_ratio)\n",
    "        self.es_rounds = es_rounds\n",
    "        self.es_metric = es_metric\n",
    " \n",
    "    def fit(self, X: pd.DataFrame, y):\n",
    "        y = np.asarray(y)\n",
    "        n_total = len(y); assert n_total == len(X)\n",
    "        pos_rate = float(np.mean(y == 1))\n",
    "    \n",
    "        # if 1% positive example\n",
    "        if pos_rate < 0.01:  # < 1% positive samples\n",
    "            print(f\"Rare class: {pos_rate*100:.2f}% positive\")\n",
    "            \n",
    "            # double the requested sample size\n",
    "            if self.n_samples is not None:\n",
    "                self.n_samples = min(self.n_samples * 2, n_total)\n",
    "            \n",
    "            # for XGBoost: class weighting\n",
    "            if self._is_xgb(self.estimator):\n",
    "                n_pos = max(1, int((y == 1).sum()))\n",
    "                n_neg = max(1, len(y) - n_pos)\n",
    "                # triple scaled_pos_weight to penalize missing rare events\n",
    "                self.estimator.set_params(scale_pos_weight=3.0 * (n_neg / n_pos))\n",
    "                \n",
    "        # train and validate indices\n",
    "        tr_idx, va_idx = self._compute_train_val_indices(y, n_total)\n",
    "        # just subset the data\n",
    "        Xtr = X.iloc[tr_idx]; ytr = y[tr_idx]\n",
    "        Xtr = Xtr.to_numpy(np.float32, copy=False)\n",
    "\n",
    "        Xva = yva = None\n",
    "        if va_idx is not None and len(va_idx) > 0:\n",
    "            Xva = X.iloc[va_idx].to_numpy(np.float32, copy=False); yva = y[va_idx]\n",
    "\n",
    "        # calculate positive rate in validation\n",
    "        pos_rate = None\n",
    "        if yva is not None and len(yva) > 0:\n",
    "            pos_rate = float(np.mean(yva == 1))\n",
    "\n",
    "        # Decide metric & patience\n",
    "        metric = self._choose_metric(pos_rate)\n",
    "        patience = self._choose_patience(pos_rate)\n",
    "\n",
    "        # Apply imbalance knobs per library\n",
    "        if self._is_xgb(self.estimator):\n",
    "            # scale_pos_weight = n_neg / n_pos on TRAIN\n",
    "            n_pos = max(1, int((ytr == 1).sum()))\n",
    "            n_neg = max(1, len(ytr) - n_pos)\n",
    "            self.estimator.set_params(scale_pos_weight=(n_neg / n_pos))\n",
    "            self.estimator.set_params(eval_metric=metric)\n",
    "\n",
    "        elif self._is_catboost(self.estimator):\n",
    "            # GPU-safe auto balancing\n",
    "            try: self.estimator.set_params(auto_class_weights=\"Balanced\")\n",
    "            except Exception: pass\n",
    "            try: self.estimator.set_params(eval_metric=metric)\n",
    "            except Exception: pass\n",
    "\n",
    "        # Fit with ES if we have any validation (single-class OK with Logloss)\n",
    "        has_valid = (Xva is not None and len(yva) > 0)\n",
    "        if has_valid and self._is_xgb(self.estimator):\n",
    "            import xgboost as xgb\n",
    "            self.estimator.fit(\n",
    "                Xtr, ytr,\n",
    "                eval_set=[(Xva, yva)],\n",
    "                verbose=False,\n",
    "                callbacks=[xgb.callback.EarlyStopping(\n",
    "                    rounds=int(patience),\n",
    "                    metric_name=metric,\n",
    "                    data_name=\"validation_0\",\n",
    "                    save_best=True\n",
    "                )]\n",
    "            )\n",
    "        elif has_valid and self._is_catboost(self.estimator):\n",
    "            from catboost import Pool\n",
    "            self.estimator.set_params(\n",
    "                use_best_model=True,\n",
    "                od_type=\"Iter\",\n",
    "                od_wait=int(patience),\n",
    "                custom_metric=[\"PRAUC:type=Classic;hints=skip_train~true\"],\n",
    "            )\n",
    "            self.estimator.fit(\n",
    "                Xtr, ytr,\n",
    "                eval_set=Pool(Xva, yva),\n",
    "                verbose=False,\n",
    "                metric_period=50\n",
    "            )\n",
    "        else:\n",
    "            # Fall back: train on train split without ES\n",
    "            self.estimator.fit(Xtr, ytr)\n",
    "\n",
    "        self.classes_ = getattr(self.estimator, \"classes_\", np.array([0, 1]))\n",
    "        self._tr_idx_ = tr_idx; self._va_idx_ = va_idx; self._pos_rate_ = pos_rate\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "    # helpers\n",
    "    def _compute_train_val_indices(self, y: np.ndarray, n_total: int):\n",
    "        \"\"\"\n",
    "        creates stratified indices.\n",
    "        if n_samples < n_total, subsamples the training data while keeping the validation set separate.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        n_classes = np.unique(y).size\n",
    "\n",
    "        # cannot stratify?? random split\n",
    "        def full_data_split():\n",
    "            if self.valid_size <= 0 or n_classes < 2:\n",
    "                idx = rng.permutation(n_total); return idx, None\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=self.valid_size, random_state=self.random_state)\n",
    "            tr, va = next(sss.split(np.zeros(n_total, dtype=np.int8), y))\n",
    "            return tr, va\n",
    "            \n",
    "        # n_samples is the size limit, if no limit, use all\n",
    "        if self.n_samples is None or self.n_samples >= n_total:\n",
    "            return full_data_split()\n",
    "\n",
    "        # sss = stratified shuffle split; select training subset\n",
    "        sss_tr = StratifiedShuffleSplit(n_splits=1, train_size=self.n_samples, random_state=self.random_state)\n",
    "        tr_idx, rest_idx = next(sss_tr.split(np.zeros(n_total, dtype=np.int8), y))\n",
    "        remaining = len(rest_idx)\n",
    "        \n",
    "        # Select the VALIDATION subset from the 'rest' (remaining data)\n",
    "        # We don't want validation data overlapping with training data.\n",
    "        min_val_needed = int(np.ceil(self.n_samples * max(self.valid_size, 0.0)))\n",
    "        val_cap = max(min_val_needed, int(round(self.val_cap_ratio * self.n_samples)))\n",
    "        want_val = min(remaining, val_cap)\n",
    "\n",
    "        y_rest = y[rest_idx]\n",
    "        if remaining < min_val_needed or np.unique(y_rest).size < 2 or self.valid_size <= 0:\n",
    "            return full_data_split()\n",
    "\n",
    "        sss_val = StratifiedShuffleSplit(n_splits=1, train_size=want_val, random_state=self.random_state)\n",
    "        try:\n",
    "            va_sel, _ = next(sss_val.split(np.zeros(remaining, dtype=np.int8), y_rest))\n",
    "        except ValueError:\n",
    "            return full_data_split()\n",
    "\n",
    "        va_idx = rest_idx[va_sel]\n",
    "        return tr_idx, va_idx\n",
    "\n",
    "    def _choose_metric(self, pos_rate=0.01) -> str:\n",
    "        if self.es_metric != \"auto\":\n",
    "            return self.es_metric\n",
    "        if pos_rate is None or pos_rate == 0.0 or pos_rate == 1.0:\n",
    "            return \"logloss\" if self._is_xgb(self.estimator) else \"Logloss\"\n",
    "        return \"aucpr\" if self._is_xgb(self.estimator) else \"PRAUC:type=Classic\"\n",
    "\n",
    "    def _choose_patience(self, pos_rate: Optional[float]) -> int:\n",
    "        if isinstance(self.es_rounds, int):\n",
    "            return self.es_rounds\n",
    "        try:\n",
    "            n_estimators = (int(self.estimator.get_params().get(\"n_estimators\", 200))\n",
    "                            if self._is_xgb(self.estimator)\n",
    "                            else int(self.estimator.get_params().get(\"iterations\", 500)))\n",
    "        except Exception:\n",
    "            n_estimators = 200\n",
    "        base = max(30, int(round(0.20 * (n_estimators or 200))))\n",
    "        if pos_rate is None:\n",
    "            return base\n",
    "        # Increase patience for highly imbalanced data\n",
    "        if pos_rate < 0.005:   # <0.5%\n",
    "            return int(round(base * 1.75))\n",
    "        if pos_rate < 0.02:    # <2%\n",
    "            return int(round(base * 1.40))\n",
    "        return base\n",
    "\n",
    "    # helpers to detect model\n",
    "    @staticmethod\n",
    "    def _is_xgb(est):\n",
    "        name = est.__class__.__name__.lower(); mod = getattr(est, \"__module__\", \"\")\n",
    "        return \"xgb\" in name or \"xgboost\" in mod or hasattr(est, \"get_xgb_params\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_catboost(est):\n",
    "        name = est.__class__.__name__.lower(); mod = getattr(est, \"__module__\", \"\")\n",
    "        return \"catboost\" in name or \"catboost\" in mod or hasattr(est, \"get_all_params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b708f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.723073Z",
     "iopub.status.busy": "2025-12-14T18:11:43.722646Z",
     "iopub.status.idle": "2025-12-14T18:11:43.728564Z",
     "shell.execute_reply": "2025-12-14T18:11:43.728034Z"
    },
    "papermill": {
     "duration": 0.014054,
     "end_time": "2025-12-14T18:11:43.729596",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.715542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StratifiedSubsetClassifier(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, estimator, n_samples, random_state=SEED):\n",
    "        self.estimator = estimator\n",
    "        self.n_samples = n_samples and int(n_samples)\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "         select a subset of data, then fit the base model.\n",
    "        \"\"\"\n",
    "        y = np.asarray(y)\n",
    "        n_total = len(y)\n",
    "\n",
    "        # if no limit or less data than limit, just use all\n",
    "        if self.n_samples is None or self.n_samples >= n_total:\n",
    "            rng = np.random.default_rng(self.random_state)\n",
    "            idx = rng.permutation(n_total)\n",
    "        # or use a subset\n",
    "        else:\n",
    "            sss = StratifiedShuffleSplit(\n",
    "                n_splits=1, train_size=self.n_samples, random_state=self.random_state\n",
    "            )\n",
    "            idx, _ = next(sss.split(np.zeros(n_total, dtype=np.int8), y))\n",
    "\n",
    "        Xn = X.iloc[idx]\n",
    "        Xn = Xn.to_numpy(np.float32, copy=False)\n",
    "        yn = y[idx]\n",
    "\n",
    "        # train for smaller subset\n",
    "        self.estimator.fit(Xn, yn)\n",
    "        self.classes_ = getattr(self.estimator, \"classes_\", np.array([0, 1]))\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239c1e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.743988Z",
     "iopub.status.busy": "2025-12-14T18:11:43.743768Z",
     "iopub.status.idle": "2025-12-14T18:11:43.749490Z",
     "shell.execute_reply": "2025-12-14T18:11:43.748906Z"
    },
    "papermill": {
     "duration": 0.014314,
     "end_time": "2025-12-14T18:11:43.750500",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.736186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ DETECTED SUBMISSION: Há»‡ thá»‘ng tá»± Ä‘á»™ng chuyá»ƒn DEBUG = False Ä‘á»ƒ train full!\n",
      "âš™ï¸ PRODUCTION MODE (Cháº¡y tháº­t)\n",
      "   -> Äang cháº¡y Submit/Batch: Cháº¿ Ä‘á»™ SUBMIT\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # 1. Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n (Giá»¯ nguyÃªn)\n",
    "    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n",
    "    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    model_path = \"/kaggle/input/social-action-recognition-in-mice-xgb-catboost\"\n",
    "    model_name = \"ensemble_v1\" # Äá»•i tÃªn Ä‘á»ƒ biáº¿t lÃ  dÃ¹ng Ensemble\n",
    "\n",
    "    # ====================================================\n",
    "    # 2. Cáº¤U HÃŒNH CHáº¾ Äá»˜ CHáº Y (QUAN TRá»ŒNG)\n",
    "    # ====================================================\n",
    "    \n",
    "    # Báº­t True: Cháº¡y siÃªu nhanh trÃªn Ã­t dá»¯ liá»‡u Ä‘á»ƒ kiá»ƒm tra lá»—i (Test code)\n",
    "    # Báº­t False: Cháº¡y tháº­t Ä‘á»ƒ láº¥y káº¿t quáº£ (Train full hoáº·c Submit)\n",
    "    debug = True  \n",
    "    \n",
    "    # --- ÄOáº N NÃ€Y QUAN TRá»ŒNG NHáº¤T ---\n",
    "    # Kiá»ƒm tra: Náº¿u khÃ´ng pháº£i Ä‘ang ngá»“i code (Interactive) => Tá»©c lÃ  Ä‘ang Submit/Save Version\n",
    "    # ThÃ¬ CÆ¯á» NG CHáº¾ táº¯t Debug ngay láº­p tá»©c!\n",
    "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE') != 'Interactive':\n",
    "        debug = False\n",
    "        print(\"ðŸš€ DETECTED SUBMISSION: Há»‡ thá»‘ng tá»± Ä‘á»™ng chuyá»ƒn DEBUG = False Ä‘á»ƒ train full!\")\n",
    "    # --------------------------------\n",
    "    \n",
    "    if debug:\n",
    "        print(\"ðŸž DEBUG MODE: ON (Cháº¡y test nhanh)\")\n",
    "        mode = 'validate'       # LuÃ´n validate Ä‘á»ƒ xem code cÃ³ crash khÃ´ng\n",
    "        n_estimators = 10       # Chá»‰ train 10 cÃ¢y cho láº¹\n",
    "        n_splits = 2            # Chá»‰ chia 2 fold\n",
    "    else:\n",
    "        print(\"âš™ï¸ PRODUCTION MODE (Cháº¡y tháº­t)\")\n",
    "        n_estimators = 300    \n",
    "        n_splits = 3\n",
    "        \n",
    "        # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n mÃ´i trÆ°á»ng Kaggle\n",
    "        if os.environ.get('KAGGLE_KERNEL_RUN_TYPE') == 'Interactive':\n",
    "            mode = 'validate'\n",
    "            print(\"   -> Äang cháº¡y Interactive: Cháº¿ Ä‘á»™ VALIDATE\")\n",
    "        else:\n",
    "            mode = 'submit'\n",
    "            print(\"   -> Äang cháº¡y Submit/Batch: Cháº¿ Ä‘á»™ SUBMIT\")\n",
    "\n",
    "    # ====================================================\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15501ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.764535Z",
     "iopub.status.busy": "2025-12-14T18:11:43.764106Z",
     "iopub.status.idle": "2025-12-14T18:11:43.769782Z",
     "shell.execute_reply": "2025-12-14T18:11:43.769292Z"
    },
    "papermill": {
     "duration": 0.013705,
     "end_time": "2025-12-14T18:11:43.770685",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.756980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_zoo():\n",
    "    # choose sample size (very arbitrary number here)\n",
    "    N_SAMPLES = 2_000_000 \n",
    "    \n",
    "    xgb_params = dict(\n",
    "        verbosity=0, random_state=42,\n",
    "        n_estimators=CFG.n_estimators, # Láº¥y tá»« CFG\n",
    "        learning_rate=0.05, \n",
    "        max_depth=8,\n",
    "        min_child_weight=3, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=3.0,\n",
    "        tree_method='gpu_hist',       # GPU!!!\n",
    "        predictor='gpu_predictor'     \n",
    "    )\n",
    "\n",
    "    xgb_pipe = make_pipeline(\n",
    "        StratifiedSubsetClassifier(\n",
    "            estimator=XGBClassifier(**xgb_params),\n",
    "            n_samples=N_SAMPLES\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cat_params = dict(\n",
    "        verbose=0, \n",
    "        random_state=42,\n",
    "        iterations=CFG.n_estimators,   # Láº¥y tá»« CFG\n",
    "        learning_rate=0.05, \n",
    "        depth=8,\n",
    "        auto_class_weights='Balanced',\n",
    "        allow_writing_files=False,\n",
    "        task_type=\"GPU\",              # DÃ¹ng GPU\n",
    "        devices='0'\n",
    "    )\n",
    "    cat_pipe = make_pipeline(\n",
    "        StratifiedSubsetClassifier(\n",
    "            estimator=CatBoostClassifier(**cat_params),\n",
    "            n_samples=N_SAMPLES\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "    lgbm_params = dict(\n",
    "        verbosity=-1, \n",
    "        random_state=42,\n",
    "        n_estimators=CFG.n_estimators, # Láº¥y tá»« CFG\n",
    "        learning_rate=0.05, \n",
    "        max_depth=8,\n",
    "        class_weight='balanced',       \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8\n",
    "    )\n",
    "    lgbm_pipe = make_pipeline(\n",
    "        StratifiedSubsetClassifier(\n",
    "            estimator=LGBMClassifier(**lgbm_params),\n",
    "            n_samples=N_SAMPLES\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return [('xgb', xgb_pipe), ('cat', cat_pipe), ('lgbm', lgbm_pipe)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade00991",
   "metadata": {
    "papermill": {
     "duration": 0.007695,
     "end_time": "2025-12-14T18:11:43.784895",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.777200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee9becd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.799036Z",
     "iopub.status.busy": "2025-12-14T18:11:43.798837Z",
     "iopub.status.idle": "2025-12-14T18:11:43.910758Z",
     "shell.execute_reply": "2025-12-14T18:11:43.909918Z"
    },
    "papermill": {
     "duration": 0.120972,
     "end_time": "2025-12-14T18:11:43.912269",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.791297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "train['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n",
    "train_without_mabe22 = train.query(\"~lab_id.str.startswith('MABe22_')\")\n",
    "\n",
    "test = pd.read_csv(CFG.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c003a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.928048Z",
     "iopub.status.busy": "2025-12-14T18:11:43.927434Z",
     "iopub.status.idle": "2025-12-14T18:11:43.934649Z",
     "shell.execute_reply": "2025-12-14T18:11:43.934087Z"
    },
    "papermill": {
     "duration": 0.015826,
     "end_time": "2025-12-14T18:11:43.935655",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.919829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"body_center\", \"ear_left\", \"ear_right\", \"forepaw_left\", \"forepaw_right\", \"hindpaw_left\", \"hindpaw_right\", \"neck\", \"nose\", \"tail_base\", \"tail_midpoint\", \"tail_tip\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"headpiece_bottombackleft\", \"headpiece_bottombackright\", \"headpiece_bottomfrontleft\", \"headpiece_bottomfrontright\", \"headpiece_topbackleft\", \"headpiece_topbackright\", \"headpiece_topfrontleft\", \"headpiece_topfrontright\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_midpoint\", \"tail_tip\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"spine_1\", \"spine_2\", \"tail_base\", \"tail_middle_1\", \"tail_middle_2\", \"tail_tip\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_midpoint\", \"tail_tip\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\", \"tail_tip\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\"]', '[\"body_center\", \"ear_left\", \"ear_right\", \"nose\", \"tail_base\"]', '[\"ear_left\", \"ear_right\", \"head\", \"tail_base\"]', '[\"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"neck\", \"nose\", \"tail_base\"]', '[\"ear_left\", \"ear_right\", \"nose\", \"tail_base\", \"tail_tip\"]']\n"
     ]
    }
   ],
   "source": [
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))\n",
    "print(body_parts_tracked_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43d3bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.949999Z",
     "iopub.status.busy": "2025-12-14T18:11:43.949632Z",
     "iopub.status.idle": "2025-12-14T18:11:43.955760Z",
     "shell.execute_reply": "2025-12-14T18:11:43.955105Z"
    },
    "papermill": {
     "duration": 0.014611,
     "end_time": "2025-12-14T18:11:43.956856",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.942245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_solution_df(dataset):\n",
    "    \"\"\"\n",
    "    tá»•ng há»£p label (ground truth) tá»« cÃ¡c video.\n",
    "    \"\"\"\n",
    "    solution = []\n",
    "    \n",
    "    # Duyá»‡t qua tá»«ng dÃ²ng (tá»«ng video) trong metadata\n",
    "    # tqdm = progress bar\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    \n",
    "        lab_id = row['lab_id']\n",
    "        \n",
    "        # ko cÃ³ tracking data\n",
    "        if lab_id.startswith('MABe22'): \n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "        \n",
    "        # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº¿n file annotation\n",
    "        path = f\"{CFG.train_annotation_path}/{lab_id}/{video_id}.parquet\"\n",
    "        \n",
    "        # ko cÃ³ thÃ¬ bá» qua (thá»±c sá»± dÃ­nh 1 file :v)\n",
    "        try:\n",
    "            annot = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "        # GÃ¡n láº¡i metadata vÃ o dataframe vá»«a Ä‘á»c\n",
    "        annot['lab_id'] = lab_id\n",
    "        annot['video_id'] = video_id\n",
    "        annot['behaviors_labeled'] = row['behaviors_labeled'] # CÃ¡c hÃ nh vi cáº§n dá»± Ä‘oÃ¡n\n",
    "        \n",
    "        # - Náº¿u lÃ  pair action: Ä‘á»•i thÃ nh \"mouse{id}\"\n",
    "        # - Náº¿u lÃ  self action: Ä‘á»•i thÃ nh \"self\"\n",
    "        annot['target_id'] = np.where(\n",
    "            annot.target_id != annot.agent_id, \n",
    "            annot['target_id'].apply(lambda s: f\"mouse{s}\"), \n",
    "            'self'\n",
    "        )\n",
    "        \n",
    "        # agent_id: Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng \"mouse{id}\" (vÃ­ dá»¥: mouse0, mouse1)\n",
    "        annot['agent_id'] = annot['agent_id'].apply(lambda s: f\"mouse{s}\")\n",
    "        \n",
    "        solution.append(annot)\n",
    "    \n",
    "    solution = pd.concat(solution)\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def validate(mode):\n",
    "    if(mode == 'validate'):\n",
    "        solution = create_solution_df(train_without_mabe22)\n",
    "# Chá»‰ cháº¡y khi Ä‘ang á»Ÿ cháº¿ Ä‘á»™ 'validate' (kiá»ƒm thá»­)\n",
    "if CFG.mode == 'validate':\n",
    "    # Táº¡o dataframe solution tá»« táº­p dá»¯ liá»‡u validation (Ä‘Ã£ loáº¡i bá» MABe22)\n",
    "    solution = create_solution_df(train_without_mabe22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7829313",
   "metadata": {
    "papermill": {
     "duration": 0.006509,
     "end_time": "2025-12-14T18:11:43.970019",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.963510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1846ea91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:43.983937Z",
     "iopub.status.busy": "2025-12-14T18:11:43.983702Z",
     "iopub.status.idle": "2025-12-14T18:11:43.987312Z",
     "shell.execute_reply": "2025-12-14T18:11:43.986633Z"
    },
    "papermill": {
     "duration": 0.011849,
     "end_time": "2025-12-14T18:11:43.988310",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.976461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop rare body parts\n",
    "drop_body_parts =  [\n",
    "    'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed15256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.002416Z",
     "iopub.status.busy": "2025-12-14T18:11:44.002066Z",
     "iopub.status.idle": "2025-12-14T18:11:44.009637Z",
     "shell.execute_reply": "2025-12-14T18:11:44.008968Z"
    },
    "papermill": {
     "duration": 0.015899,
     "end_time": "2025-12-14T18:11:44.010629",
     "exception": false,
     "start_time": "2025-12-14T18:11:43.994730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_tracking(lab_id, video_id, base_path):\n",
    "    path = f\"{base_path}/{lab_id}/{video_id}.parquet\"\n",
    "    return pd.read_parquet(path), path\n",
    "\n",
    "def clean_bodyparts(df):\n",
    "    if len(np.unique(df.bodypart)) > 5:\n",
    "        return df.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "    return df\n",
    "\n",
    "# pivot + scale pixel to cm\n",
    "def normalize_tracking(df, pix_per_cm):\n",
    "    pvid = df.pivot(\n",
    "        columns=[\"mouse_id\", \"bodypart\"],\n",
    "        index=\"video_frame\",\n",
    "        values=[\"x\", \"y\"]\n",
    "    )\n",
    "    pvid = (\n",
    "        pvid.reorder_levels([1, 2, 0], axis=1)\n",
    "        .T.sort_index().T\n",
    "    )\n",
    "    return pvid / pix_per_cm\n",
    "\n",
    "#  Parse behaviors_labeled \n",
    "def parse_behaviors(behavior_str):\n",
    "    behaviors = json.loads(behavior_str)\n",
    "    behaviors = [b.replace(\"'\", \"\") for b in behaviors]\n",
    "    behaviors = [b.split(',') for b in behaviors]\n",
    "    return pd.DataFrame(behaviors, columns=['agent', 'target', 'action'])\n",
    "\n",
    "# Load annotation (train only)\n",
    "def load_annotation(tracking_path):\n",
    "    anno_path = tracking_path.replace('train_tracking', 'train_annotation')\n",
    "    try:\n",
    "        return pd.read_parquet(anno_path)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "# Build single-mouse labels\n",
    "def build_single_label(annot, agent_id, actions, index):\n",
    "    y = pd.DataFrame(0.0, columns=actions, index=index)\n",
    "    subset = annot.query(\n",
    "        \"(agent_id == @agent_id) & (target_id == @agent_id)\"\n",
    "    )\n",
    "    for _, arow in subset.iterrows():\n",
    "        y.loc[arow.start_frame:arow.stop_frame, arow.action] = 1.0\n",
    "    return y\n",
    "\n",
    "# 7. Build pair labels \n",
    "def build_pair_label(annot, agent_id, target_id, actions, index):\n",
    "    y = pd.DataFrame(0.0, columns=actions, index=index)\n",
    "    subset = annot.query(\n",
    "        \"(agent_id == @agent_id) & (target_id == @target_id)\"\n",
    "    )\n",
    "    for _, arow in subset.iterrows():\n",
    "        y.loc[arow.start_frame:arow.stop_frame, arow.action] = 1.0\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e639e283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.024829Z",
     "iopub.status.busy": "2025-12-14T18:11:44.024329Z",
     "iopub.status.idle": "2025-12-14T18:11:44.029055Z",
     "shell.execute_reply": "2025-12-14T18:11:44.028526Z"
    },
    "papermill": {
     "duration": 0.012937,
     "end_time": "2025-12-14T18:11:44.030079",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.017142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate self features for a single video\n",
    "def generate_single_samples(pvid, behaviors, annot, video_id, is_train):\n",
    "    # get self action rows\n",
    "    single_behaviors = behaviors.query(\"target == 'self'\")\n",
    "    agents = np.unique(single_behaviors.agent)\n",
    "\n",
    "    # Iterate over each agent (mouse)\n",
    "    for agent_str in agents:\n",
    "        agent = int(agent_str[-1])\n",
    "        actions = np.unique(single_behaviors.query(\n",
    "            \"agent == @agent_str\"\n",
    "        ).action)\n",
    "\n",
    "        try:\n",
    "            X = pvid.loc[:, agent]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # Create Metadata for the current sample\n",
    "        # critical for mapping predictions back to the original video, agent, and frame.\n",
    "        meta = pd.DataFrame({\n",
    "            \"video_id\": video_id,\n",
    "            \"agent_id\": agent_str,\n",
    "            \"target_id\": \"self\",\n",
    "            \"video_frame\": X.index\n",
    "        })\n",
    "\n",
    "        if is_train:\n",
    "            y = build_single_label(annot, agent, actions, X.index)\n",
    "            yield \"single\", X, meta, y\n",
    "        else:\n",
    "            yield \"single\", X, meta, actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2d9f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.043869Z",
     "iopub.status.busy": "2025-12-14T18:11:44.043637Z",
     "iopub.status.idle": "2025-12-14T18:11:44.048650Z",
     "shell.execute_reply": "2025-12-14T18:11:44.048002Z"
    },
    "papermill": {
     "duration": 0.013245,
     "end_time": "2025-12-14T18:11:44.049757",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.036512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create pair features\n",
    "import itertools\n",
    "\n",
    "def generate_pair_samples(pvid, behaviors, annot, video_id, is_train):\n",
    "    pair_beh = behaviors.query(\"target != 'self'\")\n",
    "    if len(pair_beh) == 0:\n",
    "        return\n",
    "\n",
    "    mice = np.unique(pvid.columns.get_level_values('mouse_id'))\n",
    "\n",
    "    for agent, target in itertools.permutations(mice, 2):\n",
    "        a_str, t_str = f\"mouse{agent}\", f\"mouse{target}\"\n",
    "        actions = np.unique(pair_beh.query(\n",
    "            \"(agent == @a_str) & (target == @t_str)\"\n",
    "        ).action)\n",
    "\n",
    "        X = pd.concat([pvid[agent], pvid[target]], axis=1, keys=[\"A\", \"B\"])\n",
    "\n",
    "        meta = pd.DataFrame({\n",
    "            \"video_id\": video_id,\n",
    "            \"agent_id\": a_str,\n",
    "            \"target_id\": t_str,\n",
    "            \"video_frame\": X.index\n",
    "        })\n",
    "\n",
    "        if is_train:\n",
    "            y = build_pair_label(annot, agent, target, actions, X.index)\n",
    "            yield \"pair\", X, meta, y\n",
    "        else:\n",
    "            yield \"pair\", X, meta, actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503e54c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.063916Z",
     "iopub.status.busy": "2025-12-14T18:11:44.063488Z",
     "iopub.status.idle": "2025-12-14T18:11:44.069099Z",
     "shell.execute_reply": "2025-12-14T18:11:44.068613Z"
    },
    "papermill": {
     "duration": 0.013926,
     "end_time": "2025-12-14T18:11:44.070096",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.056170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mouse_data(dataset, traintest,\n",
    "                        traintest_directory=None,\n",
    "                        generate_single=True,\n",
    "                        generate_pair=True):\n",
    "\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "\n",
    "        if row.lab_id.startswith(\"MABe22\") or type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "\n",
    "        # Load tracking\n",
    "        df, track_path = load_tracking(row.lab_id, row.video_id, traintest_directory)\n",
    "        df = clean_bodyparts(df)\n",
    "        pvid = normalize_tracking(df, row.pix_per_cm_approx)\n",
    "\n",
    "        # interpolate to fill\n",
    "        pvid = pvid.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "        \n",
    "        # Savitzky-Golay filter (GiÃºp loáº¡i bá» rung nhiá»…u)\n",
    "        if len(pvid) > 7:\n",
    "            try:\n",
    "                # window_length=7 (tÆ°Æ¡ng Ä‘Æ°Æ¡ng 0.2s), polyorder=2\n",
    "                pvid.iloc[:] = savgol_filter(pvid.values, window_length=7, polyorder=2, axis=0)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Parse behaviors\n",
    "        behaviors = parse_behaviors(row.behaviors_labeled)\n",
    "\n",
    "        # Load labels (train)\n",
    "        annot = None\n",
    "        if traintest == \"train\":\n",
    "            annot = load_annotation(track_path)\n",
    "            if annot is None:\n",
    "                continue\n",
    "\n",
    "        # Generate samples\n",
    "        if generate_single:\n",
    "            yield from generate_single_samples(\n",
    "                pvid, behaviors, annot, row.video_id, traintest == 'train'\n",
    "            )\n",
    "\n",
    "        if generate_pair:\n",
    "            yield from generate_pair_samples(\n",
    "                pvid, behaviors, annot, row.video_id, traintest == 'train'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc22f75",
   "metadata": {
    "papermill": {
     "duration": 0.006568,
     "end_time": "2025-12-14T18:11:44.083105",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.076537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforming coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5682efa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.097448Z",
     "iopub.status.busy": "2025-12-14T18:11:44.096987Z",
     "iopub.status.idle": "2025-12-14T18:11:44.102461Z",
     "shell.execute_reply": "2025-12-14T18:11:44.101923Z"
    },
    "papermill": {
     "duration": 0.013998,
     "end_time": "2025-12-14T18:11:44.103462",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.089464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_rolling(series, window, func, min_periods=None):\n",
    "    if min_periods is None:\n",
    "        min_periods = max(1, window // 4)\n",
    "    return series.rolling(window, min_periods=min_periods, center=True).apply(func, raw=True)\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n",
    "        return float(meta_df['frames_per_second'].iloc[0])\n",
    "    vid = meta_df['video_id'].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506fc3d",
   "metadata": {
    "papermill": {
     "duration": 0.006683,
     "end_time": "2025-12-14T18:11:44.116806",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.110123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Táº¡o Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u tá»a Ä‘á»™ chuá»—i thá»i gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da6207ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.131269Z",
     "iopub.status.busy": "2025-12-14T18:11:44.130994Z",
     "iopub.status.idle": "2025-12-14T18:11:44.144903Z",
     "shell.execute_reply": "2025-12-14T18:11:44.144377Z"
    },
    "papermill": {
     "duration": 0.022742,
     "end_time": "2025-12-14T18:11:44.145974",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.123232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    TÃ­nh toÃ¡n cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan Ä‘áº¿n Ä‘á»™ cong (curvature) vÃ  Ä‘á»™ ngoáº·t (turning) cá»§a quá»¹ Ä‘áº¡o.\n",
    "    GiÃºp nháº­n biáº¿t cÃ¡c hÃ nh vi xoay vÃ²ng, Ä‘áº£o chiá»u hoáº·c di chuyá»ƒn phá»©c táº¡p.\n",
    "    \"\"\"\n",
    "    # Váº­n tá»‘c (Velocity): Äáº¡o hÃ m báº­c 1 cá»§a vá»‹ trÃ­\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    # Gia tá»‘c (Acceleration): Äáº¡o hÃ m báº­c 1 cá»§a váº­n tá»‘c (báº­c 2 cá»§a vá»‹ trÃ­)\n",
    "    acc_x = vel_x.diff()\n",
    "    acc_y = vel_y.diff()\n",
    "\n",
    "    # TÃ­nh Ä‘á»™ cong (Curvature)\n",
    "    # CÃ´ng thá»©c Ä‘á»™ cong k = |x'y'' - y'x''| / (x'^2 + y'^2)^(3/2)\n",
    "    # TÃ­ch chÃ©o (Cross product) giá»¯a vector váº­n tá»‘c vÃ  gia tá»‘c\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    # Äá»™ lá»›n váº­n tá»‘c (tá»‘c Ä‘á»™ tá»©c thá»i)\n",
    "    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    # TÃ­nh Ä‘á»™ cong (thÃªm 1e-6 Ä‘á»ƒ trÃ¡nh lá»—i chia cho 0 khi Ä‘á»©ng yÃªn)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n",
    "\n",
    "    # TÃ­nh trung bÃ¬nh Ä‘á»™ cong trong cÃ¡c cá»­a sá»• trÆ°á»£t (rolling windows) khÃ¡c nhau\n",
    "    # _scale(w, fps) lÃ  hÃ m quy Ä‘á»•i tá»« thá»i gian/frames sang kÃ­ch thÆ°á»›c cá»­a sá»•\n",
    "    for w in [25, 50, 75]:\n",
    "        ws = _scale(w, fps)\n",
    "        # min_periods giÃºp tÃ­nh toÃ¡n Ä‘Æ°á»£c ngay cáº£ khi chÆ°a Ä‘á»§ dá»¯ liá»‡u (á»Ÿ Ä‘áº§u chuá»—i)\n",
    "        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "\n",
    "    # TÃ­nh tá»‘c Ä‘á»™ ngoáº·t (Turn Rate)\n",
    "    # GÃ³c cá»§a vector váº­n tá»‘c (hÆ°á»›ng di chuyá»ƒn)\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    # Sá»± thay Ä‘á»•i gÃ³c giá»¯a cÃ¡c frame liÃªn tiáº¿p (Ä‘á»™ lá»›n gÃ³c ngoáº·t)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    \n",
    "    # TÃ­nh tá»•ng gÃ³c ngoáº·t trong má»™t khoáº£ng thá»i gian (Ä‘o lÆ°á»ng Ä‘á»™ \"ngoáº±n ngoÃ¨o\" cá»§a Ä‘Æ°á»ng Ä‘i)\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    TÃ­nh toÃ¡n Ä‘áº·c trÆ°ng tá»‘c Ä‘á»™ á»Ÿ nhiá»u quy mÃ´ (scale) thá»i gian khÃ¡c nhau.\n",
    "    GiÃºp phÃ¢n biá»‡t hÃ nh vi ngáº¯n háº¡n (giáº­t mÃ¬nh) vs dÃ i háº¡n (di chuyá»ƒn tuáº§n tra).\n",
    "    \"\"\"\n",
    "    # TÃ­nh tá»‘c Ä‘á»™ vÃ´ hÆ°á»›ng (cm/s hoáº·c px/s tÃ¹y Ä‘Æ¡n vá»‹ gá»‘c)\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            # Tá»‘c Ä‘á»™ trung bÃ¬nh trong cá»­a sá»•\n",
    "            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            # Äá»™ biáº¿n Ä‘á»™ng cá»§a tá»‘c Ä‘á»™ (Standard Deviation) - xem con váº­t di chuyá»ƒn Ä‘á»u hay giáº­t cá»¥c\n",
    "            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n",
    "\n",
    "    # Tá»· lá»‡ giá»¯a tá»‘c Ä‘á»™ ngáº¯n háº¡n vÃ  dÃ i háº¡n (vÃ­ dá»¥: Ä‘ang tÄƒng tá»‘c Ä‘á»™t ngá»™t so vá»›i má»©c bÃ¬nh thÆ°á»ng)\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n",
    "        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    PhÃ¢n loáº¡i tráº¡ng thÃ¡i di chuyá»ƒn (Äá»©ng yÃªn, Äi cháº­m, Cháº¡y...) vÃ  tÃ­nh toÃ¡n táº§n suáº¥t xuáº¥t hiá»‡n.\n",
    "    \"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    \n",
    "    # LÃ m mÆ°á»£t tá»‘c Ä‘á»™ báº±ng Moving Average trÆ°á»›c khi phÃ¢n loáº¡i Ä‘á»ƒ giáº£m nhiá»…u\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "\n",
    "    try:\n",
    "        # --- PhÃ¢n chia tráº¡ng thÃ¡i (Discretization) ---\n",
    "        # CÃ¡c ngÆ°á»¡ng (bins): <0.5 (Äá»©ng yÃªn), 0.5-2.0 (Cháº­m), 2.0-5.0 (Vá»«a), >5.0 (Nhanh)\n",
    "        # LÆ°u Ã½: CÃ¡c con sá»‘ nÃ y phá»¥ thuá»™c vÃ o Ä‘Æ¡n vá»‹ cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o (pixel hay cm)\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        # GÃ¡n nhÃ£n tráº¡ng thÃ¡i: 0, 1, 2, 3\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        for window in [20, 40, 60, 80]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                # TÃ­nh tá»· lá»‡ thá»i gian á»Ÿ trong tá»«ng tráº¡ng thÃ¡i (vÃ­ dá»¥: dÃ nh 80% thá»i gian Ä‘á»ƒ Ä‘á»©ng yÃªn)\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    X[f's{state}_{window}'] = (\n",
    "                        (speed_states == state).astype(float)\n",
    "                        .rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "                    )\n",
    "                \n",
    "                # TÃ­nh sá»‘ láº§n chuyá»ƒn tráº¡ng thÃ¡i (State transitions)\n",
    "                # Äo lÆ°á»ng sá»± á»•n Ä‘á»‹nh cá»§a hÃ nh vi (cÃ³ hay thay Ä‘á»•i tá»‘c Ä‘á»™ liÃªn tá»¥c khÃ´ng)\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    except Exception:\n",
    "        pass # Bá» qua náº¿u cÃ³ lá»—i (vÃ­ dá»¥ dá»¯ liá»‡u quÃ¡ ngáº¯n)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    CÃ¡c Ä‘áº·c trÆ°ng dÃ i háº¡n: So sÃ¡nh vá»‹ trÃ­/tá»‘c Ä‘á»™ hiá»‡n táº¡i vá»›i lá»‹ch sá»­ quÃ¡ khá»© xa hÆ¡n.\n",
    "    \"\"\"\n",
    "    # 1. Vá»‹ trÃ­ trung bÃ¬nh trong cá»­a sá»• lá»›n (Rolling Mean)\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "\n",
    "    # 2. Vá»‹ trÃ­ trung bÃ¬nh theo trá»ng sá»‘ mÅ© (Exponential Weighted Mean - EWM)\n",
    "    # EWM giÃºp mÆ°á»£t hÃ³a quá»¹ Ä‘áº¡o nhÆ°ng váº«n bÃ¡m sÃ¡t cÃ¡c thay Ä‘á»•i gáº§n nháº¥t hÆ¡n lÃ  Rolling Mean\n",
    "    for span in [30, 60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    # 3. Xáº¿p háº¡ng pháº§n trÄƒm tá»‘c Ä‘á»™ (Percentile Rank)\n",
    "    # Tá»‘c Ä‘á»™ hiá»‡n táº¡i náº±m á»Ÿ má»©c nÃ o so vá»›i quÃ¡ khá»© (vÃ­ dá»¥: cao hÆ¡n 90% thá»i gian trÆ°á»›c Ä‘Ã³)\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef36e04",
   "metadata": {
    "papermill": {
     "duration": 0.006905,
     "end_time": "2025-12-14T18:11:44.159430",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.152525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09de3d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.173757Z",
     "iopub.status.busy": "2025-12-14T18:11:44.173546Z",
     "iopub.status.idle": "2025-12-14T18:11:44.189460Z",
     "shell.execute_reply": "2025-12-14T18:11:44.188828Z"
    },
    "papermill": {
     "duration": 0.024648,
     "end_time": "2025-12-14T18:11:44.190527",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.165879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"\n",
    "    Quan trá»ng cho MABe Challenge: TÃ­nh tÆ°Æ¡ng tÃ¡c xÃ£ há»™i giá»¯a 2 con chuá»™t (A vÃ  B).\n",
    "    \"\"\"\n",
    "    # Kiá»ƒm tra xem dá»¯ liá»‡u body_center cá»§a cáº£ 2 con cÃ³ tá»“n táº¡i khÃ´ng\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    # Vector khoáº£ng cÃ¡ch tá»« B Ä‘áº¿n A\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    # Váº­n tá»‘c riÃªng cá»§a tá»«ng con\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "\n",
    "    # Cosine Similarity giá»¯a Vector váº­n tá»‘c vÃ  Vector khoáº£ng cÃ¡ch\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    # LÆ°u Ã½ dáº¥u trá»« (-rel_x) vÃ¬ vector khoáº£ng cÃ¡ch tÃ­nh tá»« B Ä‘áº¿n A ngÆ°á»£c láº¡i\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        # Trung bÃ¬nh má»©c Ä‘á»™ hÆ°á»›ng vá» nhau trong khoáº£ng thá»i gian\n",
    "        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    # there are 26 behaviors, we are gonna do 2?\n",
    "    # approach: Tá»‘c Ä‘á»™ thu háº¹p khoáº£ng cÃ¡ch (- Ä‘áº¡o hÃ m khoáº£ng cÃ¡ch)\n",
    "    approach = -rel_dist.diff()\n",
    "    # chase: Káº¿t há»£p viá»‡c \"Ä‘ang láº¡i gáº§n\" vÃ  \"Ä‘ang hÆ°á»›ng máº·t vá» phÃ­a Ä‘á»‘i phÆ°Æ¡ng\"\n",
    "    chase = approach * B_lead \n",
    "    \n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    # Speed Correlation\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_egocentric_features(X, mouse_pair, avail_A, avail_B):\n",
    "    \"\"\"\n",
    "    Xoay há»‡ trá»¥c tá»a Ä‘á»™ sao cho Chuá»™t A náº±m táº¡i (0,0) vÃ  Ä‘áº§u hÆ°á»›ng lÃªn trÃªn.\n",
    "    GiÃºp mÃ´ hÃ¬nh biáº¿t chuá»™t B Ä‘ang á»Ÿ vá»‹ trÃ­ nÃ o tÆ°Æ¡ng Ä‘á»‘i so vá»›i hÆ°á»›ng nhÃ¬n cá»§a A.\n",
    "    \"\"\"\n",
    "    # Chá»‰ tÃ­nh Ä‘Æ°á»£c khi cÃ³ Ä‘á»§ MÅ©i vÃ  ÄuÃ´i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh hÆ°á»›ng\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and 'body_center' in avail_B:\n",
    "        # Tá»a Ä‘á»™ A\n",
    "        ax_tail = mouse_pair['A']['tail_base']['x']\n",
    "        ay_tail = mouse_pair['A']['tail_base']['y']\n",
    "        ax_nose = mouse_pair['A']['nose']['x']\n",
    "        ay_nose = mouse_pair['A']['nose']['y']\n",
    "\n",
    "        # Tá»a Ä‘á»™ B\n",
    "        bx = mouse_pair['B']['body_center']['x']\n",
    "        by = mouse_pair['B']['body_center']['y']\n",
    "\n",
    "        # 1. Tá»‹nh tiáº¿n: ÄÆ°a Ä‘uÃ´i A vá» gá»‘c (0,0)\n",
    "        dx = bx - ax_tail\n",
    "        dy = by - ay_tail\n",
    "\n",
    "        # 2. TÃ­nh gÃ³c quay (Ä‘á»ƒ A hÆ°á»›ng tháº³ng Ä‘á»©ng lÃªn trá»¥c Y)\n",
    "        vx = ax_nose - ax_tail\n",
    "        vy = ay_nose - ay_tail\n",
    "        angle_a = np.arctan2(vy, vx)\n",
    "        theta = np.pi/2 - angle_a # GÃ³c cáº§n xoay\n",
    "        \n",
    "        cos_t = np.cos(theta)\n",
    "        sin_t = np.sin(theta)\n",
    "\n",
    "        # 3. Xoay tá»a Ä‘á»™ cá»§a B\n",
    "        X['ego_bx'] = dx * cos_t - dy * sin_t\n",
    "        X['ego_by'] = dx * sin_t + dy * cos_t\n",
    "        \n",
    "        # 4. Xoay cáº£ váº­n tá»‘c cá»§a B (Quan trá»ng Ä‘á»ƒ biáº¿t B Ä‘ang lao tá»›i hay bá» cháº¡y)\n",
    "        if 'body_center' in avail_B:\n",
    "             b_vx = mouse_pair['B']['body_center']['x'].diff().fillna(0)\n",
    "             b_vy = mouse_pair['B']['body_center']['y'].diff().fillna(0)\n",
    "             X['ego_b_vx'] = b_vx * cos_t - b_vy * sin_t\n",
    "             X['ego_b_vy'] = b_vx * sin_t + b_vy * cos_t\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_behavior_specific_features(X, mouse_data, behavior_type, fps):\n",
    "    \"\"\"\n",
    "    ThÃªm features Ä‘áº·c trÆ°ng riÃªng cho tá»«ng loáº¡i hÃ nh vi\n",
    "    \"\"\"\n",
    "    \n",
    "    if behavior_type == 'single':\n",
    "        # REAR (Äá»©ng dáº­y 2 chÃ¢n sau)\n",
    "        # Äáº·c trÆ°ng: ThÃ¢n cao lÃªn, Ä‘áº§u cao, tá»‘c Ä‘á»™ cháº­m\n",
    "        if 'body_center' in mouse_data.columns.get_level_values(0):\n",
    "            cx = mouse_data['body_center']['x']\n",
    "            cy = mouse_data['body_center']['y']\n",
    "            \n",
    "            # Äá»™ cao trung bÃ¬nh cá»§a body_center\n",
    "            X['body_height_mean'] = cy.rolling(_scale(30, fps)).mean()\n",
    "            X['body_height_std'] = cy.rolling(_scale(30, fps)).std()\n",
    "            \n",
    "            # Rear thÆ°á»ng cÃ³ tá»‘c Ä‘á»™ di chuyá»ƒn X gáº§n 0 (Ä‘á»©ng yÃªn táº¡i chá»—)\n",
    "            X['horizontal_stillness'] = (\n",
    "                cx.diff().abs().rolling(_scale(20, fps)).mean()\n",
    "            )\n",
    "    \n",
    "    elif behavior_type == 'pair':\n",
    "        if 'body_center' not in mouse_data['A'].columns.get_level_values(0):\n",
    "            return X\n",
    "        \n",
    "        # CHASE (Äuá»•i báº¯t)\n",
    "        # Äáº·c trÆ°ng: A cháº¡y nhanh Vá»€ PHÃA B, khoáº£ng cÃ¡ch thu háº¹p nhanh\n",
    "        A_cx = mouse_data['A']['body_center']['x']\n",
    "        A_cy = mouse_data['A']['body_center']['y']\n",
    "        B_cx = mouse_data['B']['body_center']['x']\n",
    "        B_cy = mouse_data['B']['body_center']['y']\n",
    "        \n",
    "        dist = np.sqrt((A_cx - B_cx)**2 + (A_cy - B_cy)**2)\n",
    "        \n",
    "        # Tá»‘c Ä‘á»™ thay Ä‘á»•i khoáº£ng cÃ¡ch (cÃ ng Ã¢m = Ä‘ang Ä‘uá»•i ká»‹p)\n",
    "        X['chase_closing_speed'] = -dist.diff() * fps\n",
    "        \n",
    "        # Chase cÃ³ tÃ­nh tuáº§n hoÃ n: gáº§n â†’ xa â†’ gáº§n\n",
    "        X['chase_periodicity'] = dist.rolling(_scale(60, fps)).apply(\n",
    "            lambda x: len(np.where(np.diff(np.sign(np.diff(x))))[0])  # Äáº¿m sá»‘ láº§n Ä‘á»•i chiá»u\n",
    "        )\n",
    "        \n",
    "        # === SUBMIT (Chá»‹u thua) ===\n",
    "        # Äáº·c trÆ°ng: Náº±m báº¹p, Ã­t cá»­ Ä‘á»™ng, B á»Ÿ trÃªn/gáº§n A\n",
    "        A_speed = np.sqrt(A_cx.diff()**2 + A_cy.diff()**2)\n",
    "        B_speed = np.sqrt(B_cx.diff()**2 + B_cy.diff()**2)\n",
    "        \n",
    "        # A gáº§n nhÆ° Ä‘á»©ng yÃªn\n",
    "        X['submit_immobility'] = (A_speed < 0.5).astype(float).rolling(\n",
    "            _scale(30, fps)\n",
    "        ).mean()\n",
    "        \n",
    "        # B di chuyá»ƒn nhiá»u hÆ¡n A (Ä‘ang thá»‘ng trá»‹)\n",
    "        X['submit_dominance_ratio'] = B_speed / (A_speed + 1e-6)\n",
    "        \n",
    "        # ESCAPE (Bá» cháº¡y)\n",
    "        # Äáº·c trÆ°ng: A cháº¡y ráº¥t nhanh RA XA B\n",
    "        rel_x = A_cx - B_cx\n",
    "        rel_y = A_cy - B_cy\n",
    "        rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "        \n",
    "        # Vector váº­n tá»‘c A\n",
    "        A_vx = A_cx.diff()\n",
    "        A_vy = A_cy.diff()\n",
    "        \n",
    "        # Kiá»ƒm tra A cÃ³ cháº¡y THáº²NG ra xa B khÃ´ng\n",
    "        # (Cosine similarity giá»¯a hÆ°á»›ng cháº¡y vÃ  vector xa B)\n",
    "        escape_alignment = (A_vx * rel_x + A_vy * rel_y) / (\n",
    "            np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6\n",
    "        )\n",
    "        \n",
    "        # Escape: alignment dÆ°Æ¡ng + tá»‘c Ä‘á»™ cao\n",
    "        X['escape_score'] = escape_alignment * A_speed\n",
    "        X['escape_acceleration'] = A_speed.diff()  # TÄƒng tá»‘c Ä‘á»™t ngá»™t\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "803e05a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.204707Z",
     "iopub.status.busy": "2025-12-14T18:11:44.204300Z",
     "iopub.status.idle": "2025-12-14T18:11:44.208693Z",
     "shell.execute_reply": "2025-12-14T18:11:44.208038Z"
    },
    "papermill": {
     "duration": 0.012763,
     "end_time": "2025-12-14T18:11:44.209889",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.197126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_context(X):\n",
    "    # ThÃªm ngá»¯ cáº£nh quÃ¡ khá»©/tÆ°Æ¡ng lai cho cÃ¡c biáº¿n quan trá»ng\n",
    "    # Chá»n ra vÃ i biáº¿n quan trá»ng nháº¥t Ä‘á»ƒ táº¡o lag (Ä‘á»¡ náº·ng RAM)\n",
    "    cols_to_lag = []\n",
    "    \n",
    "    # Æ¯u tiÃªn cÃ¡c biáº¿n tá»‘c Ä‘á»™ vÃ  khoáº£ng cÃ¡ch\n",
    "    for c in ['sp_m20', 'd_m30', 'angle_diff', 'head_area']: \n",
    "        if c in X.columns:\n",
    "            cols_to_lag.append(c)\n",
    "\n",
    "    for col in cols_to_lag:\n",
    "        # Láº¥y thÃ´ng tin 10 frame trÆ°á»›c (khoáº£ng 0.3s)\n",
    "        X[f'{col}_prev10'] = X[col].shift(10).fillna(method='bfill')\n",
    "        # Láº¥y thÃ´ng tin 10 frame sau (Future context - ráº¥t máº¡nh cho Offline detection)\n",
    "        X[f'{col}_next10'] = X[col].shift(-10).fillna(method='ffill')\n",
    "        # TÃ­nh Ä‘á»™ thay Ä‘á»•i (Delta)\n",
    "        X[f'{col}_delta'] = X[col] - X[f'{col}_prev10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1769c38c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.224114Z",
     "iopub.status.busy": "2025-12-14T18:11:44.223940Z",
     "iopub.status.idle": "2025-12-14T18:11:44.238528Z",
     "shell.execute_reply": "2025-12-14T18:11:44.238056Z"
    },
    "papermill": {
     "duration": 0.022925,
     "end_time": "2025-12-14T18:11:44.239440",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.216515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "\n",
    "    X = pd.DataFrame({\n",
    "        f\"{p1}+{p2}\": np.square(single_mouse[p1] - single_mouse[p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n",
    "        if p1 in available_body_parts and p2 in available_body_parts\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n",
    "\n",
    "    # TÃ­nh diá»‡n tÃ­ch tam giÃ¡c Ä‘áº§u (Nose - EarL - EarR)\n",
    "    # phÃ¡t hiá»‡n hÃ nh vi REAR (Ä‘á»©ng lÃªn) hoáº·c cÃºi Ä‘áº§u\n",
    "    if all(p in available_body_parts for p in ['nose', 'ear_left', 'ear_right']):\n",
    "        x1, y1 = single_mouse['nose']['x'], single_mouse['nose']['y']\n",
    "        x2, y2 = single_mouse['ear_left']['x'], single_mouse['ear_left']['y']\n",
    "        x3, y3 = single_mouse['ear_right']['x'], single_mouse['ear_right']['y']\n",
    "        \n",
    "        X['head_area'] = 0.5 * np.abs(x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2))\n",
    "        \n",
    "        # Biáº¿n thiÃªn diá»‡n tÃ­ch (Ä‘ang co láº¡i hay to ra)\n",
    "        X['head_area_change'] = X['head_area'].diff()\n",
    "        \n",
    "    \n",
    "    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        lag = _scale(10, fps)\n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_lf': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n",
    "            'sp_rt': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n",
    "            'sp_lf2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "            'sp_rt2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose'] - single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "        X['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (\n",
    "            np.sqrt(v1['x']**2 + v1['y']**2) * np.sqrt(v2['x']**2 + v2['y']**2) + 1e-6)\n",
    "\n",
    "    if 'body_center' in available_body_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'cx_m{w}'] = cx.rolling(ws, **roll).mean()\n",
    "            X[f'cy_m{w}'] = cy.rolling(ws, **roll).mean()\n",
    "            X[f'cx_s{w}'] = cx.rolling(ws, **roll).std()\n",
    "            X[f'cy_s{w}'] = cy.rolling(ws, **roll).std()\n",
    "            X[f'x_rng{w}'] = cx.rolling(ws, **roll).max() - cx.rolling(ws, **roll).min()\n",
    "            X[f'y_rng{w}'] = cy.rolling(ws, **roll).max() - cy.rolling(ws, **roll).min()\n",
    "            X[f'disp{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).sum()**2 +\n",
    "                                     cy.diff().rolling(ws, min_periods=1).sum()**2)\n",
    "            X[f'act{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).var() +\n",
    "                                   cy.diff().rolling(ws, min_periods=1).var())\n",
    "\n",
    "        X = add_curvature_features(X, cx, cy, fps)\n",
    "        X = add_multiscale_features(X, cx, cy, fps)\n",
    "        X = add_state_features(X, cx, cy, fps)\n",
    "        X = add_longrange_features(X, cx, cy, fps)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n",
    "        nt_dist = np.sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n",
    "                          (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nt_lg{lag}'] = nt_dist.shift(l)\n",
    "            X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 +\n",
    "                        (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n",
    "        for off in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "        w = _scale(30, fps)\n",
    "        X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "    X = add_behavior_specific_features(X, single_mouse, 'single', fps)\n",
    "\n",
    "    add_context(X)\n",
    "    X = X.fillna(X.median())\n",
    "    X = X.astype(np.float16, copy=False)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85024aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.253410Z",
     "iopub.status.busy": "2025-12-14T18:11:44.253174Z",
     "iopub.status.idle": "2025-12-14T18:11:44.273591Z",
     "shell.execute_reply": "2025-12-14T18:11:44.273089Z"
    },
    "papermill": {
     "duration": 0.028762,
     "end_time": "2025-12-14T18:11:44.274607",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.245845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    avail_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    avail_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "\n",
    "    X = pd.DataFrame({\n",
    "        f\"12+{p1}+{p2}\": np.square(mouse_pair['A'][p1] - mouse_pair['B'][p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n",
    "        if p1 in avail_A and p2 in avail_B\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shA = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shB = mouse_pair['B']['ear_left'].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_A': np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False),\n",
    "            'sp_AB': np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "            'sp_B': np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    # TÃ­nh vector hÆ°á»›ng cÆ¡ thá»ƒ (Nose -> Tail base)\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        # Vector A\n",
    "        vec_A_x = mouse_pair['A']['nose']['x'] - mouse_pair['A']['tail_base']['x']\n",
    "        vec_A_y = mouse_pair['A']['nose']['y'] - mouse_pair['A']['tail_base']['y']\n",
    "        # Vector B\n",
    "        vec_B_x = mouse_pair['B']['nose']['x'] - mouse_pair['B']['tail_base']['x']\n",
    "        vec_B_y = mouse_pair['B']['nose']['y'] - mouse_pair['B']['tail_base']['y']\n",
    "        \n",
    "        # GÃ³c cá»§a tá»«ng con so vá»›i trá»¥c tá»a Ä‘á»™\n",
    "        ang_A = np.arctan2(vec_A_y, vec_A_x)\n",
    "        ang_B = np.arctan2(vec_B_y, vec_B_x)\n",
    "        \n",
    "        # GÃ³c tÆ°Æ¡ng Ä‘á»‘i giá»¯a 2 con (tá»« 0 Ä‘áº¿n PI)\n",
    "        # GiÃºp phÃ¢n biá»‡t: Äá»‘i Ä‘áº§u vs Äuá»•i theo vs VuÃ´ng gÃ³c\n",
    "        X['angle_diff'] = np.abs(np.arctan2(np.sin(ang_A - ang_B), np.cos(ang_A - ang_B)))\n",
    "\n",
    "    # Khoáº£ng cÃ¡ch chÃ©o: MÅ©i A Ä‘áº¿n ÄuÃ´i B (vÃ  ngÆ°á»£c láº¡i)\n",
    "    if ('nose' in avail_A and 'tail_base' in avail_B):\n",
    "        X['dist_noseA_tailB'] = np.sqrt(\n",
    "            (mouse_pair['A']['nose']['x'] - mouse_pair['B']['tail_base']['x'])**2 + \n",
    "            (mouse_pair['A']['nose']['y'] - mouse_pair['B']['tail_base']['y'])**2\n",
    "        )\n",
    "        \n",
    "    if ('nose' in avail_B and 'tail_base' in avail_A):\n",
    "        X['dist_noseB_tailA'] = np.sqrt(\n",
    "            (mouse_pair['B']['nose']['x'] - mouse_pair['A']['tail_base']['x'])**2 + \n",
    "            (mouse_pair['B']['nose']['y'] - mouse_pair['A']['tail_base']['y'])**2\n",
    "        )\n",
    "        \n",
    "    #*********************************************\n",
    "    \n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n",
    "            np.sqrt(dir_A['x']**2 + dir_A['y']**2) * np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n",
    "\n",
    "    if all(p in avail_A for p in ['nose']) and all(p in avail_B for p in ['nose']):\n",
    "        cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n",
    "        lag = _scale(10, fps)\n",
    "        shA_n = mouse_pair['A']['nose'].shift(lag)\n",
    "        shB_n = mouse_pair['B']['nose'].shift(lag)\n",
    "        past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n",
    "        X['appr'] = cur - past\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd = np.sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "                     (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "        X['v_cls'] = (cd < 5.0).astype(float)\n",
    "        X['cls']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n",
    "        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "        X['far']   = (cd >= 30.0).astype(float)\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd_full = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'd_m{w}']  = cd_full.rolling(ws, **roll).mean()\n",
    "            X[f'd_s{w}']  = cd_full.rolling(ws, **roll).std()\n",
    "            X[f'd_mn{w}'] = cd_full.rolling(ws, **roll).min()\n",
    "            X[f'd_mx{w}'] = cd_full.rolling(ws, **roll).max()\n",
    "\n",
    "            d_var = cd_full.rolling(ws, **roll).var()\n",
    "            X[f'int{w}'] = 1 / (1 + d_var)\n",
    "\n",
    "            Axd = mouse_pair['A']['body_center']['x'].diff()\n",
    "            Ayd = mouse_pair['A']['body_center']['y'].diff()\n",
    "            Bxd = mouse_pair['B']['body_center']['x'].diff()\n",
    "            Byd = mouse_pair['B']['body_center']['y'].diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{w}'] = coord.rolling(ws, **roll).mean()\n",
    "            X[f'co_s{w}'] = coord.rolling(ws, **roll).std()\n",
    "\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn = np.sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n",
    "                     (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}']  = nn.shift(l)\n",
    "            X[f'nn_ch{lag}']  = nn - nn.shift(l)\n",
    "            is_cl = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}']  = is_cl.rolling(l, min_periods=1).mean()\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        Avx = mouse_pair['A']['body_center']['x'].diff()\n",
    "        Avy = mouse_pair['A']['body_center']['y'].diff()\n",
    "        Bvx = mouse_pair['B']['body_center']['x'].diff()\n",
    "        Bvy = mouse_pair['B']['body_center']['y'].diff()\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "\n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'va_{off}'] = val.shift(-o)\n",
    "\n",
    "        w = _scale(30, fps)\n",
    "        X['int_con'] = cd_full.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (cd_full.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "        X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "\n",
    "        X = add_egocentric_features(X, mouse_pair, avail_A, avail_B)\n",
    "\n",
    "    X = add_behavior_specific_features(X, mouse_pair, 'pair', fps)\n",
    "\n",
    "    \n",
    "    add_context(X)\n",
    "    X = X.fillna(X.median())\n",
    "    X = X.astype(np.float16, copy=False)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdecd742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.288990Z",
     "iopub.status.busy": "2025-12-14T18:11:44.288663Z",
     "iopub.status.idle": "2025-12-14T18:11:44.298097Z",
     "shell.execute_reply": "2025-12-14T18:11:44.297549Z"
    },
    "papermill": {
     "duration": 0.017839,
     "end_time": "2025-12-14T18:11:44.299055",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.281216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def robustify(submission, dataset, traintest, traintest_directory=None):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "\n",
    "\n",
    "    old_submission = submission.copy()\n",
    "    # simple sanity check: start < stop\n",
    "    submission = submission[submission.start_frame < submission.stop_frame]\n",
    "\n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped frames with start >= stop\")\n",
    "    \n",
    "    # An agent (Mouse A) cannot perform two different actions on the SAME Target (Mouse B) at the SAME time.\n",
    "    old_submission = submission.copy()\n",
    "    group_list = []\n",
    "\n",
    "    # iterate by (video, agent, target)\n",
    "    for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n",
    "        # sort by time (start_frame)\n",
    "        group = group.sort_values('start_frame')\n",
    "\n",
    "        mask = np.ones(len(group), dtype=bool)  # giá»¯ láº¡i dÃ²ng khÃ´ng bá»‹ overlap\n",
    "        last_stop_frame = 0\n",
    "\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            # Náº¿u start < stop cá»§a Ä‘oáº¡n trÆ°á»›c â†’ overlap â†’ bá»\n",
    "            if row['start_frame'] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                # cáº­p nháº­t last_stop_frame\n",
    "                last_stop_frame = row['stop_frame']\n",
    "\n",
    "        group_list.append(group[mask])\n",
    "        \n",
    "    submission = pd.concat(group_list)\n",
    "    \n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped duplicate frames\")\n",
    "        \n",
    "\n",
    "    # ----------------------------- #\n",
    "    # 3) Xá»­ lÃ½ cÃ¡c video *khÃ´ng cÃ³ báº¥t ká»³ prediction nÃ o*\n",
    "    #    NhÆ°ng cÃ³ nhÃ£n trong dataset\n",
    "    #    â†’ Táº¡o cÃ¡c prediction dummy Ä‘á»ƒ trÃ¡nh lá»—i submission\n",
    "    # ----------------------------- #\n",
    "    s_list = []\n",
    "\n",
    "    # Duyá»‡t táº¥t cáº£ video trong dataset\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row['lab_id']\n",
    "\n",
    "        # Bá» qua video thuá»™c MABe22 (khÃ´ng cáº§n robustify)\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "\n",
    "        # Náº¿u video nÃ y Ä‘Ã£ cÃ³ prediction â†’ bá» qua\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "        \n",
    "        # Náº¿u hÃ ng dataset nÃ y khÃ´ng cÃ³ behaviors_labeled â†’ bá» qua\n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "\n",
    "        print(f\"Video {video_id} has no predictions.\")\n",
    "        \n",
    "        # Äá»c file tracking\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "    \n",
    "        # behaviors_labeled lÃ  list dáº¡ng chuá»—i â†’ parse & lÃ m sáº¡ch\n",
    "        vid_behaviors = json.loads(row['behaviors_labeled'])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "    \n",
    "        # TÃ­nh frame báº¯t Ä‘áº§u & káº¿t thÃºc toÃ n video\n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "    \n",
    "        # Gá»™p theo (agent, target)\n",
    "        for (agent, target), actions in vid_behaviors.groupby(['agent', 'target']):\n",
    "            # Chia Ä‘á»u Ä‘á»™ dÃ i video theo sá»‘ lÆ°á»£ng action\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "\n",
    "            # Táº¡o cÃ¡c Ä‘oáº¡n dummy\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "\n",
    "                s_list.append((video_id, agent, target, action_row['action'], batch_start, batch_stop))\n",
    "\n",
    "    # Náº¿u cÃ³ dummy predictions â†’ thÃªm vÃ o submission\n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n",
    "        ])\n",
    "        print(\"ERROR: Filled empty videos\")\n",
    "\n",
    "    # Reset index sau khi concat\n",
    "    submission = submission.reset_index(drop=True)\n",
    "    \n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb0c34d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.312815Z",
     "iopub.status.busy": "2025-12-14T18:11:44.312607Z",
     "iopub.status.idle": "2025-12-14T18:11:44.320447Z",
     "shell.execute_reply": "2025-12-14T18:11:44.319983Z"
    },
    "papermill": {
     "duration": 0.0161,
     "end_time": "2025-12-14T18:11:44.321451",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.305351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_multiclass(pred, meta, thresholds):\n",
    "    # -------------------------------------------------------\n",
    "    # pred: DataFrame (n_frames Ã— n_actions) chá»©a xÃ¡c suáº¥t tá»«ng action táº¡i má»—i frame\n",
    "    # meta: DataFrame chá»©a video_id, agent_id, target_id, video_frame\n",
    "    # thresholds: dict quy Ä‘á»‹nh ngÆ°á»¡ng cho tá»«ng action (VD: {'attack':0.32,...})\n",
    "    # Má»¥c tiÃªu: Chuyá»ƒn chuá»—i dá»± Ä‘oÃ¡n frame-by-frame â†’ cÃ¡c segment hÃ nh vi\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # use rolling mean for smooth prob\n",
    "    # 5 frames\n",
    "    pred = pred.rolling(window=5, min_periods=1, center=True).mean()\n",
    "\n",
    "    # 1) Vá»›i má»—i frame, chá»n action cÃ³ xÃ¡c suáº¥t cao nháº¥t (argmax)\n",
    "    ama = np.argmax(pred.values, axis=1)\n",
    "\n",
    "    # Láº¥y giÃ¡ trá»‹ xÃ¡c suáº¥t cao nháº¥t táº¡i frame Ä‘Ã³\n",
    "    max_proba = pred.max(axis=1).values\n",
    "\n",
    "    # 2) GÃ¡n ngÆ°á»¡ng cho tá»«ng action theo dict threshold\n",
    "    # Náº¿u action khÃ´ng cÃ³ trong dict â†’ dÃ¹ng ngÆ°á»¡ng máº·c Ä‘á»‹nh = 0.27\n",
    "    threshold_array = np.array([thresholds.get(col, 0.27) for col in pred.columns])\n",
    "\n",
    "    # Láº¥y ngÆ°á»¡ng tÆ°Æ¡ng á»©ng vá»›i action argmax táº¡i tá»«ng frame\n",
    "    action_thresholds = threshold_array[ama]\n",
    "\n",
    "    # 3) Nhá»¯ng frame cÃ³ xÃ¡c suáº¥t < threshold â†’ gÃ¡n -1 (tá»©c lÃ  \"no action\")\n",
    "    ama = np.where(max_proba >= action_thresholds, ama, -1)\n",
    "\n",
    "    # Chuyá»ƒn thÃ nh Series vá»›i index = video_frame (giÃºp xÃ¡c Ä‘á»‹nh start/stop)\n",
    "    ama = pd.Series(ama, index=meta.video_frame)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # BÆ¯á»šC 4: TÃŒM CÃC FRAME CÃ“ Sá»° THAY Äá»”I ACTION\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Máº·t náº¡: frame hiá»‡n táº¡i khÃ¡c frame trÆ°á»›c â†’ cÃ³ thay Ä‘á»•i hÃ nh vi\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "\n",
    "    # Láº¥y ra chá»‰ cÃ¡c frame nÆ¡i hÃ nh vi thay Ä‘á»•i\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # BÆ¯á»šC 5: XÃ‚Y Dá»°NG SUBMISSION Tá»ª CÃC ÄIá»‚M THAY Äá»”I\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Loáº¡i bá» cÃ¡c frame cÃ³ action = -1 (no action)\n",
    "    mask = ama_changes.values >= 0\n",
    "\n",
    "    # Frame cuá»‘i cÃ¹ng khÃ´ng thá»ƒ lÃ m start cá»§a segment â†’ bá»\n",
    "    mask[-1] = False\n",
    "\n",
    "    # Táº¡o DataFrame chá»©a cÃ¡c segment\n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes['video_id'][mask].values,\n",
    "        'agent_id': meta_changes['agent_id'][mask].values,\n",
    "        'target_id': meta_changes['target_id'][mask].values,\n",
    "        'action': pred.columns[ama_changes[mask].values], # truy xuáº¥t tÃªn action: pred.columns[index_of_action]\n",
    "        'start_frame': ama_changes.index[mask],   # start = frame táº¡i thay Ä‘á»•i\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]  # stop = frame thay Ä‘á»•i tiáº¿p theo\n",
    "    })\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # BÆ¯á»šC 6: Xá»¬ LÃ TRÆ¯á»œNG Há»¢P Bá»Š Gá»˜P NHáº¦M segment KHÃC VIDEO\n",
    "    # -------------------------------------------------------\n",
    "    # Náº¿u hai Ä‘iá»ƒm thay Ä‘á»•i náº±m á»Ÿ video/agent/target khÃ¡c nhau\n",
    "    # â†’ KhÃ´ng thá»ƒ dÃ¹ng frame káº¿ tiáº¿p lÃ m stop_frame\n",
    "    # â†’ Pháº£i set stop_frame = frame cuá»‘i cá»§a video Ä‘Ã³\n",
    "\n",
    "    stop_video_id  = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id  = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "\n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "\n",
    "        # Náº¿u khÃ¡c video/agent/target â†’ dá»«ng segment táº¡i frame cuá»‘i video\n",
    "        if stop_video_id[i] != video_id or stop_agent_id[i] != agent_id or stop_target_id[i] != target_id:\n",
    "\n",
    "            # Láº¥y frame cuá»‘i cá»§a video\n",
    "            new_stop_frame = meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "\n",
    "            # Ghi Ä‘Ã¨ stop_frame\n",
    "            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "\n",
    "    return submission_part\n",
    "\n",
    "# Chá»n action cÃ³ xÃ¡c suáº¥t lá»›n nháº¥t á»Ÿ má»—i frame (argmax).\n",
    "# Ãp dá»¥ng ngÆ°á»¡ng (threshold) riÃªng cho tá»«ng action Ä‘á»ƒ loáº¡i bá» cÃ¡c frame cÃ³ Ä‘á»™ tin cáº­y tháº¥p (gÃ¡n thÃ nh -1, tá»©c \"no action\").\n",
    "# TÃ¬m cÃ¡c Ä‘iá»ƒm thay Ä‘á»•i hÃ nh vi theo thá»i gian, vÃ­ dá»¥: no action â†’ attack â†’ chase â†’ ....\n",
    "# GhÃ©p cÃ¡c Ä‘iá»ƒm thay Ä‘á»•i thÃ nh cÃ¡c segment:\n",
    "# start_frame = thá»i Ä‘iá»ƒm báº¯t Ä‘áº§u hÃ nh Ä‘á»™ng má»›i\n",
    "# stop_frame = thá»i Ä‘iá»ƒm hÃ nh Ä‘á»™ng tiáº¿p theo xuáº¥t hiá»‡n\n",
    "# Sá»­a lá»—i khi frame thay Ä‘á»•i rÆ¡i sang video khÃ¡c, Ä‘áº£m báº£o stop_frame khÃ´ng bao giá» thuá»™c video khÃ¡c.\n",
    "# Xuáº¥t ra DataFrame chuáº©n submission:\n",
    "# video_id, agent_id, target_id, action, start_frame, stop_frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71340586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.337220Z",
     "iopub.status.busy": "2025-12-14T18:11:44.336992Z",
     "iopub.status.idle": "2025-12-14T18:11:44.341432Z",
     "shell.execute_reply": "2025-12-14T18:11:44.340786Z"
    },
    "papermill": {
     "duration": 0.013708,
     "end_time": "2025-12-14T18:11:44.342546",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.328838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def tune_threshold(oof_action, y_action):\n",
    "    # simple linear scan\n",
    "    thresholds = np.arange(0.1, 0.9, 0.02) \n",
    "    best_threshold = 0.5\n",
    "    best_score = -1\n",
    "    \n",
    "    for th in thresholds:\n",
    "        score = f1_score(y_action, (oof_action >= th), zero_division=0)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = th\n",
    "            \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cfa5269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.358970Z",
     "iopub.status.busy": "2025-12-14T18:11:44.358694Z",
     "iopub.status.idle": "2025-12-14T18:11:44.368283Z",
     "shell.execute_reply": "2025-12-14T18:11:44.367509Z"
    },
    "papermill": {
     "duration": 0.019333,
     "end_time": "2025-12-14T18:11:44.369503",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.350170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate_classifier(X, label, meta, body_parts_tracked_str, section):\n",
    "    oof = pd.DataFrame(index=meta.video_frame)\n",
    "    f1_list = []\n",
    "    submission_list = []\n",
    "    thresholds = {}\n",
    "    \n",
    "    # Láº¥y danh sÃ¡ch 3 mÃ´ hÃ¬nh\n",
    "    models = get_model_zoo() \n",
    "    \n",
    "    for action in label.columns:\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        X_action = X[action_mask]\n",
    "        groups_action = meta.video_id[action_mask]\n",
    "        \n",
    "        if len(np.unique(groups_action)) < CFG.n_splits:\n",
    "            continue\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            try:\n",
    "                # Biáº¿n chá»©a tá»•ng xÃ¡c suáº¥t cá»§a cáº£ 3 mÃ´ hÃ¬nh\n",
    "                ensemble_oof_preds = np.zeros(len(y_action))\n",
    "                \n",
    "                # Duyá»‡t qua tá»«ng mÃ´ hÃ¬nh (xgb, cat, lgbm)\n",
    "                for name, model_template in models:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings('ignore')\n",
    "                        \n",
    "                        # ÄÆ°á»ng dáº«n lÆ°u riÃªng cho tá»«ng model\n",
    "                        # VD: xgboost/1/attack, catboost/1/attack...\n",
    "                        save_path = f\"{CFG.model_name}/{name}/{section}/{action}\"\n",
    "                        \n",
    "                        trainer = Trainer(\n",
    "                            estimator=clone(model_template),\n",
    "                            cv=CFG.cv,\n",
    "                            cv_args={\"groups\": groups_action},\n",
    "                            metric=f1_score,\n",
    "                            task=\"binary\",\n",
    "                            verbose=False,\n",
    "                            save=True,\n",
    "                            save_path=save_path\n",
    "                        )\n",
    "\n",
    "                        trainer.fit(X_action, y_action)\n",
    "                        \n",
    "                        # Cá»™ng dá»“n káº¿t quáº£ (Chia trung bÃ¬nh sau)\n",
    "                        ensemble_oof_preds += trainer.oof_preds\n",
    "                        \n",
    "                        # LÆ°u file OOF láº» (náº¿u cáº§n debug)\n",
    "                        joblib.dump(trainer.oof_preds, f\"{save_path}/oof.pkl\")\n",
    "                        del trainer\n",
    "                        gc.collect()\n",
    "                \n",
    "                # Láº¥y trung bÃ¬nh cá»™ng (Soft Voting)\n",
    "                ensemble_oof_preds /= len(models) \n",
    "\n",
    "                # --- Tá»I Æ¯U NGÆ¯á» NG TRÃŠN Káº¾T QUáº¢ Tá»”NG Há»¢P ---\n",
    "                threshold = tune_threshold(ensemble_oof_preds, y_action)\n",
    "                thresholds[action] = threshold\n",
    "        \n",
    "                f1 = f1_score(y_action, (ensemble_oof_preds >= threshold), zero_division=0)\n",
    "                f1_list.append((body_parts_tracked_str, action, f1))\n",
    "                \n",
    "                print(f\"\\t[Ensemble] F1: {f1:.4f} ({threshold:.2f}) - {action}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # In lá»—i chi tiáº¿t Ä‘á»ƒ debug\n",
    "                print(f\"\\t!!! Error {action}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                oof_action = np.zeros(len(y_action))\n",
    "        \n",
    "        else:\n",
    "            ensemble_oof_preds = np.zeros(len(y_action))\n",
    "        \n",
    "        # LÆ°u káº¿t quáº£ ensemble vÃ o báº£ng OOF tá»•ng\n",
    "        oof_column = np.zeros(len(label))\n",
    "        oof_column[action_mask] = ensemble_oof_preds\n",
    "        oof[action] = oof_column\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    submission_part = predict_multiclass(oof, meta, thresholds)\n",
    "    submission_list.append(submission_part)\n",
    "    \n",
    "    return submission_list, f1_list, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf61223b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.385387Z",
     "iopub.status.busy": "2025-12-14T18:11:44.385187Z",
     "iopub.status.idle": "2025-12-14T18:11:44.393992Z",
     "shell.execute_reply": "2025-12-14T18:11:44.393224Z"
    },
    "papermill": {
     "duration": 0.01827,
     "end_time": "2025-12-14T18:11:44.395080",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.376810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ThÃªm feature_map\n",
    "def submit(body_parts_tracked_str, switch_tr, section, thresholds, feature_map=None):    \n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "        \n",
    "    test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "    \n",
    "    fps_lookup = (\n",
    "        test_subset[['video_id', 'frames_per_second']]\n",
    "        .drop_duplicates('video_id')\n",
    "        .set_index('video_id')['frames_per_second']\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    model_names = ['xgb', 'cat', 'lgbm']\n",
    "    submission_list = []\n",
    "    \n",
    "    generator = generate_mouse_data(\n",
    "        test_subset, 'test',\n",
    "        generate_single=(switch_tr == 'single'), \n",
    "        generate_pair=(switch_tr == 'pair')\n",
    "    )\n",
    "\n",
    "    for switch_te, data_te, meta_te, actions_te in generator:\n",
    "        try:\n",
    "            fps_i = _fps_from_meta(meta_te, fps_lookup)\n",
    "            if switch_te == 'single':\n",
    "                X_te = transform_single(data_te, body_parts_tracked, fps_i)\n",
    "            else:\n",
    "                X_te = transform_pair(data_te, body_parts_tracked, fps_i)\n",
    "            \n",
    "            # FIX  FEATURE MISMATCH \n",
    "            if feature_map is not None:\n",
    "                # Láº¥y danh sÃ¡ch cá»™t tá»« file Ä‘Ã£ lÆ°u lÃºc train\n",
    "                # Key: \"1_single\" hoáº·c \"1_pair\"\n",
    "                key = f\"{section}_{switch_tr}\"\n",
    "                required_cols = feature_map.get(key)\n",
    "                \n",
    "                if required_cols is not None:\n",
    "                    # bá» cá»™t thá»«a á»Ÿ Test mÃ  Train khÃ´ng cÃ³\n",
    "                    # thÃªm cá»™t thiáº¿u vÃ  fill 0\n",
    "                    # sáº¯p xáº¿p Ä‘Ãºng thá»© tá»±\n",
    "                    X_te = X_te.reindex(columns=required_cols, fill_value=0)\n",
    "            \n",
    "            del data_te\n",
    "            gc.collect()\n",
    "    \n",
    "            pred = pd.DataFrame(index=meta_te.video_frame)\n",
    "            \n",
    "            for action in actions_te:\n",
    "                ensemble_prob = np.zeros(len(X_te))\n",
    "                valid_models_count = 0\n",
    "                \n",
    "                for name in model_names:\n",
    "                    path_pattern = f\"{CFG.model_name}/{name}/{section}/{action}/*_trainer_*.pkl\"\n",
    "                    files = glob.glob(path_pattern)\n",
    "                    \n",
    "                    if len(files) >= 1:\n",
    "                        trainer = joblib.load(files[0])\n",
    "                        ensemble_prob += trainer.predict(X_te)\n",
    "                        valid_models_count += 1\n",
    "                        del trainer\n",
    "                \n",
    "                if valid_models_count > 0:\n",
    "                    pred[action] = ensemble_prob / valid_models_count\n",
    "                else:\n",
    "                    pred[action] = 0.0\n",
    "                \n",
    "            del X_te\n",
    "            gc.collect()\n",
    "\n",
    "            if pred.shape[1] != 0:\n",
    "                submission_part = predict_multiclass(pred, meta_te, thresholds)\n",
    "                submission_list.append(submission_part)\n",
    "                \n",
    "        except KeyError:\n",
    "            del data_te\n",
    "            gc.collect()\n",
    "            \n",
    "    return submission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "966cb4dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.411379Z",
     "iopub.status.busy": "2025-12-14T18:11:44.410679Z",
     "iopub.status.idle": "2025-12-14T18:11:44.416626Z",
     "shell.execute_reply": "2025-12-14T18:11:44.416113Z"
    },
    "papermill": {
     "duration": 0.015354,
     "end_time": "2025-12-14T18:11:44.417788",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.402434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth_predictions(submission, min_duration_frames=2, merge_gap_frames=3):\n",
    "    \"\"\"\n",
    "    Lá»c bá» cÃ¡c segment quÃ¡ ngáº¯n vÃ  gá»™p cÃ¡c segment gáº§n nhau\n",
    "        min_duration_frames: Bá» segment ngáº¯n hÆ¡n N frames ()\n",
    "        merge_gap_frames: Gá»™p náº¿u khoáº£ng cÃ¡ch < N frames\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for (video, agent, target, action), group in submission.groupby(\n",
    "        ['video_id', 'agent_id', 'target_id', 'action']\n",
    "    ):\n",
    "        group = group.sort_values('start_frame')\n",
    "        \n",
    "        segments = []\n",
    "        for _, row in group.iterrows():\n",
    "            duration = row['stop_frame'] - row['start_frame']\n",
    "            \n",
    "            #skip segment quÃ¡ ngáº¯n\n",
    "            if duration < min_duration_frames:\n",
    "                continue\n",
    "            \n",
    "            # merge vÃ o segment trÆ°á»›c náº¿u gáº§n\n",
    "            if segments and (row['start_frame'] - segments[-1][1]) <= merge_gap_frames:\n",
    "                segments[-1] = (segments[-1][0], row['stop_frame'])\n",
    "            else:\n",
    "                segments.append((row['start_frame'], row['stop_frame']))\n",
    "        \n",
    "        for start, stop in segments:\n",
    "            result.append({\n",
    "                'video_id': video, 'agent_id': agent,\n",
    "                'target_id': target, 'action': action,\n",
    "                'start_frame': start, 'stop_frame': stop\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36cdd5fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.433523Z",
     "iopub.status.busy": "2025-12-14T18:11:44.433095Z",
     "iopub.status.idle": "2025-12-14T18:11:44.446918Z",
     "shell.execute_reply": "2025-12-14T18:11:44.446180Z"
    },
    "papermill": {
     "duration": 0.022952,
     "end_time": "2025-12-14T18:11:44.448072",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.425120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_validation(dataset, body_parts_tracked_list):\n",
    "    print(f\" running validation\")\n",
    "    \n",
    "    f1_list = []\n",
    "    submission_list = []\n",
    "    learned_thresholds = {\"single\": {}, \"pair\": {}}\n",
    "    # store features used for training\n",
    "    feature_map = {} \n",
    "\n",
    "    for section in range(1, len(body_parts_tracked_list)):\n",
    "        body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "        try:\n",
    "            body_parts = json.loads(body_parts_tracked_str)\n",
    "            print(f\">>> Processing Section {section}/{len(body_parts_tracked_list)-1}\")\n",
    "            \n",
    "            if len(body_parts) > 5:\n",
    "                body_parts_tracked_cleaned = [b for b in body_parts if b not in drop_body_parts]\n",
    "            \n",
    "            subset = dataset[dataset.body_parts_tracked == body_parts_tracked_str]\n",
    "            \n",
    "            _fps_lookup = (\n",
    "                subset[['video_id', 'frames_per_second']]\n",
    "                .drop_duplicates('video_id')\n",
    "                .set_index('video_id')['frames_per_second']\n",
    "                .to_dict()\n",
    "            )\n",
    "            \n",
    "            # --- Gom dá»¯ liá»‡u Single & Pair ---\n",
    "            single_data, single_meta, single_label = [], [], []\n",
    "            pair_data, pair_meta, pair_label = [], [], []\n",
    "            \n",
    "            for switch, data, meta, label in generate_mouse_data(subset, 'train'):\n",
    "                if switch == 'single':\n",
    "                    single_data.append(data); single_meta.append(meta); single_label.append(label)\n",
    "                else:\n",
    "                    pair_data.append(data); pair_meta.append(meta); pair_label.append(label)\n",
    "                del data, meta, label\n",
    "            gc.collect()\n",
    "\n",
    "            # --- A. Xá»¬ LÃ SINGLE MOUSE (CÃ“ Cáº®T GIáº¢M Dá»® LIá»†U) ---\n",
    "            if len(single_data) > 0:\n",
    "                print(f\"   [Single Mouse] Training...\")\n",
    "                X_list, y_list, meta_list = [], [], []\n",
    "                \n",
    "                # Duyá»‡t qua tá»«ng video\n",
    "                for d, m, l in zip(single_data, single_meta, single_label):\n",
    "                    fps = _fps_from_meta(m, _fps_lookup)\n",
    "                    # Táº¡o feature\n",
    "                    feat = transform_single(d, body_parts_tracked_cleaned, fps)\n",
    "                    \n",
    "                    # === Cáº®T NHá»Ž Dá»® LIá»†U Táº I ÄÃ‚Y (Downsampling) ===\n",
    "                    # Chá»‰ láº¥y má»—i 2 dÃ²ng 1 dÃ²ng (Step = 2)\n",
    "                    indices = np.arange(0, len(feat), 2)\n",
    "                    \n",
    "                    X_list.append(feat.iloc[indices])\n",
    "                    y_list.append(l.iloc[indices])\n",
    "                    meta_list.append(m.iloc[indices])\n",
    "                \n",
    "                # Ná»‘i láº¡i (LÃºc nÃ y dá»¯ liá»‡u Ä‘Ã£ nhá», khÃ´ng sá»£ sáº­p RAM)\n",
    "                X_tr = pd.concat(X_list, ignore_index=True).astype(np.float32)\n",
    "                X_tr = X_tr.replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                y_tr = pd.concat(y_list, ignore_index=True)\n",
    "                meta_tr = pd.concat(meta_list, ignore_index=True)\n",
    "                \n",
    "                # XÃ³a list cÅ©\n",
    "                del single_data, single_meta, single_label, X_list, y_list, meta_list\n",
    "                gc.collect()\n",
    "                \n",
    "                # Train\n",
    "                # === [Sá»¬A 2] LÆ¯U DANH SÃCH Cá»˜T SINGLE ===\n",
    "                feature_map[f\"{section}_single\"] = X_tr.columns.tolist()\n",
    "                # ========================================\n",
    "\n",
    "                subs, f1s, threshs = cross_validate_classifier(X_tr, y_tr, meta_tr, body_parts_tracked_str, section)\n",
    "                \n",
    "                f1_list.extend(f1s)\n",
    "                submission_list.extend(subs)\n",
    "                learned_thresholds[\"single\"][str(section)] = threshs\n",
    "                \n",
    "                del X_tr, y_tr, meta_tr\n",
    "                gc.collect()\n",
    "\n",
    "            # --- B. Xá»¬ LÃ PAIR MOUSE (CÃ“ Cáº®T GIáº¢M Dá»® LIá»†U) ---\n",
    "            if len(pair_data) > 0:\n",
    "                print(f\"   [Pair Mouse] Training...\")\n",
    "                X_list, y_list, meta_list = [], [], []\n",
    "                \n",
    "                for d, m, l in zip(pair_data, pair_meta, pair_label):\n",
    "                    fps = _fps_from_meta(m, _fps_lookup)\n",
    "                    feat = transform_pair(d, body_parts_tracked_cleaned, fps)\n",
    "                    \n",
    "                    # === Cáº®T NHá»Ž Dá»® LIá»†U Táº I ÄÃ‚Y ===\n",
    "                    indices = np.arange(0, len(feat), 5)\n",
    "                    \n",
    "                    X_list.append(feat.iloc[indices])\n",
    "                    y_list.append(l.iloc[indices])\n",
    "                    meta_list.append(m.iloc[indices])\n",
    "                \n",
    "                X_tr = pd.concat(X_list, ignore_index=True).astype(np.float32)\n",
    "                X_tr = X_tr.replace([np.inf, -np.inf], np.nan)\n",
    "                y_tr = pd.concat(y_list, ignore_index=True)\n",
    "                meta_tr = pd.concat(meta_list, ignore_index=True)\n",
    "                \n",
    "                del pair_data, pair_meta, pair_label, X_list, y_list, meta_list\n",
    "                gc.collect()\n",
    "                \n",
    "                feature_map[f\"{section}_pair\"] = X_tr.columns.tolist()\n",
    "                # ======================================\n",
    "\n",
    "                subs, f1s, threshs = cross_validate_classifier(X_tr, y_tr, meta_tr, body_parts_tracked_str, section)\n",
    "                \n",
    "                f1_list.extend(f1s)\n",
    "                submission_list.extend(subs)\n",
    "                learned_thresholds[\"pair\"][str(section)] = threshs\n",
    "                \n",
    "                del X_tr, y_tr, meta_tr\n",
    "                gc.collect()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"!!! Lá»–I táº¡i Section {section}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    if f1_list:\n",
    "        f1_df = pd.DataFrame(f1_list, columns=['config', 'action', 'f1'])\n",
    "        if CFG.mode == 'validate':\n",
    "            print(\"\\n=== Káº¾T QUáº¢ VALIDATION ===\")\n",
    "            print(f\"Mean F1 Score: {f1_df['f1'].mean():.4f}\")\n",
    "        \n",
    "        joblib.dump(learned_thresholds, f\"{CFG.model_name}_thresholds.pkl\")\n",
    "        print(\"ÄÃ£ lÆ°u ngÆ°á»¡ng há»c Ä‘Æ°á»£c vÃ o file pkl\")\n",
    "\n",
    "        joblib.dump(feature_map, f\"{CFG.model_name}_features.pkl\")\n",
    "        print(\"ÄÃ£ lÆ°u danh sÃ¡ch features vÃ o file pkl\")\n",
    "        \n",
    "    return f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2bee7",
   "metadata": {
    "papermill": {
     "duration": 0.006772,
     "end_time": "2025-12-14T18:11:44.461453",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.454681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "badc26fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.475637Z",
     "iopub.status.busy": "2025-12-14T18:11:44.475439Z",
     "iopub.status.idle": "2025-12-14T18:11:44.482267Z",
     "shell.execute_reply": "2025-12-14T18:11:44.481782Z"
    },
    "papermill": {
     "duration": 0.015199,
     "end_time": "2025-12-14T18:11:44.483262",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.468063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_submission(dataset, body_parts_tracked_list):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  Báº®T Äáº¦U QUÃ TRÃŒNH: SUBMISSION\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "    \n",
    "    submission_list = []\n",
    "    \n",
    "    try:\n",
    "        loaded_thresholds = joblib.load(f\"{CFG.model_name}_thresholds.pkl\")\n",
    "        loaded_features = joblib.load(f\"{CFG.model_name}_features.pkl\")\n",
    "        print(\"ÄÃ£ load thÃ nh cÃ´ng ngÆ°á»¡ng threshold vÃ  feature map tá»« file!\")\n",
    "    except:\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y file pkl. Sáº½ dÃ¹ng máº·c Ä‘á»‹nh (cÃ³ thá»ƒ gÃ¢y lá»—i lá»‡ch cá»™t).\")\n",
    "        loaded_thresholds = {\"single\": {}, \"pair\": {}}\n",
    "        loaded_features = {} \n",
    "\n",
    "    for section in range(1, len(body_parts_tracked_list)):\n",
    "        body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "        try:\n",
    "            print(f\">>> Predicting Section {section}/{len(body_parts_tracked_list)-1}\")\n",
    "            \n",
    "            thresh_single = loaded_thresholds[\"single\"].get(str(section), {})\n",
    "            thresh_pair = loaded_thresholds[\"pair\"].get(str(section), {})\n",
    "            \n",
    "            subs_single = submit(body_parts_tracked_str, 'single', section, thresh_single, loaded_features)\n",
    "            submission_list.extend(subs_single)\n",
    "            \n",
    "            subs_pair = submit(body_parts_tracked_str, 'pair', section, thresh_pair, loaded_features)\n",
    "            submission_list.extend(subs_pair)\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"!!! Lá»–I táº¡i Section {section}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if len(submission_list) > 0:\n",
    "        full_submission = pd.concat(submission_list)\n",
    "        full_submission = smooth_predictions(\n",
    "            full_submission,\n",
    "            min_duration_frames=2,\n",
    "            merge_gap_frames=3\n",
    "        )\n",
    "        final_submission = robustify(full_submission, dataset, 'test')\n",
    "        \n",
    "        final_submission.index.name = 'row_id'\n",
    "        final_submission.to_csv('submission.csv')\n",
    "        print(\"\\nSUCCESS: ÄÃ£ táº¡o file 'submission.csv'.\")\n",
    "        display(final_submission.head())\n",
    "    else:\n",
    "        print(\"\\nWARNING: KhÃ´ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£c gÃ¬. Táº¡o file dummy.\")\n",
    "        # dummy fallback\n",
    "        pd.DataFrame({'video_id': [438887472], 'action': ['rear']}).to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c2cfec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:11:44.497198Z",
     "iopub.status.busy": "2025-12-14T18:11:44.496776Z",
     "iopub.status.idle": "2025-12-14T21:05:03.705677Z",
     "shell.execute_reply": "2025-12-14T21:05:03.704967Z"
    },
    "papermill": {
     "duration": 10399.217356,
     "end_time": "2025-12-14T21:05:03.707073",
     "exception": false,
     "start_time": "2025-12-14T18:11:44.489717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " running validation\n",
      ">>> Processing Section 1/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.4991 (0.50) - rear\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.4190 (0.40) - approach\n",
      "\t[Ensemble] F1: 0.4166 (0.20) - attack\n",
      "\t[Ensemble] F1: 0.4890 (0.60) - avoid\n",
      "\t[Ensemble] F1: 0.4981 (0.34) - chase\n",
      "\t[Ensemble] F1: 0.5354 (0.52) - chaseattack\n",
      "\t[Ensemble] F1: 0.0000 (0.10) - submit\n",
      ">>> Processing Section 2/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.6644 (0.38) - huddle\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.6233 (0.58) - reciprocalsniff\n",
      "\t[Ensemble] F1: 0.5385 (0.70) - sniffgenital\n",
      ">>> Processing Section 3/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.4263 (0.56) - rear\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.3164 (0.38) - approach\n",
      "\t[Ensemble] F1: 0.1299 (0.56) - attack\n",
      "\t[Ensemble] F1: 0.2014 (0.62) - avoid\n",
      "\t[Ensemble] F1: 0.0232 (0.48) - chase\n",
      "\t[Ensemble] F1: 0.1983 (0.32) - chaseattack\n",
      "\t[Ensemble] F1: 0.0968 (0.12) - submit\n",
      ">>> Processing Section 4/9\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.3636 (0.58) - attack\n",
      "\t[Ensemble] F1: 0.6462 (0.44) - dominance\n",
      "\t[Ensemble] F1: 0.4811 (0.32) - sniff\n",
      "\t[Ensemble] F1: 0.1189 (0.14) - chase\n",
      "\t[Ensemble] F1: 0.1900 (0.36) - escape\n",
      "\t[Ensemble] F1: 0.7355 (0.72) - follow\n",
      ">>> Processing Section 5/9\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.7324 (0.68) - attack\n",
      "\t[Ensemble] F1: 0.8831 (0.60) - sniff\n",
      "\t[Ensemble] F1: 0.5699 (0.68) - defend\n",
      "\t[Ensemble] F1: 0.7397 (0.60) - escape\n",
      "\t[Ensemble] F1: 0.8411 (0.40) - mount\n",
      ">>> Processing Section 6/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.0658 (0.16) - biteobject\n",
      "\t[Ensemble] F1: 0.5508 (0.54) - climb\n",
      "\t[Ensemble] F1: 0.5828 (0.58) - dig\n",
      "\t[Ensemble] F1: 0.1178 (0.10) - exploreobject\n",
      "\t[Ensemble] F1: 0.3503 (0.46) - rear\n",
      "\t[Ensemble] F1: 0.4693 (0.56) - selfgroom\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.6051 (0.76) - shepherd\n",
      "\t[Ensemble] F1: 0.5308 (0.64) - approach\n",
      "\t[Ensemble] F1: 0.5862 (0.66) - attack\n",
      "\t[Ensemble] F1: 0.6223 (0.34) - chase\n",
      "\t[Ensemble] F1: 0.4376 (0.56) - defend\n",
      "\t[Ensemble] F1: 0.6633 (0.70) - escape\n",
      "\t[Ensemble] F1: 0.0926 (0.20) - flinch\n",
      "\t[Ensemble] F1: 0.4363 (0.40) - follow\n",
      "\t[Ensemble] F1: 0.5042 (0.62) - sniff\n",
      "\t[Ensemble] F1: 0.6337 (0.60) - sniffface\n",
      "\t[Ensemble] F1: 0.2930 (0.38) - sniffgenital\n",
      "\t[Ensemble] F1: 0.3341 (0.10) - tussle\n",
      ">>> Processing Section 7/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.5387 (0.72) - rear\n",
      "\t[Ensemble] F1: 0.5392 (0.50) - rest\n",
      "\t[Ensemble] F1: 0.2910 (0.66) - selfgroom\n",
      "\t[Ensemble] F1: 0.1923 (0.48) - climb\n",
      "\t[Ensemble] F1: 0.3253 (0.66) - dig\n",
      "\t[Ensemble] F1: 0.1193 (0.40) - run\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.6766 (0.70) - sniff\n",
      "\t[Ensemble] F1: 0.5687 (0.74) - sniffgenital\n",
      "\t[Ensemble] F1: 0.4282 (0.72) - approach\n",
      "\t[Ensemble] F1: 0.1337 (0.16) - defend\n",
      "\t[Ensemble] F1: 0.2545 (0.62) - escape\n",
      "\t[Ensemble] F1: 0.3422 (0.20) - attemptmount\n",
      ">>> Processing Section 8/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.2442 (0.64) - rear\n",
      "\t[Ensemble] F1: 0.2808 (0.64) - selfgroom\n",
      "\t[Ensemble] F1: 0.6881 (0.38) - genitalgroom\n",
      "\t[Ensemble] F1: 0.1634 (0.52) - dig\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.4764 (0.72) - approach\n",
      "\t[Ensemble] F1: 0.6672 (0.76) - attack\n",
      "\t[Ensemble] F1: 0.4859 (0.56) - disengage\n",
      "\t[Ensemble] F1: 0.5563 (0.78) - mount\n",
      "\t[Ensemble] F1: 0.6680 (0.66) - sniff\n",
      "\t[Ensemble] F1: 0.5021 (0.70) - sniffgenital\n",
      "\t[Ensemble] F1: 0.4854 (0.72) - dominancemount\n",
      "\t[Ensemble] F1: 0.5743 (0.62) - sniffbody\n",
      "\t[Ensemble] F1: 0.6812 (0.68) - sniffface\n",
      "\t[Ensemble] F1: 0.2019 (0.34) - attemptmount\n",
      "\t[Ensemble] F1: 0.8577 (0.70) - intromit\n",
      "\t[Ensemble] F1: 0.1641 (0.32) - chase\n",
      "\t[Ensemble] F1: 0.5356 (0.64) - escape\n",
      "\t[Ensemble] F1: 0.7737 (0.62) - reciprocalsniff\n",
      "\t[Ensemble] F1: 0.1118 (0.18) - allogroom\n",
      "\t[Ensemble] F1: 0.3724 (0.10) - ejaculate\n",
      "\t[Ensemble] F1: 0.1356 (0.18) - dominancegroom\n",
      ">>> Processing Section 9/9\n",
      "   [Single Mouse] Training...\n",
      "\t[Ensemble] F1: 0.4133 (0.48) - freeze\n",
      "\t[Ensemble] F1: 0.4034 (0.42) - rear\n",
      "   [Pair Mouse] Training...\n",
      "\t[Ensemble] F1: 0.2140 (0.26) - approach\n",
      "\t[Ensemble] F1: 0.7283 (0.72) - attack\n",
      "\t[Ensemble] F1: 0.5779 (0.60) - defend\n",
      "\t[Ensemble] F1: 0.7080 (0.78) - escape\n",
      "\t[Ensemble] F1: 0.6730 (0.44) - sniff\n",
      "ÄÃ£ lÆ°u ngÆ°á»¡ng há»c Ä‘Æ°á»£c vÃ o file pkl\n",
      "ÄÃ£ lÆ°u danh sÃ¡ch features vÃ o file pkl\n",
      "\n",
      "========================================\n",
      "  Báº®T Äáº¦U QUÃ TRÃŒNH: SUBMISSION\n",
      "========================================\n",
      "\n",
      "ÄÃ£ load thÃ nh cÃ´ng ngÆ°á»¡ng threshold vÃ  feature map tá»« file!\n",
      ">>> Predicting Section 1/9\n",
      ">>> Predicting Section 2/9\n",
      ">>> Predicting Section 3/9\n",
      ">>> Predicting Section 4/9\n",
      ">>> Predicting Section 5/9\n",
      ">>> Predicting Section 6/9\n",
      ">>> Predicting Section 7/9\n",
      ">>> Predicting Section 8/9\n",
      ">>> Predicting Section 9/9\n",
      "\n",
      "SUCCESS: ÄÃ£ táº¡o file 'submission.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>action</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>approach</td>\n",
       "      <td>2283</td>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1272</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1429</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1453</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1496</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id agent_id target_id    action  start_frame  stop_frame\n",
       "row_id                                                                 \n",
       "0       438887472   mouse1    mouse3  approach         2283        2292\n",
       "1       438887472   mouse1    mouse4    attack         1272        1281\n",
       "2       438887472   mouse1    mouse4    attack         1429        1433\n",
       "3       438887472   mouse1    mouse4    attack         1453        1491\n",
       "4       438887472   mouse1    mouse4    attack         1496        1527"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "run_validation(train, body_parts_tracked_list)\n",
    "\n",
    "if CFG.mode == 'submit' or CFG.debug:\n",
    "\n",
    "    run_submission(test, body_parts_tracked_list)\n",
    "else:\n",
    "    print(\"validating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6df6d4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:05:03.731081Z",
     "iopub.status.busy": "2025-12-14T21:05:03.730413Z",
     "iopub.status.idle": "2025-12-14T21:05:03.740138Z",
     "shell.execute_reply": "2025-12-14T21:05:03.739602Z"
    },
    "papermill": {
     "duration": 0.022588,
     "end_time": "2025-12-14T21:05:03.741196",
     "exception": false,
     "start_time": "2025-12-14T21:05:03.718608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>action</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse3</td>\n",
       "      <td>approach</td>\n",
       "      <td>2283</td>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1272</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1429</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1453</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>438887472</td>\n",
       "      <td>mouse1</td>\n",
       "      <td>mouse4</td>\n",
       "      <td>attack</td>\n",
       "      <td>1496</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id   video_id agent_id target_id    action  start_frame  stop_frame\n",
       "0       0  438887472   mouse1    mouse3  approach         2283        2292\n",
       "1       1  438887472   mouse1    mouse4    attack         1272        1281\n",
       "2       2  438887472   mouse1    mouse4    attack         1429        1433\n",
       "3       3  438887472   mouse1    mouse4    attack         1453        1491\n",
       "4       4  438887472   mouse1    mouse4    attack         1496        1527"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pd.read_csv('submission.csv')\n",
    "display(result.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8619229,
     "sourceId": 13752019,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8904845,
     "sourceId": 13968630,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10414.03337,
   "end_time": "2025-12-14T21:05:04.670100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-14T18:11:30.636730",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
